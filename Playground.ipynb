{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importacoes, carregamento dos dados, definicao da rede classificadora e treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(trainset_reduzido, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parametro inicial, camada final: 0.04712291806936264\n",
      "parametros finais, camada final: 0.046314749866724014 gradi total: 0.08081717044115067\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "params_init = [param.clone().detach() for param in net.parameters()]\n",
    "for i, param in enumerate(net.parameters()):\n",
    "    if i == 9:\n",
    "        print(f'parametro inicial, camada final: {param[0]}')\n",
    "grads_acu = [torch.zeros_like(param) for param in net.parameters()]\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "net.train()\n",
    "for epoch in range(2):\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        params_anterior = [param.clone().detach() for param in net.parameters()]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for i, param in enumerate(net.parameters()):\n",
    "            grads_acu[i] += param.grad\n",
    "            # if i == 9:\n",
    "            #     print(f'parametro anterior: {params_anterior[i][0]}')\n",
    "            #     print(f'gradiente da iteracao: {param.grad[0]}')\n",
    "            #     print(f'parametro anterior - gradiente: {params_anterior[i][0] - 0.01*param.grad[0]}')\n",
    "            #     print(f'parametro atualizado: {param[0]}')\n",
    "gradients = [grad.cpu().numpy() for grad in grads_acu] \n",
    "for i, param in enumerate(net.parameters()):\n",
    "    if i == 9:\n",
    "        print(f'parametros finais, camada final: {param[0]} gradi total: {gradients[9][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste para qualidade visual das imagens geradas pelo modelo generativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN(nn.Module):\n",
    "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100, batch_size=64):\n",
    "        super(CGAN, self).__init__()\n",
    "        if dataset == \"mnist\":\n",
    "            self.classes = 10\n",
    "            self.channels = 1\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "        self.adv_loss = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
    "            *self._create_layer_gen(128, 256),\n",
    "            *self._create_layer_gen(256, 512),\n",
    "            *self._create_layer_gen(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
    "            *self._create_layer_disc(1024, 512, True, True),\n",
    "            *self._create_layer_disc(512, 256, True, True),\n",
    "            *self._create_layer_disc(256, 128, False, False),\n",
    "            *self._create_layer_disc(128, 1, False, False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(0.4))\n",
    "        if act_func:\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        if input.dim() == 2:\n",
    "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
    "            x = self.generator(z)\n",
    "            x = x.view(x.size(0), *self.img_shape) #Em\n",
    "            return x\n",
    "        elif input.dim() == 4:\n",
    "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs, learning_rate, device, dataset=\"mnist\", latent_dim=100):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    if dataset == \"mnist\":\n",
    "      imagem = \"image\"\n",
    "    elif dataset == \"cifar10\":\n",
    "      imagem = \"img\"\n",
    "\n",
    "    net.to(device)  # move model to GPU if available\n",
    "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "            # Train G\n",
    "            net.zero_grad()\n",
    "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "            x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "            x_fake = net(z_noise, x_fake_labels)\n",
    "            y_fake_g = net(x_fake, x_fake_labels)\n",
    "            g_loss = net.loss(y_fake_g, real_ident)\n",
    "            g_loss.backward()\n",
    "            optim_G.step()\n",
    "\n",
    "            # Train D\n",
    "            net.zero_grad()\n",
    "            y_real = net(images, labels)\n",
    "            d_real_loss = net.loss(y_real, real_ident)\n",
    "            y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optim_D.step()\n",
    "\n",
    "            g_losses.append(g_loss.item())\n",
    "            d_losses.append(d_loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
    "                            epoch, batch_idx, len(trainloader),\n",
    "                            d_loss.mean().item(),\n",
    "                            g_loss.mean().item()))\n",
    "\n",
    "\n",
    "\n",
    "def test(net, testloader, device, dataset=\"mnist\", latent_dim=100):\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    if dataset == \"mnist\":\n",
    "      imagem = \"image\"\n",
    "    elif dataset == \"cifar10\":\n",
    "      imagem = \"img\"\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(testloader):\n",
    "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "            #Gen loss\n",
    "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "            x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "            x_fake = net(z_noise, x_fake_labels)\n",
    "            y_fake_g = net(x_fake, x_fake_labels)\n",
    "            g_loss = net.loss(y_fake_g, real_ident)\n",
    "\n",
    "            #Disc loss\n",
    "            y_real = net(images, labels)\n",
    "            d_real_loss = net.loss(y_real, real_ident)\n",
    "            y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            g_losses.append(g_loss.item())\n",
    "            d_losses.append(d_loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                print('[{}/{}] loss_D_teste: {:.4f} loss_G_teste: {:.4f}'.format(\n",
    "                            batch_idx, len(testloader),\n",
    "                            d_loss.mean().item(),\n",
    "                            g_loss.mean().item()))\n",
    "    return np.mean(g_losses), np.mean(d_losses)\n",
    "\n",
    "\n",
    "def get_weights(net):\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_weights(net, parameters):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "latent_dim = 100\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cpu\"\n",
    "net = CGAN(dataset=\"mnist\", latent_dim=latent_dim).to(device)\n",
    "net.load_state_dict(torch.load('modelo_gen_round_20_client_3.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# Assuming netG is your generator model, classes is the total number of classes, and latent_dim is the latent vector size\n",
    "examples_per_class = 5\n",
    "classes = 10\n",
    "batch_size = examples_per_class * classes  # Generate enough images to have `examples_per_class` for each class\n",
    "\n",
    "# Generate latent vectors and corresponding labels\n",
    "latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
    "labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    generated_images = net(latent_vectors, labels)\n",
    "\n",
    "fig = plt.figure(figsize=(examples_per_class, classes))\n",
    "plt.title(\"Generated Images\")\n",
    "for i in range(generated_images.shape[0]):\n",
    "    plt.subplot(10, 5, i+1)\n",
    "    plt.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"mnist_CGAN_20r_2e_64b_100z_10c_28i_0001lr_niid_01dir.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeraFed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
