{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F2U_GAN(nn.Module):\n",
    "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=128, condition=True):\n",
    "        super(F2U_GAN, self).__init__()\n",
    "        if dataset == \"mnist\":\n",
    "            self.classes = 10\n",
    "            self.channels = 1\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only MNIST is supported\")\n",
    "        \n",
    "        self.condition = condition\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes) if condition else None\n",
    "        #self.label_embedding_disc = nn.Embedding(self.classes, self.img_size*self.img_size) if condition else None\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.input_shape_gen = self.latent_dim + self.label_embedding.embedding_dim if condition else self.latent_dim\n",
    "        self.input_shape_disc = self.channels + self.classes if condition else self.channels\n",
    "\n",
    "        self.adv_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Generator (unchanged) To calculate output shape of convtranspose layers, we can use the formula:\n",
    "        # output_shape = (input_shape - 1) * stride - 2 * padding + kernel_size + output_padding (or dilation * (kernel_size - 1) + 1 inplace of kernel_size if using dilation)\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(self.input_shape_gen, 256 * 7 * 7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Unflatten(1, (256, 7, 7)),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (256,7,7) -> (128,14,14)\n",
    "            nn.BatchNorm2d(128, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # (128,14,14) -> (64,28,28)\n",
    "            nn.BatchNorm2d(64, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, self.channels, kernel_size=3, stride=1, padding=1), # (64,28,28) -> (1,28,28)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Discriminator (corrected) To calculate output shape of conv layers, we can use the formula:\n",
    "        # output_shape = ⌊(input_shape - kernel_size + 2 * padding) / stride + 1⌋ (or (dilation * (kernel_size - 1) - 1) inplace of kernel_size if using dilation)\n",
    "        self.discriminator = nn.Sequential(\n",
    "        # Camada 1: (1,28,28) -> (32,13,13)\n",
    "        nn.utils.spectral_norm(nn.Conv2d(self.input_shape_disc, 32, kernel_size=3, stride=2, padding=0)),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        # Camada 2: (32,14,14) -> (64,7,7)\n",
    "        nn.utils.spectral_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        # Camada 3: (64,7,7) -> (128,3,3)\n",
    "        nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0)),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        # Camada 4: (128,3,3) -> (256,1,1)\n",
    "        nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0)),  # Padding 0 aqui!\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        # Achata e concatena com as labels\n",
    "        nn.Flatten(), # (256,1,1) -> (256*1*1,)\n",
    "        nn.utils.spectral_norm(nn.Linear(256 * 1 * 1, 1))  # 256 (features)\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels=None):\n",
    "        if input.dim() == 2:\n",
    "            # Generator forward pass (unchanged)\n",
    "            if self.condition:\n",
    "                embedded_labels = self.label_embedding(labels)\n",
    "                gen_input = torch.cat((input, embedded_labels), dim=1)\n",
    "                x = self.generator(gen_input)\n",
    "            else:\n",
    "                x = self.generator(input)\n",
    "            return x.view(-1, *self.img_shape)\n",
    "\n",
    "        elif input.dim() == 4:\n",
    "            # Discriminator forward pass\n",
    "            if self.condition:\n",
    "                embedded_labels = self.label_embedding(labels)\n",
    "                image_labels = embedded_labels.view(embedded_labels.size(0), self.label_embedding.embedding_dim, 1, 1).expand(-1, -1, self.img_size, self.img_size)\n",
    "                x = torch.cat((input, image_labels), dim=1)\n",
    "            else:\n",
    "                x = input\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_round54.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m F2U_GAN(condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Assuming the model is unconditional\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_path = \"gen_round54.pt\"\n",
    "model = F2U_GAN(condition=True)  # Assuming the model is unconditional\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(net, device, round_number, client_id = None, examples_per_class: int=5, classes: int=10, latent_dim: int=100, server: bool=False):\n",
    "    \"\"\"Gera plot de imagens de cada classe\"\"\"\n",
    "    if server:\n",
    "        import matplotlib\n",
    "        matplotlib.use(\"Agg\")\n",
    "        import matplotlib.pyplot as plt\n",
    "    else:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    net_type = type(net).__name__\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    batch_size = examples_per_class * classes\n",
    "\n",
    "    latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
    "    labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if net_type == \"Generator\":\n",
    "            labels_one_hot = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
    "            generated_images = net(torch.cat([latent_vectors, labels_one_hot], dim=1))\n",
    "        else:\n",
    "            generated_images = net(latent_vectors, labels)\n",
    "\n",
    "    # Criar uma figura com 10 linhas e 5 colunas de subplots\n",
    "    fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
    "\n",
    "    # Adiciona título no topo da figura\n",
    "    if isinstance(client_id, int):\n",
    "        fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
    "    else:\n",
    "        fig.text(0.5, 0.98, f\"Round: {round_number}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "    # Exibir as imagens nos subplots\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Ajustar o layout antes de calcular as posições\n",
    "    plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
    "\n",
    "    # Reduzir espaço entre colunas\n",
    "    # plt.subplots_adjust(wspace=0.05)\n",
    "\n",
    "    # Adicionar os rótulos das classes corretamente alinhados\n",
    "    fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
    "    for row in range(classes):\n",
    "        # Obter posição do subplot em coordenadas da figura\n",
    "        bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
    "        pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
    "        center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
    "\n",
    "        # Adicionar o rótulo\n",
    "        fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
    "\n",
    "    IN_COLAB = False\n",
    "    try:\n",
    "        # Tenta importar um módulo específico do Colab\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "    except:\n",
    "        pass\n",
    "    if IN_COLAB:\n",
    "        if isinstance(client_id, int):\n",
    "            fig.savefig(os.path.join(save_dir, f\"mnist_{net_type}_r{round_number}_c{client_id}.png\"))\n",
    "            print(\"Imagem do cliente salva no drive\")\n",
    "        else:\n",
    "            fig.savefig(os.path.join(save_dir, f\"mnist_{net_type}_r{round_number}.png\"))\n",
    "            print(\"Imagem do servidor salva no drive\")\n",
    "    else:\n",
    "        if isinstance(client_id, int):\n",
    "            fig.savefig(f\"mnist_{net_type}_r{round_number}_c{client_id}.png\")\n",
    "            print(\"Imagem do cliente salva\")\n",
    "        else:\n",
    "            fig.savefig(f\"mnist_{net_type}_r{round_number}.png\")\n",
    "            print(\"Imagem do servidor salva\")\n",
    "    plt.close(fig)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagem do servidor salva\n"
     ]
    }
   ],
   "source": [
    "generate_plot(model, torch.device(\"cpu\"), 54, server=True, examples_per_class=5, classes=10, latent_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeneratedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that generates images on the fly from a GAN generator.\n",
    "\n",
    "    Args:\n",
    "        generator (torch.nn.Module): GAN generator model.\n",
    "        num_samples (int): Number of samples per class (if balanced) or total samples (if random).\n",
    "        latent_dim (int): Dimensionality of the latent vector z.\n",
    "        num_classes (int): Number of classes.\n",
    "        device (torch.device or str): Device to run generation on.\n",
    "        balanced (bool): If True, generates num_samples per class (balanced dataset).\n",
    "                         If False, generates num_samples samples with random class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, num_samples, latent_dim, num_classes, device, balanced=True):\n",
    "        self.generator = generator\n",
    "        self.num_samples = num_samples\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.balanced = balanced\n",
    "        self.model_name = type(self.generator).__name__\n",
    "        self.classes = [i for i in range(self.num_classes)]\n",
    "\n",
    "        # Generate data once at initialization\n",
    "        if self.balanced:\n",
    "            # Balanced: num_samples per class\n",
    "            self.images = self._generate_balanced()\n",
    "            self.total_len = num_samples * num_classes\n",
    "        else:\n",
    "            # Random: total num_samples images with random labels\n",
    "            self.images = self._generate_random()\n",
    "            self.total_len = num_samples\n",
    "\n",
    "    def _generate_balanced(self):\n",
    "        \"\"\"\n",
    "        Generate num_samples images for each class.\n",
    "        Returns a dict mapping class_idx to tensor of images.\n",
    "        \"\"\"\n",
    "        self.generator.eval()\n",
    "        gen_imgs = {}\n",
    "        # Prepare one-hot labels if needed\n",
    "        for class_idx in range(self.num_classes):\n",
    "            # Create labels tensor\n",
    "            labels = torch.full((self.num_samples,), class_idx, dtype=torch.long, device=self.device)\n",
    "\n",
    "            # Prepare latent vectors\n",
    "            z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.model_name == 'Generator':\n",
    "                    # One-hot encoding\n",
    "                    labels_one_hot = F.one_hot(labels, num_classes=self.num_classes).float().to(self.device)\n",
    "                    gen = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
    "                else:\n",
    "                    gen = self.generator(z, labels)\n",
    "\n",
    "            gen_imgs[class_idx] = gen\n",
    "\n",
    "        return gen_imgs\n",
    "\n",
    "    def _generate_random(self):\n",
    "        \"\"\"\n",
    "        Generate num_samples images with random class labels.\n",
    "        Returns a list of generated images.\n",
    "        \"\"\"\n",
    "        self.generator.eval()\n",
    "        images = []\n",
    "\n",
    "        # Sample random class labels\n",
    "        labels = torch.randint(0, self.num_classes, (self.num_samples,), device=self.device)\n",
    "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.model_name == 'Generator':\n",
    "                labels_one_hot = F.one_hot(labels, num_classes=self.num_classes).float().to(self.device)\n",
    "                gen = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
    "            else:\n",
    "                gen = self.generator(z, labels)\n",
    "\n",
    "        # gen shape: [num_samples, channels, height, width]\n",
    "        # Split into list\n",
    "        for i in range(self.num_samples):\n",
    "            images.append(gen[i])\n",
    "\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns only the image at index idx.\n",
    "        \"\"\"\n",
    "        if self.balanced:\n",
    "            class_idx = idx // self.num_samples\n",
    "            sample_idx = idx % self.num_samples\n",
    "            return self.images[class_idx][sample_idx]\n",
    "        else:\n",
    "            return self.images[idx]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gerafed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
