{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bewK05okXn9u"
      },
      "source": [
        "## Treinamento modelo classificador e geradora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vda6HdlXn90"
      },
      "source": [
        "### Centralizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcekFyKmXn90"
      },
      "source": [
        "#### Importacoes, carregamento dos dados, definicao da rede classificadora e treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQq3sXpTXn92"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2HQ8Sih1Xn_c"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E7ZYEUmkXn_h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P3JP3W75Xn_i"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*4*4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mVt9PtdGXn_j"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gxtofuOWXn_m"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
        "# Create data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainloader_reduzido = DataLoader(trainset_reduzido, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QISJ5_duXn_n"
      },
      "source": [
        "##### Salvando imagens MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON7L-wrUXn_o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpOrcDK2Xn_o"
      },
      "outputs": [],
      "source": [
        "# Function to save a random sample of images\n",
        "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    if classes is None:\n",
        "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
        "\n",
        "    if balanced:\n",
        "        # Get the number of classes\n",
        "        num_classes = len(classes)\n",
        "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
        "        indices = []\n",
        "        class_counts = {i: 0 for i in classes}\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        shuffled_indices = list(range(len(dataset)))\n",
        "        random.shuffle(shuffled_indices)\n",
        "\n",
        "        for idx in shuffled_indices:\n",
        "            img = dataset[idx][0]\n",
        "            label = int(dataset[idx][1])\n",
        "            if label in classes and class_counts[label] < samples_per_class:\n",
        "                indices.append(idx)\n",
        "                class_counts[label] += 1\n",
        "            if len(indices) >= num_samples:\n",
        "                break\n",
        "    else:\n",
        "        indices = []\n",
        "        while len(indices) < num_samples:\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            if int(dataset[idx][1]) in classes:\n",
        "                indices.append(idx)\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = dataset[idx]\n",
        "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
        "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoanL3q-Xn_p"
      },
      "outputs": [],
      "source": [
        "save_random_samples(trainset, num_samples=2048, balanced=True, classes=[i], folder=filter\"Imagens Testes/mnist_samples_{i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTkej1xYXn_p"
      },
      "source": [
        "#### Definicao da GAN e funcoes de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j9zuYSTUXn_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yihGlxjjXn_p"
      },
      "outputs": [],
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(CGAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        if input.dim() == 2:\n",
        "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "            x = self.generator(z)\n",
        "            x = x.view(x.size(0), *self.img_shape) #Em\n",
        "            return x\n",
        "        elif input.dim() == 4:\n",
        "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)\n",
        "\n",
        "\n",
        "def train_gen(net, trainloader, epochs, lr, device, dataset=\"mnist\", latent_dim=100, f2a: bool = False, cliente: bool = False, D=None):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    if dataset == \"mnist\":\n",
        "      imagem = \"image\"\n",
        "    elif dataset == \"cifar10\":\n",
        "      imagem = \"img\"\n",
        "\n",
        "    net.to(device)  # move model to GPU if available\n",
        "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, batch in enumerate(trainloader):\n",
        "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
        "            batch_size = images.size(0)\n",
        "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "            if not f2a:\n",
        "                # Train G\n",
        "                net.zero_grad()\n",
        "                z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                x_fake = net(z_noise, x_fake_labels)\n",
        "                y_fake_g = net(x_fake, x_fake_labels)\n",
        "                g_loss = net.loss(y_fake_g, real_ident)\n",
        "                g_loss.backward()\n",
        "                optim_G.step()\n",
        "\n",
        "                # Train D\n",
        "                net.zero_grad()\n",
        "                y_real = net(images, labels)\n",
        "                d_real_loss = net.loss(y_real, real_ident)\n",
        "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                d_loss.backward()\n",
        "                optim_D.step()\n",
        "\n",
        "                g_losses.append(g_loss.item())\n",
        "                d_losses.append(d_loss.item())\n",
        "\n",
        "                if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                    print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                d_loss.mean().item(),\n",
        "                                g_loss.mean().item()))\n",
        "\n",
        "            else:\n",
        "                if cliente:\n",
        "                    # Train D\n",
        "                    net.zero_grad()\n",
        "                    y_real = net(images, labels)\n",
        "                    d_real_loss = net.loss(y_real, real_ident)\n",
        "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                    x_fake = net(z_noise, x_fake_labels)\n",
        "                    y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "                    d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "                    d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                    d_loss.backward()\n",
        "                    optim_D.step()\n",
        "\n",
        "                    d_losses.append(d_loss.item())\n",
        "\n",
        "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                        print('Epoch {} [{}/{}] loss_D_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                d_loss.mean().item()))\n",
        "\n",
        "                else:\n",
        "                    # Train G\n",
        "                    net.zero_grad()\n",
        "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                    x_fake = net(z_noise, x_fake_labels)\n",
        "                    y_fake_g = net(x_fake, x_fake_labels)\n",
        "                    g_loss = net.loss(y_fake_g, real_ident)\n",
        "                    g_loss.backward()\n",
        "                    optim_G.step()\n",
        "\n",
        "                    g_losses.append(g_loss.item())\n",
        "\n",
        "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                        print('Epoch {} [{}/{}] loss_G_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                g_loss.mean().item()))\n",
        "\n",
        "\n",
        "\n",
        "def test(net, testloader, device, dataset=\"mnist\", latent_dim=100):\n",
        "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
        "    if dataset == \"mnist\":\n",
        "      imagem = \"image\"\n",
        "    elif dataset == \"cifar10\":\n",
        "      imagem = \"img\"\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(testloader):\n",
        "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
        "            batch_size = images.size(0)\n",
        "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "            #Gen loss\n",
        "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "            x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "            x_fake = net(z_noise, x_fake_labels)\n",
        "            y_fake_g = net(x_fake, x_fake_labels)\n",
        "            g_loss = net.loss(y_fake_g, real_ident)\n",
        "\n",
        "            #Disc loss\n",
        "            y_real = net(images, labels)\n",
        "            d_real_loss = net.loss(y_real, real_ident)\n",
        "            y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "            g_losses.append(g_loss.item())\n",
        "            d_losses.append(d_loss.item())\n",
        "\n",
        "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                print('[{}/{}] loss_D_teste: {:.4f} loss_G_teste: {:.4f}'.format(\n",
        "                            batch_idx, len(testloader),\n",
        "                            d_loss.mean().item(),\n",
        "                            g_loss.mean().item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUPN43l4Xn_r"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "CXzJElIeXn_w"
      },
      "outputs": [],
      "source": [
        "# Configurações\n",
        "LATENT_DIM = 128\n",
        "LEARNING_RATE = 0.0002\n",
        "BETA1 = 0.5\n",
        "BETA2 = 0.9\n",
        "GP_SCALE = 10\n",
        "NUM_CHANNELS = 1\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 50\n",
        "# Camada de Convolução para o Discriminador\n",
        "def conv_block(in_channels, out_channels, kernel_size=5, stride=2, padding=2, use_bn=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
        "    if use_bn:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Discriminador\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            conv_block(NUM_CHANNELS + NUM_CLASSES, 64, use_bn=False),\n",
        "            conv_block(64, 128, use_bn=True),\n",
        "            conv_block(128, 256, use_bn=True),\n",
        "            conv_block(256, 512, use_bn=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 2 * 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Camada de upsample para o Gerador\n",
        "def upsample_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_bn=True):\n",
        "    layers = [\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels) if use_bn else nn.Identity(),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    ]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Gerador\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + NUM_CLASSES, 4 * 4 * 256),\n",
        "            nn.BatchNorm1d(4 * 4 * 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Unflatten(1, (256, 4, 4)),\n",
        "            upsample_block(256, 128),\n",
        "            upsample_block(128, 64),\n",
        "            upsample_block(64, 32),\n",
        "            nn.Conv2d(32, NUM_CHANNELS, kernel_size=5, stride=1, padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fPY0beuXoBV"
      },
      "source": [
        "#### Geração de Dados Sintéticos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c-l-6raaXoBW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GeneratedDataset(Dataset):\n",
        "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.model = type(self.generator).__name__\n",
        "        self.images, self.labels = self.generate_data()\n",
        "        self.classes = [i for i in range(self.num_classes)]\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        self.generator.eval()\n",
        "        labels = torch.tensor([i for i in range(self.num_classes) for _ in range(self.num_samples // self.num_classes)], device=self.device)\n",
        "        if self.model == 'Generator':\n",
        "            labels_one_hot = F.one_hot(labels, self.num_classes).float().to(self.device) #\n",
        "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "        with torch.no_grad():\n",
        "            if self.model == 'Generator':\n",
        "                gen_imgs = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
        "            elif self.model == 'CGAN':\n",
        "                gen_imgs = self.generator(z, labels)\n",
        "\n",
        "        return gen_imgs.cpu(), labels.cpu()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abkIz6SPXoBW"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cEGGzm2XoBW"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 10000\n",
        "latent_dim = 128\n",
        "\n",
        "G = Generator(latent_dim=128).to(\"cpu\")\n",
        "G.load_state_dict(torch.load(\"wgan_43e_128b_0.0002lr.pth\", map_location=torch.device('cpu'))[\"generator\"])\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=G, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGU4X9cFXoBX"
      },
      "source": [
        "##### CGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dR8Apn1XXoBX"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 60000\n",
        "latent_dim = 100\n",
        "\n",
        "gan = CGAN()\n",
        "gan.load_state_dict(torch.load(\"CGAN_50epochs.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=gan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9407SFvSXoBX"
      },
      "source": [
        "Salvando imagens CGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zEozbb5XoBX"
      },
      "outputs": [],
      "source": [
        "save_random_samples(generated_dataset, num_samples=2048, balanced=True, folder='Imagens Testes/cgan_samples_niid_0.7acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DjkOWq3mXoBY"
      },
      "outputs": [],
      "source": [
        "net = Net()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "net.train()\n",
        "for epoch in range(5):\n",
        "    for data in generated_dataloader:\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NSQ_NYB_XoBY"
      },
      "outputs": [],
      "source": [
        "correct, loss = 0, 0.0\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in testloader:\n",
        "        images = batch[0]\n",
        "        labels = batch[1]\n",
        "        outputs = net(images)\n",
        "        loss += criterion(outputs, labels).item()\n",
        "        correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "accuracy = correct / len(testloader.dataset)\n",
        "loss = loss / len(testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Er2sYOlBXoBY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9497"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFt_vSA_XoBY"
      },
      "source": [
        "#### Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jW6ykXwXoBZ"
      },
      "outputs": [],
      "source": [
        "net = CGAN()\n",
        "train(net=net,\n",
        "      trainloader=trainloader,\n",
        "      epochs=50,\n",
        "      learning_rate=0.0001,\n",
        "      device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOjPqx2oXoBZ"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), 'CGAN_50epochs.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCs7kMrjXoBZ"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xfvrkD1XoBa"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihvzrqgrXoBa"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "445r_BCSXoBa"
      },
      "outputs": [],
      "source": [
        "# Inicializar modelos\n",
        "D = Discriminator().to(device)\n",
        "G = Generator(latent_dim=LATENT_DIM).to(device)\n",
        "# Otimizadores\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
        "\n",
        " # Função de perda Wasserstein\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return fake_output.mean() - real_output.mean()\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return -fake_output.mean()\n",
        "\n",
        "# Função para calcular Gradient Penalty\n",
        "def gradient_penalty(D, real_samples, fake_samples):\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
        "    interpolated = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "    d_interpolated = D(interpolated)\n",
        "    gradients = torch.autograd.grad(outputs=d_interpolated, inputs=interpolated,\n",
        "                                    grad_outputs=torch.ones_like(d_interpolated),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "I0HlgddkXoBb"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'LEARNING_RATE' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gan \u001b[38;5;241m=\u001b[39m CGAN(latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m optimizer_D \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(gan\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[43mLEARNING_RATE\u001b[49m, betas\u001b[38;5;241m=\u001b[39m(BETA1, BETA2))\n\u001b[1;32m      3\u001b[0m optimizer_G \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(gan\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, betas\u001b[38;5;241m=\u001b[39m(BETA1, BETA2))\n\u001b[1;32m      5\u001b[0m scheduler_D \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer_D, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LEARNING_RATE' is not defined"
          ]
        }
      ],
      "source": [
        "gan = CGAN(latent_dim=128).to(device)\n",
        "optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optim_D, step_size=5, gamma=0.9)\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUGhPPWsXoBb"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7M7HXKbXoBc"
      },
      "outputs": [],
      "source": [
        "# Treinamento\n",
        "historico_metricas = []\n",
        "wgan = True\n",
        "epoch_bar = tqdm(range(EPOCHS), desc=\"Treinamento\", leave=True, position=0)\n",
        "for epoch in epoch_bar:\n",
        "\n",
        "    print(f\"\\n🔹 Epoch {epoch+1}/{EPOCHS}\")\n",
        "    G_loss = 0\n",
        "    D_loss = 0\n",
        "    batches = 0\n",
        "\n",
        "    batch_bar = tqdm(trainloader_reduzido, desc=\"Batches\", leave=False, position=1)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for real_images, labels in batch_bar:\n",
        "        real_images = real_images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch = real_images.size(0)\n",
        "        fake_labels = torch.randint(0, NUM_CLASSES, (batch,), device=device)\n",
        "        z = torch.randn(batch, LATENT_DIM).to(device)\n",
        "        optimizer_D.zero_grad()\n",
        "        if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "            fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
        "\n",
        "            # Adicionar labels ao real_images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            real_images = torch.cat([real_images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z = torch.cat([z, fake_labels], dim=1)\n",
        "            fake_images = G(z).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            D(real_images)\n",
        "            loss_D = discriminator_loss(D(real_images), D(fake_images)) + GP_SCALE * gradient_penalty(D, real_images, fake_images)\n",
        "\n",
        "        else:\n",
        "            real_ident = torch.full((batch, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch, 1), 0., device=device)\n",
        "            x_fake = gan(z, fake_labels)\n",
        "\n",
        "            y_real = gan(real_images, labels)\n",
        "            d_real_loss = gan.loss(y_real, real_ident)\n",
        "            y_fake_d = gan(x_fake.detach(), fake_labels)\n",
        "            d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
        "            loss_D = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # z = torch.randn(batch, LATENT_DIM).to(device)\n",
        "        # z = torch.cat([z, fake_labels], dim=1)\n",
        "        if wgan:\n",
        "            fake_images = G(z)\n",
        "            loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
        "        else:\n",
        "            y_fake_g = gan(x_fake, fake_labels)\n",
        "            loss_G = gan.loss(y_fake_g, real_ident)\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        G_loss += loss_G.item()\n",
        "        D_loss += loss_D.item()\n",
        "        batches += BATCH_SIZE\n",
        "\n",
        "    avg_epoch_G_loss = G_loss/batches\n",
        "    avg_epoch_D_loss = D_loss/batches\n",
        "    # Create the dataset and dataloader\n",
        "    if wgan:\n",
        "        generated_dataset = GeneratedDataset(generator=G, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
        "    else:\n",
        "        generated_dataset = GeneratedDataset(generator=gan, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
        "    generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    net = Net()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "    net.train()\n",
        "    for _ in range(5):\n",
        "        for data in generated_dataloader:\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    net.eval()\n",
        "    correct, loss = 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images = batch[0]\n",
        "            labels = batch[1]\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    epoch_bar.set_postfix({\n",
        "        \"D_loss\": f\"{avg_epoch_D_loss:.4f}\",\n",
        "        \"G_loss\": f\"{avg_epoch_G_loss:.4f}\",\n",
        "        \"Acc\": f\"{accuracy:.4f}\"\n",
        "    })\n",
        "\n",
        "    with open(\"Treino_GAN.txt\", \"a\") as f:\n",
        "            f.write(f\"Epoca: {epoch+1}, D_loss: {avg_epoch_D_loss:.4f}, G_loss: {avg_epoch_G_loss:.4f}, Acc: {accuracy:.4f}, Tempo: {total_time:.4f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Atualiza o learning_rate\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "    print(f\"Após Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
        "    if wgan:\n",
        "         # Salvar modelo a cada época\n",
        "        torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, f\"wgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
        "    else:\n",
        "        torch.save(gan.state_dict(), f\"cgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
        "\n",
        "print(\"✅ Treinamento Concluído!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8n9eIqhXoCF"
      },
      "outputs": [],
      "source": [
        "torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, \"wgan_29e_64b_0.002lr.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyhBKXfHXoCI"
      },
      "source": [
        "##### Ajuste de hiperparametro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMuI-AbbXoCI"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8lgBKXqXoCI"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import optuna\n",
        "from optuna.importance import get_param_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89xbKVJMXoCJ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=\"cgan\"\n",
        "EPOCHS = 5\n",
        "# Função Objetiva (a ser otimizada pelo Optuna)\n",
        "def objective(trial):\n",
        "    # Escolher os hiperparâmetros dentro de um intervalo\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 16, 1024)\n",
        "    latent_dim = trial.suggest_int(\"latent_dim\", 10, 1000)\n",
        "    lr = trial.suggest_float(\"learning_rate\", 0.0001, 0.05, log=True)\n",
        "    beta1 = trial.suggest_float(\"beta1\", 0.0, 0.9)\n",
        "    beta2 = trial.suggest_float(\"beta2\", 0.8, 0.999)\n",
        "    global model\n",
        "    model = model.lower()\n",
        "    if model==\"wgan\":\n",
        "        gp_scale = trial.suggest_int(\"gp_scale\", 0, 100)\n",
        "\n",
        "    # Criar DataLoader com batch_size otimizado\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Criar novos modelos e otimizadores\n",
        "    if model == \"wgan\":\n",
        "        D = Discriminator().to(device)\n",
        "        G = Generator(latent_dim=latent_dim).to(device)\n",
        "        optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "        optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "        G.train()\n",
        "        D.train()\n",
        "    elif model == \"cgan\":\n",
        "        gan = CGAN(latent_dim=latent_dim).to(device)\n",
        "        optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "        optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "    scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "    scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    # Treinar por algumas épocas\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_batches = 0\n",
        "        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for real_images, labels in progress_bar:\n",
        "            real_images = real_images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            fake_labels = torch.randint(0, NUM_CLASSES, (real_images.size(0),), device=device)\n",
        "            z = torch.randn(real_images.size(0), latent_dim).to(device)\n",
        "            optimizer_D.zero_grad()\n",
        "            if model==\"wgan\":\n",
        "                labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "                fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
        "\n",
        "                # Adicionar labels ao real_images para treinamento do Discriminador\n",
        "                image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "                image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "                real_images = torch.cat([real_images, image_labels], dim=1)\n",
        "\n",
        "                # Treinar Discriminador\n",
        "                z = torch.cat([z, labels], dim=1)\n",
        "                fake_images = G(z).detach()\n",
        "                fake_images = torch.cat([fake_images, image_labels], dim=1)\n",
        "\n",
        "                loss_D = discriminator_loss(D(real_images), D(fake_images)) + gp_scale * gradient_penalty(D, real_images, fake_images)\n",
        "\n",
        "\n",
        "            else:\n",
        "                real_ident = torch.full((real_images.size(0), 1), 1., device=device)\n",
        "                fake_ident = torch.full((real_images.size(0), 1), 0., device=device)\n",
        "                x_fake = gan(z, fake_labels)\n",
        "\n",
        "                y_real = gan(real_images, labels)\n",
        "                d_real_loss = gan.loss(y_real, real_ident)\n",
        "                y_fake_d = gan(x_fake.detach(), fake_labels)\n",
        "                d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
        "                loss_D = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "\n",
        "            # Treinar Gerador\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            if model==\"wgan\":\n",
        "                fake_images = G(z)\n",
        "                loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
        "            else:\n",
        "                y_fake_g = gan(x_fake, fake_labels)\n",
        "                loss_G = gan.loss(y_fake_g, real_ident)\n",
        "\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            epoch_loss += loss_G.item()\n",
        "            total_loss += loss_G.item()\n",
        "            total_batches += 1\n",
        "            epoch_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix(d_loss=loss_D.item(), g_loss=loss_G.item())\n",
        "\n",
        "        # Calcular a loss média dessa época\n",
        "        epoch_avg_loss = epoch_loss / epoch_batches\n",
        "        # Reporta a loss média da época para pruning\n",
        "        trial.report(epoch_avg_loss, epoch)\n",
        "        if trial.should_prune() and epoch >=3:\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        scheduler_G.step()\n",
        "        scheduler_D.step()\n",
        "        print(f\"Após Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "\n",
        "    return avg_loss  # Optuna tentará minimizar essa métrica\n",
        "\n",
        "# Criar estudo do Optuna e otimizar hiperparâmetros\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Exibir os melhores hiperparâmetros encontrados\n",
        "print(\"\\n🔹 Melhores Hiperparâmetros Encontrados:\")\n",
        "print(study.best_params)\n",
        "\n",
        "importance = get_param_importances(study)\n",
        "print(\"Hyperparameter Importances:\")\n",
        "for param, imp in importance.items():\n",
        "    print(f\"{param}: {imp:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt2EiBMCXoCL"
      },
      "source": [
        "#### Teste para qualidade visual das imagens geradas pelo modelo generativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hdas7twXoCM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByyDvVxcXoCM"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "latent_dim = 128\n",
        "# random.seed(seed)\n",
        "# np.random.seed(seed)\n",
        "#torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "path = \"\"\n",
        "device = \"cpu\"\n",
        "net = CGAN(dataset=\"mnist\", latent_dim=latent_dim).to(device)\n",
        "net.load_state_dict(torch.load(f'{path}/model_round_5_mnist.pt'))\n",
        "# G = Generator(latent_dim=128)\n",
        "# G.load_state_dict(torch.load(\"wgan_5e_512b_0.002lr_0.5B1_0.9B2_10gp_128_ld.pth\")[\"generator\"])\n",
        "#net = CGAN()\n",
        "#net.load_state_dict(torch.load('CGAN_50epochs.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "#net.eval()\n",
        "G.eval()\n",
        "# Assuming netG is your generator model, classes is the total number of classes, and latent_dim is the latent vector size\n",
        "examples_per_class = 5\n",
        "classes = 10\n",
        "batch_size = examples_per_class * classes  # Generate enough images to have `examples_per_class` for each class\n",
        "\n",
        "# Generate latent vectors and corresponding labels\n",
        "latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
        "labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
        "labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "\n",
        "# Generate images\n",
        "with torch.no_grad():\n",
        "    #generated_images = net(latent_vectors, labels)\n",
        "    generated_images = G(torch.cat([latent_vectors, labels], dim=1))\n",
        "\n",
        "# Criar uma figura com 10 linhas e 5 colunas de subplots\n",
        "fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
        "\n",
        "#fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "fig.text(0.5, 0.98, f\"Round: {5}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "# Exibir as imagens nos subplots\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Ajustar o layout antes de calcular as posições\n",
        "plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "# Reduzir espaço entre colunas\n",
        "# plt.subplots_adjust(wspace=0.05)\n",
        "\n",
        "# Adicionar os rótulos das classes corretamente alinhados\n",
        "fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
        "for row in range(classes):\n",
        "    # Obter posição do subplot em coordenadas da figura\n",
        "    bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "    pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "    center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "    # Adicionar o rótulo\n",
        "    fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
        "    plt.savefig(f\"{path}teste.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtY4KCiWXoCO"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import Image as IPImage, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6PlY_TYXoCP"
      },
      "outputs": [],
      "source": [
        "def create_gif(image_files, output_path, duration=200):\n",
        "    \"\"\"\n",
        "    Cria um GIF animado a partir de uma sequência de imagens.\n",
        "\n",
        "    Args:\n",
        "        image_files (list): Lista de caminhos das imagens.\n",
        "        output_path (str): Caminho para salvar o GIF.\n",
        "        duration (int): Tempo de exibição de cada frame (em ms).\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    frames = [Image.open(img) for img in image_files]  # Carregar imagens\n",
        "    frames[0].save(output_path, format=\"GIF\", save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
        "    display(IPImage(filename=output_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txiXYjHEXoCP"
      },
      "outputs": [],
      "source": [
        "# Exemplo de uso\n",
        "image_files = [\"../imagens geradas/mnist_CGAN_r0_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r1_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r2_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r3_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r4_100e_64_100z_10c_0.0001lr_niid_01dir.png\"]\n",
        "create_gif(image_files, \"global.gif\", duration=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra5PcELgXoCQ"
      },
      "outputs": [],
      "source": [
        "def create_federated_collage(\n",
        "    agg_image_paths,       # Lista de caminhos para as imagens grandes (1 por round)\n",
        "    clients_image_paths,   # Lista de listas: para cada round, lista de caminhos de imagens de clientes\n",
        "    big_scale=2,           # Escala da imagem grande em relação à imagem pequena\n",
        "    small_size=(500, 900),   # Tamanho desejado para cada imagem pequena (largura, altura)\n",
        "    h_gap=0,               # Espaço horizontal entre bloco de imagens\n",
        "    background_color=(255, 255, 255),\n",
        "    save_path=\"collage.png\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Cria um mosaico onde cada round tem:\n",
        "      - 1 imagem \"agregada\" (maior) à esquerda\n",
        "      - N imagens de cliente empilhadas verticalmente à direita\n",
        "\n",
        "    Parâmetros:\n",
        "      agg_image_paths      : lista de strings (caminhos) para as imagens agregadas (1 por round)\n",
        "      clients_image_paths  : lista de listas de strings. Cada sublista é a lista de caminhos das imagens de cada cliente daquele round\n",
        "      big_scale            : fator de escala da imagem grande em relação às pequenas\n",
        "      small_size           : (largura, altura) desejado para cada imagem pequena\n",
        "      background_color     : cor de fundo do mosaico (RGB)\n",
        "      save_path            : caminho do arquivo final a ser salvo\n",
        "\n",
        "    Retorna:\n",
        "      Um objeto PIL.Image com o mosaico criado.\n",
        "    \"\"\"\n",
        "    # Verifica se temos a mesma quantidade de rounds em agg_image_paths e clients_image_paths\n",
        "    assert len(agg_image_paths) == len(clients_image_paths), \\\n",
        "        \"Número de imagens agregadas deve bater com número de listas de clientes.\"\n",
        "\n",
        "    # Carrega todas as imagens agregadas (rounds)\n",
        "    agg_images = []\n",
        "    for path in agg_image_paths:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        agg_images.append(img)\n",
        "\n",
        "    # Carrega todas as imagens de clientes\n",
        "    # clients_image_paths é lista de listas, cada sublista para um round\n",
        "    client_images = []\n",
        "    for round_paths in clients_image_paths:\n",
        "        imgs = [Image.open(p).convert(\"RGB\") for p in round_paths]\n",
        "        client_images.append(imgs)\n",
        "\n",
        "    # Dimensiona as imagens pequenas para small_size\n",
        "    # e as grandes para (big_scale * small_size)\n",
        "    small_w, small_h = small_size\n",
        "    big_w, big_h = big_scale * small_w, big_scale * small_h\n",
        "\n",
        "    # Faz o resize de todas as imagens\n",
        "    for i, img in enumerate(agg_images):\n",
        "        agg_images[i] = img.resize((big_w, big_h), Image.Resampling.LANCZOS)\n",
        "\n",
        "    for i, imgs in enumerate(client_images):\n",
        "        resized_list = []\n",
        "        for im in imgs:\n",
        "            resized_list.append(im.resize((small_w, small_h), Image.Resampling.LANCZOS))\n",
        "        client_images[i] = resized_list\n",
        "\n",
        "    # Calcula quantos rounds e quantos clientes\n",
        "    num_rounds = len(agg_images)\n",
        "\n",
        "    # Para cada round, vamos colocar:\n",
        "    # - Imagem grande (largura big_w, altura big_h)\n",
        "    # - N clientes empilhados (cada um small_h de altura, total N * small_h)\n",
        "    # A largura de cada \"bloco\" de round = (big_w + small_w)\n",
        "    # A altura do bloco = max(big_h, N * small_h) (para acomodar todas as imagens)\n",
        "\n",
        "    # Descobre o número máximo de clientes em qualquer round (para dimensionar corretamente)\n",
        "    max_clients = max(len(imgs) for imgs in client_images)\n",
        "\n",
        "    # Altura total do bloco para cada round\n",
        "    block_h = max(big_h + small_h, max_clients * small_h)\n",
        "    block_w = big_w + small_w  # Largura do bloco do round\n",
        "\n",
        "    # Largura total = num_rounds * block_w\n",
        "    # Altura total = block_h (vamos colocar rounds lado a lado)\n",
        "    total_w = num_rounds * block_w  + h_gap*2*num_rounds-1\n",
        "    total_h = block_h\n",
        "\n",
        "    # Cria imagem de fundo\n",
        "    collage = Image.new(\"RGB\", (total_w, total_h), color=background_color)\n",
        "\n",
        "    # Posiciona cada round\n",
        "    for r in range(num_rounds):\n",
        "        # Posição x para este round\n",
        "        x_offset = r * block_w + 2*r*h_gap\n",
        "\n",
        "        # Coloca a imagem grande (agg)\n",
        "        collage.paste(agg_images[r], (x_offset, small_h))\n",
        "\n",
        "        # Agora empilha as imagens de cliente ao lado (à direita da imagem grande)\n",
        "        y_offset = 0\n",
        "        for c_img in client_images[r]:\n",
        "            collage.paste(c_img, (x_offset + big_w + h_gap, y_offset))\n",
        "            y_offset += small_h\n",
        "\n",
        "    # Salva o resultado\n",
        "    collage.save(save_path)\n",
        "    print(f\"Mosaico criado e salvo em: {save_path}\")\n",
        "\n",
        "    return collage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoP558u6XoCR"
      },
      "outputs": [],
      "source": [
        "path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p8mhqvHXoCR"
      },
      "outputs": [],
      "source": [
        "agg_image_paths = [f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir.png\" for i in range(10, 20)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wWETQNqXoCS"
      },
      "outputs": [],
      "source": [
        "client_image_paths = [[f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir_cliente{j}.png\" for j in range(4)] for i in range(11, 21)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJztr1mtXoCT"
      },
      "outputs": [],
      "source": [
        "create_federated_collage(\n",
        "    agg_image_paths=agg_image_paths,\n",
        "    clients_image_paths=client_image_paths,\n",
        "    big_scale=2,\n",
        "    small_size=(500, 900),\n",
        "    h_gap=80,\n",
        "    background_color=(255, 255, 255),\n",
        "    save_path=f\"{path}CGAN_evol.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu8-avfIXoCT"
      },
      "source": [
        "### FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI2FOV2rXoCU"
      },
      "source": [
        "### Importacoes, classes e configuracoes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7zeFBY8XoCV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjVnx-3FXoCX"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "num_cpus = os.cpu_count()\n",
        "num_workers = min(8, num_cpus,0)\n",
        "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "dims = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz1K-r-OXoCY"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhtBYnxkXoCY"
      },
      "outputs": [],
      "source": [
        "def _inception_v3(*args, **kwargs):\n",
        "    \"\"\"Wraps `torchvision.models.inception_v3`\"\"\"\n",
        "    try:\n",
        "        version = tuple(map(int, torchvision.__version__.split(\".\")[:2]))\n",
        "    except ValueError:\n",
        "        # Just a caution against weird version strings\n",
        "        version = (0,)\n",
        "\n",
        "    # Skips default weight inititialization if supported by torchvision\n",
        "    # version. See https://github.com/mseitzer/pytorch-fid/issues/28.\n",
        "    if version >= (0, 6):\n",
        "        kwargs[\"init_weights\"] = False\n",
        "\n",
        "    # Backwards compatibility: `weights` argument was handled by `pretrained`\n",
        "    # argument prior to version 0.13.\n",
        "    if version < (0, 13) and \"weights\" in kwargs:\n",
        "        if kwargs[\"weights\"] == \"DEFAULT\":\n",
        "            kwargs[\"pretrained\"] = True\n",
        "        elif kwargs[\"weights\"] is None:\n",
        "            kwargs[\"pretrained\"] = False\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"weights=={} not supported in torchvision {}\".format(\n",
        "                    kwargs[\"weights\"], torchvision.__version__\n",
        "                )\n",
        "            )\n",
        "        del kwargs[\"weights\"]\n",
        "\n",
        "    return torchvision.models.inception_v3(*args, **kwargs)\n",
        "\n",
        "\n",
        "def fid_inception_v3():\n",
        "    \"\"\"Build pretrained Inception model for FID computation\n",
        "\n",
        "    The Inception model for FID computation uses a different set of weights\n",
        "    and has a slightly different structure than torchvision's Inception.\n",
        "\n",
        "    This method first constructs torchvision's Inception and then patches the\n",
        "    necessary parts that are different in the FID Inception model.\n",
        "    \"\"\"\n",
        "    inception = _inception_v3(num_classes=1008, aux_logits=False, weights=None)\n",
        "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
        "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
        "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
        "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
        "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
        "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
        "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
        "\n",
        "    state_dict = load_state_dict_from_url(\"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\", progress=True)\n",
        "    inception.load_state_dict(state_dict)\n",
        "    return inception\n",
        "\n",
        "\n",
        "class FIDInceptionA(torchvision.models.inception.InceptionA):\n",
        "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, pool_features):\n",
        "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionC(torchvision.models.inception.InceptionC):\n",
        "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, channels_7x7):\n",
        "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_1(torchvision.models.inception.InceptionE):\n",
        "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n",
        "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: The FID Inception model uses max pooling instead of average\n",
        "        # pooling. This is likely an error in this specific Inception\n",
        "        # implementation, as other Inception models use average pooling here\n",
        "        # (which matches the description in the paper).\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgZPF8WdXoCZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC2LlVt4XoCa"
      },
      "outputs": [],
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,  # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3,  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_blocks=(DEFAULT_BLOCK_INDEX,),\n",
        "        resize_input=True,\n",
        "        normalize_input=True,\n",
        "        requires_grad=False,\n",
        "        use_fid_inception=True,\n",
        "    ):\n",
        "        \"\"\"Build pretrained InceptionV3\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_blocks : list of int\n",
        "            Indices of blocks to return features of. Possible values are:\n",
        "                - 0: corresponds to output of first max pooling\n",
        "                - 1: corresponds to output of second max pooling\n",
        "                - 2: corresponds to output which is fed to aux classifier\n",
        "                - 3: corresponds to output of final average pooling\n",
        "        resize_input : bool\n",
        "            If true, bilinearly resizes input to width and height 299 before\n",
        "            feeding input to model. As the network without fully connected\n",
        "            layers is fully convolutional, it should be able to handle inputs\n",
        "            of arbitrary size, so resizing might not be strictly needed\n",
        "        normalize_input : bool\n",
        "            If true, scales the input from range (0, 1) to the range the\n",
        "            pretrained Inception network expects, namely (-1, 1)\n",
        "        requires_grad : bool\n",
        "            If true, parameters of the model require gradients. Possibly useful\n",
        "            for finetuning the network\n",
        "        use_fid_inception : bool\n",
        "            If true, uses the pretrained Inception model used in Tensorflow's\n",
        "            FID implementation. If false, uses the pretrained Inception model\n",
        "            available in torchvision. The FID Inception model has different\n",
        "            weights and a slightly different structure from torchvision's\n",
        "            Inception model. If you want to compute FID scores, you are\n",
        "            strongly advised to set this parameter to true to get comparable\n",
        "            results.\n",
        "        \"\"\"\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \"Last possible output block index is 3\"\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        if use_fid_inception:\n",
        "            inception = fid_inception_v3()\n",
        "        else:\n",
        "            inception = _inception_v3(weights=\"DEFAULT\")\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tOV_I6AXoCc"
      },
      "source": [
        "#### Calculo da distribuicao gerada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snRXJWMYXoCc"
      },
      "outputs": [],
      "source": [
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36YTdjuXXoCc"
      },
      "outputs": [],
      "source": [
        "model = InceptionV3([block_idx]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIXrWCu6XoCd"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDvQF-u3XoCd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2Ia0an8XoCd"
      },
      "outputs": [],
      "source": [
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYgFFYEcXoCd"
      },
      "outputs": [],
      "source": [
        "class ImagePathDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, transforms=None):\n",
        "        self.files = files\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path = self.files[i]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3bQrqaXoCd"
      },
      "source": [
        "Por imagens geradas prontas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOUTdiayXoCe"
      },
      "outputs": [],
      "source": [
        "path = \"../imagens geradas/cgan_samples\"\n",
        "path = pathlib.Path(path)\n",
        "files = sorted(file for file in path.glob(\"*.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCrv8my3XoCe"
      },
      "outputs": [],
      "source": [
        "pred_arr = np.empty((len(files), dims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0LGyDp9XoCe"
      },
      "outputs": [],
      "source": [
        "dataset = ImagePathDataset(files, transforms=torchvision.transforms.ToTensor())\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtFXwOHQXoCe"
      },
      "source": [
        "Por modelo pre-treinado gerando imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiwYtDuvXoCe"
      },
      "outputs": [],
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(CGAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        #self._initialize_weights()\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # Itera sobre todos os módulos da rede geradora\n",
        "        for m in self.generator:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        if input.dim() == 2:\n",
        "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "            x = self.generator(z)\n",
        "            x = x.view(x.size(0), *self.img_shape) #Em\n",
        "            return x\n",
        "        elif input.dim() == 4:\n",
        "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UnF0P-uXoCf"
      },
      "outputs": [],
      "source": [
        "cgan = CGAN()\n",
        "cgan.load_state_dict(torch.load(\"CGAN_50epochs.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU6PYq0JXoCf"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "from datasets import Dataset, Features, ClassLabel\n",
        "from datasets import Image as IMG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YmuBLHNXoCf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GeneratedDataset(Dataset):\n",
        "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.model = type(self.generator).__name__\n",
        "        self.images = self.generate_data()\n",
        "        self.classes = [i for i in range(self.num_classes)]\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        gen_imgs = {}\n",
        "        self.generator.eval()\n",
        "        labels = {c: torch.tensor([c for i in range(self.num_samples)], device=self.device) for c in range(self.num_classes)}\n",
        "        for c, label in labels.items():\n",
        "          if self.model == 'Generator':\n",
        "              labels_one_hot = F.one_hot(label, self.num_classes).float().to(self.device) #\n",
        "          z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "          with torch.no_grad():\n",
        "              if self.model == 'Generator':\n",
        "                  gen_imgs_class = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
        "              elif self.model == 'CGAN':\n",
        "                  gen_imgs_class = self.generator(z, label)\n",
        "          gen_imgs[c] = gen_imgs_class\n",
        "\n",
        "        return gen_imgs\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples * self.num_classes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Mapear o índice global para (classe, índice interno)\n",
        "        class_idx = idx // self.num_samples\n",
        "        sample_idx = idx % self.num_samples\n",
        "        # Retorna apenas a imagem (sem o rótulo)\n",
        "        return self.images[class_idx][sample_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIh5F0qqXoCf"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 2048\n",
        "latent_dim = 100\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=cgan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "gen_dataset = generated_dataset.images\n",
        "\n",
        "for c in gen_dataset.keys():\n",
        "    gen_dataset[c] = (gen_dataset[c] + 1) / 2\n",
        "    gen_dataset[c] = gen_dataset[c].repeat(1, 3, 1, 1)\n",
        "# # Ajustar para o intervalo [0, 1]\n",
        "# gen_dataset = (gen_dataset + 1) / 2\n",
        "# Expandir o canal para RGB (replicando o canal 1 para 3)\n",
        "# gen_dataset = gen_dataset.repeat(1, 3, 1, 1)  # Agora tem shape [2050, 3, 28, 28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceinxLQKXoCf"
      },
      "outputs": [],
      "source": [
        "dataloaders = [torch.utils.data.DataLoader(gen_dataset[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwsjJ5fsXoCh"
      },
      "source": [
        "Calculo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQweQMlEXoCh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2x0DraxXoCi"
      },
      "outputs": [],
      "source": [
        "mus_gen = []\n",
        "sigmas_gen = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgtpBE95XoCl"
      },
      "outputs": [],
      "source": [
        "for c in range(10):\n",
        "  pred_arr = np.empty((len(gen_dataset[c]), dims))\n",
        "  start_idx = 0\n",
        "  for batch in tqdm(dataloaders[c]):\n",
        "          batch = batch.to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              pred = model(batch)[0]\n",
        "\n",
        "          # If model output is not scalar, apply global spatial average pooling.\n",
        "          # This happens if you choose a dimensionality not equal 2048.\n",
        "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "\n",
        "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
        "\n",
        "          start_idx = start_idx + pred.shape[0]\n",
        "  mus_gen.append(np.mean(pred_arr, axis=0))\n",
        "  sigmas_gen.append(np.cov(pred_arr, rowvar=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_w71nZ8XoCl"
      },
      "source": [
        "Calculo da distribuicao real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsx4vJ9BXoCm"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWEAlYaGXoCm"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
        "# Create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "trainloader_reduzido = torch.utils.data.DataLoader(trainset_reduzido, batch_size=128, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ODa5VTrXoCm"
      },
      "outputs": [],
      "source": [
        "testset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8js9LM5qXoCm"
      },
      "source": [
        "Pegando imagens sem salvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgwQDuAyXoCn"
      },
      "outputs": [],
      "source": [
        "def select_samples_per_class(dataset, num_samples):\n",
        "    \"\"\"\n",
        "    Selects a specified number of samples per class from the dataset and returns them as tensors.\n",
        "\n",
        "    Parameters:\n",
        "    dataset (torch.utils.data.Dataset): The dataset to select samples from.\n",
        "    num_samples (int): The number of samples to select per class.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where each key corresponds to a class and the value is a tensor of shape [num_samples, 1, 28, 28].\n",
        "    \"\"\"\n",
        "    class_samples = {i: [] for i in range(len(dataset.classes))}\n",
        "    class_counts = {i: 0 for i in range(len(dataset.classes))}\n",
        "\n",
        "    for img, label in dataset:\n",
        "        if class_counts[label] < num_samples:\n",
        "            class_samples[label].append(img)\n",
        "            class_counts[label] += 1\n",
        "        if all(count >= num_samples for count in class_counts.values()):\n",
        "            break\n",
        "    else:\n",
        "        print(\"Warning: Not all classes have the requested number of samples.\")\n",
        "\n",
        "    # Convert lists of tensors to a single tensor per class\n",
        "    for label in class_samples:\n",
        "        if class_samples[label]:  # Check if the list is not empty\n",
        "            class_samples[label] = torch.stack(class_samples[label], dim=0)\n",
        "            class_samples[label] = (class_samples[label] + 1) / 2\n",
        "            class_samples[label] = class_samples[label].repeat(1, 3, 1, 1)\n",
        "        else:\n",
        "            # Handle empty classes if necessary; here we leave an empty tensor\n",
        "            class_samples[label] = torch.Tensor()\n",
        "\n",
        "    return class_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxtmTmD6XoCn"
      },
      "outputs": [],
      "source": [
        "img_reais = select_samples_per_class(testset, 800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_r4SNe9XoCv"
      },
      "outputs": [],
      "source": [
        "dataloaders = [torch.utils.data.DataLoader(img_reais[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXFM_KcOXoCw"
      },
      "source": [
        "Salvando imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4HPBo8iXoCw"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRy47-hWXoCw"
      },
      "outputs": [],
      "source": [
        "# Function to save a random sample of images\n",
        "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    if classes is None:\n",
        "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
        "\n",
        "    if balanced:\n",
        "        # Get the number of classes\n",
        "        num_classes = len(classes)\n",
        "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
        "        indices = []\n",
        "        class_counts = {i: 0 for i in classes}\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        shuffled_indices = list(range(len(dataset)))\n",
        "        random.shuffle(shuffled_indices)\n",
        "\n",
        "        for idx in shuffled_indices:\n",
        "            img = dataset[idx][0]\n",
        "            label = int(dataset[idx][1])\n",
        "            if label in classes and class_counts[label] < samples_per_class:\n",
        "                indices.append(idx)\n",
        "                class_counts[label] += 1\n",
        "            if len(indices) >= num_samples:\n",
        "                break\n",
        "    else:\n",
        "        indices = []\n",
        "        while len(indices) < num_samples:\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            if int(dataset[idx][1]) in classes:\n",
        "                indices.append(idx)\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = dataset[idx]\n",
        "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
        "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhEt3LNsXoCx"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  save_random_samples(trainset, num_samples=2050, folder=f'Imagens Testes/mnist_samples_{i}', balanced=True, classes=[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXpIFPDtXoCx"
      },
      "outputs": [],
      "source": [
        "pathes = [f\"Imagens Testes/mnist_samples_{i}\" for i in range(10)]\n",
        "pathes = [pathlib.Path(path) for path in pathes]\n",
        "files = [sorted(file for file in path.glob(\"*.png\")) for path in pathes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U4U3uYZXoCx"
      },
      "outputs": [],
      "source": [
        "datasets = [ImagePathDataset(file, transforms=torchvision.transforms.ToTensor()) for file in files]\n",
        "dataloaders = [torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False) for dataset in datasets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVRjN1RbXoCx"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OHeQeC6XoCx"
      },
      "source": [
        "Calulo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeivOPmZXoCy"
      },
      "outputs": [],
      "source": [
        "mus_real = []\n",
        "sigmas_real = []\n",
        "for c in range(10):\n",
        "  model = InceptionV3([block_idx]).to(device)\n",
        "  model.eval()\n",
        "  pred_arr = np.empty((len(img_reais[0]), dims))\n",
        "  start_idx = 0\n",
        "  for batch in tqdm(dataloaders[c]):\n",
        "          batch = batch.to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              pred = model(batch)[0]\n",
        "\n",
        "          # If model output is not scalar, apply global spatial average pooling.\n",
        "          # This happens if you choose a dimensionality not equal 2048.\n",
        "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "\n",
        "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
        "\n",
        "          start_idx = start_idx + pred.shape[0]\n",
        "  mus_real.append(np.mean(pred_arr, axis=0))\n",
        "  sigmas_real.append(np.cov(pred_arr, rowvar=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXa0TM2lXoCy"
      },
      "source": [
        "Calculo FID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XtoaLpPXoCy"
      },
      "outputs": [],
      "source": [
        "from scipy import linalg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAGo9hN6XoCz"
      },
      "outputs": [],
      "source": [
        "mus_gen = [np.atleast_1d(mu_gen) for mu_gen in mus_gen]\n",
        "mus_real = [np.atleast_1d(mu_real) for mu_real in mus_real]\n",
        "\n",
        "sigmas_gen = [np.atleast_2d(sigma_gen) for sigma_gen in sigmas_gen]\n",
        "sigmas_real = [np.atleast_2d(sigma_real) for sigma_real in sigmas_real]\n",
        "\n",
        "for mu_gen, mu_real, sigma_gen, sigma_real in zip(mus_gen, mus_real, sigmas_gen, sigmas_real):\n",
        "  assert (\n",
        "      mu_gen.shape == mu_real.shape\n",
        "  ), \"Training and test mean vectors have different lengths\"\n",
        "  assert (\n",
        "      sigma_gen.shape == sigma_real.shape\n",
        "  ), \"Training and test covariances have different dimensions\"\n",
        "\n",
        "diffs = [mu_gen - mu_real for mu_gen, mu_real in zip(mus_gen, mus_real)]\n",
        "\n",
        "# Product might be almost singular\n",
        "covmeans = [linalg.sqrtm(sigmas_gen.dot(sigmas_real), disp=False)[0] for sigmas_gen, sigmas_real in zip(sigmas_gen, sigmas_real)]\n",
        "for covmean, sigma_gen, sigma_real in zip(covmeans, sigmas_gen, sigmas_real):\n",
        "  if not np.isfinite(covmean).all():\n",
        "    msg = (\n",
        "        \"fid calculation produces singular product; \"\n",
        "        \"adding %s to diagonal of cov estimates\"\n",
        "    ) % 1e-6\n",
        "    print(msg)\n",
        "    offset = np.eye(sigma_gen.shape[0]) * 1e-6\n",
        "    covmean = linalg.sqrtm((sigma_gen + offset).dot(sigma_real + offset))\n",
        "\n",
        "# Numerical error might give slight imaginary component\n",
        "for i, covmean in enumerate(covmeans):\n",
        "  if np.iscomplexobj(covmean):\n",
        "      if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "          m = np.max(np.abs(covmean.imag))\n",
        "          raise ValueError(\"Imaginary component {}\".format(m))\n",
        "      covmeans[i] = covmean.real\n",
        "\n",
        "tr_covmeans = [np.trace(covmean) for covmean in covmeans]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlYmUB0RXoCz"
      },
      "outputs": [],
      "source": [
        "fids = [diff.dot(diff) + np.trace(sigma_gen) + np.trace(sigma_real) - 2 * tr_covmean for diff, sigma_gen, sigma_real, tr_covmean in zip(diffs, sigmas_gen, sigmas_real, tr_covmeans)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UAzyULzXoCz"
      },
      "outputs": [],
      "source": [
        "fids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYo0rAs8XoCz"
      },
      "source": [
        "### Federado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CGAN duas classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Gen(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(Gen, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "    \n",
        "    def forward(self, input, labels):\n",
        "        z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "        x = self.generator(z)\n",
        "        x = x.view(x.size(0), *self.img_shape) #Em\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Disc(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(Disc, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "        return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jgO5dbyOX9IB",
        "outputId": "a3b2b562-b7db-4f9e-b6b9-306668fa6d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flwr_datasets\n",
            "  Downloading flwr_datasets-0.5.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting datasets<=3.1.0,>=2.14.6 (from flwr_datasets)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.7.5 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (1.26.4)\n",
            "Requirement already satisfied: seaborn<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (0.13.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets<=3.1.0,>=2.14.6->flwr_datasets) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1.31)\n",
            "Downloading flwr_datasets-0.5.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.0/87.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, flwr_datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 flwr_datasets-0.5.0 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flwr_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBvMjRnOXoCz"
      },
      "outputs": [],
      "source": [
        "from flwr_datasets.partitioner import DirichletPartitioner, IidPartitioner\n",
        "from flwr_datasets import FederatedDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "85htNladXoC0"
      },
      "outputs": [],
      "source": [
        "num_partitions = 4\n",
        "alpha_dir = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "R2Xjlca_XoC0"
      },
      "outputs": [],
      "source": [
        "partitioner = DirichletPartitioner(\n",
        "    num_partitions=num_partitions,\n",
        "    partition_by=\"label\",\n",
        "    alpha=alpha_dir,\n",
        "    min_partition_size=0,\n",
        "    self_balancing=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "partitioner = IidPartitioner(num_partitions=num_partitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "227p665iXoC0"
      },
      "outputs": [],
      "source": [
        "fds = FederatedDataset(\n",
        "    dataset=\"mnist\",\n",
        "    partitioners={\"train\": partitioner}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "46b96767f5e94b1dab5289f7ca493845",
            "fe0dac5b8f4b4b2db3b12745a0ea41bb",
            "4c559859b32d4c8e9ff5bdc179a3dbc2",
            "9fe53a5854df445dad0487dfc9035021",
            "7f3112f458a1497bafc6dfa40b3873f6",
            "5a673e78c8104c17b88cf65efddf4fcd",
            "2c7ed38f9c094e99b04b89a4121085f0",
            "1964b90585344c1880894924f1d058c5",
            "79be0691cc304fbc974e2463f055b5ad",
            "582503ab5a96473f974e7e68491f5ef2",
            "02ebbb4212b44d70b3c2860c8d98a1e1",
            "ecd6b36466d7420faa39384fbfff990a",
            "d7a1a9f475f9408e991033c95e4c2a7d",
            "928472df19414ba380a724a5d986c5f7",
            "3ede49350efb4a66bb9d9bd9c50d8402",
            "f1d0fd160d944dba80868e3024371e1b",
            "063a8820ba58452d88fdbc48049680cb",
            "456b4c7409c54888acd47a0c085b57de",
            "75abc3232c9a4e71aca3ac1cbc333e65",
            "28fa1055f85e42d584662c332df28cdb",
            "b5fadde8538c40399d0d6175992a2c08",
            "33c90b7d0dff423bb7bb6b6fe1bcd9f5",
            "c65cbcde25be4b67ad0972777fba7164",
            "856b2392293943a0a31ce86e48abf3b4",
            "bf453f0267b841bfbeb8dface06da913",
            "be0c4ae734eb405ab74533eb8ae6d677",
            "bfdd786d2c1640728fa4701f3de65680",
            "07f4993d5ce94b90896882f75058ad4a",
            "0c64263c0cbb4d6a8bb03085f87f806b",
            "5687f3380b0f4371aab7b74c96bf3eae",
            "50ba90dce2634ba7bd282722d41eb7d6",
            "2e4bf9df1c924c8fb2ac5da26778d5f0",
            "a8a6950ad39545de9aaab01e502b1c7e",
            "4f105781904045389b9f80fef1422204",
            "03fe67e2b82e46eda94c5735eb0b3838",
            "0c335ee394e5454688e169253ccc202a",
            "2184aff93e5e4ad4a2bcba4e1b352353",
            "d09cd624408c4b12bca2f98fe5cc9e4d",
            "38f5264ea38e490ab1730095821d5732",
            "b2c2ad47f5714749b1e233d731137a32",
            "71d3ef1e998342b6a909d098edbe5840",
            "eb53eb56d6814e3495324cd1720e403f",
            "bec86a3b96264fa4b742f12438b65c89",
            "9ad15303c12b4fed98d8179f75fa6c6d",
            "748f44fc54004f80b40dc65aa3092679",
            "fac0eb3e353641ce9aab2b90f03a75a1",
            "d8237488423e4f8aab99caecee63f0be",
            "e9655c60cd43415db6a81d0f688b2bf7",
            "2047512a3b8343ea9b84080d1355820b",
            "f1cf09270eb745ea8364b3e44acf920a",
            "518ce70cd0104e37a68d62e9b92c6fc0",
            "c7e04c9861094782b045c8eb510f954f",
            "317cc826243e41c6baea198d26fc8ea7",
            "006c513f6fd54337835cd1073b081b0a",
            "bed1ff516b5e4c33b68312f32fdd7a6f"
          ]
        },
        "id": "jw5CezaqXoC0",
        "outputId": "ce63cdcf-8d4d-48b2-962f-f906f96cda02"
      },
      "outputs": [],
      "source": [
        "train_partitions = [fds.load_partition(i, split=\"train\") for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ05wNS_ZSi3"
      },
      "source": [
        "##### Rodar proxima celula somente se quiser testar com dataset reduzido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "u-DfNUiSXoC0"
      },
      "outputs": [],
      "source": [
        "num_samples = [int(len(train_partition)/10) for train_partition in train_partitions]\n",
        "train_partitions = [train_partition.select(range(n)) for train_partition, n in zip(train_partitions, num_samples)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "BkqcHp6eXoC1"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "LIroIscqXoC1"
      },
      "outputs": [],
      "source": [
        "pytorch_transforms = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "nPXZ9XAjXoC1"
      },
      "outputs": [],
      "source": [
        "train_partitions = [train_partition.with_transform(apply_transforms) for train_partition in train_partitions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "kF7BB2O-XoC1"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "trainloaders = [DataLoader(train_partition, batch_size=batch_size, shuffle=True) for train_partition in train_partitions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "00vLeJqCXoC2"
      },
      "outputs": [],
      "source": [
        "models = [CGAN() for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [Disc() for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [Discriminator() for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3NZ6Ak66XoC4"
      },
      "outputs": [],
      "source": [
        "def generate_plot(net, device, round_number, client_id = None, examples_per_class: int=5, classes: int=10, latent_dim: int=100, server: bool=False):\n",
        "    \"\"\"Gera plot de imagens de cada classe\"\"\"\n",
        "    if server:\n",
        "        import matplotlib\n",
        "        matplotlib.use(\"Agg\")\n",
        "        import matplotlib.pyplot as plt\n",
        "    else:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    batch_size = examples_per_class * classes\n",
        "\n",
        "    latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
        "    labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_images = net(latent_vectors, labels).cpu()\n",
        "\n",
        "    # Criar uma figura com 10 linhas e 5 colunas de subplots\n",
        "    fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
        "\n",
        "    # Adiciona título no topo da figura\n",
        "    if client_id:\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
        "    else:\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "    # Exibir as imagens nos subplots\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    # Ajustar o layout antes de calcular as posições\n",
        "    plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "    # Reduzir espaço entre colunas\n",
        "    # plt.subplots_adjust(wspace=0.05)\n",
        "\n",
        "    # Adicionar os rótulos das classes corretamente alinhados\n",
        "    fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
        "    for row in range(classes):\n",
        "        # Obter posição do subplot em coordenadas da figura\n",
        "        bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "        pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "        center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "        # Adicionar o rótulo\n",
        "        fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
        "\n",
        "    fig.savefig(f\"mnist_CGAN_r{round_number}_f2a.png\")\n",
        "    plt.close(fig)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VZuTSmgYXoC4"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "X7L_24AOXoC5"
      },
      "outputs": [],
      "source": [
        "gen = CGAN().to(device)\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "# scheduler_D = torch.optim.lr_scheduler.StepLR(optim_D, step_size=5, gamma=0.9)\n",
        "# scheduler_G = torch.optim.lr_scheduler.StepLR(optim_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = Disc().to(device)\n",
        "optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Z8SN_A1oXoC5"
      },
      "outputs": [],
      "source": [
        "net = CGAN().to(device)\n",
        "optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "# scheduler_D = torch.optim.lr_scheduler.StepLR(optim_D, step_size=5, gamma=0.9)\n",
        "# scheduler_G = torch.optim.lr_scheduler.StepLR(optim_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen = Gen().to(device)\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "D = Discriminator().to(device)\n",
        "G = Generator().to(device)\n",
        "\n",
        "# Otimizadores\n",
        "#optim_D = torch.optim.Adam(D.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optim_G = torch.optim.Adam(G.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "# scheduler_D = torch.optim.lr_scheduler.StepLR(optim_D, step_size=5, gamma=0.9)\n",
        "# scheduler_G = torch.optim.lr_scheduler.StepLR(optim_G, step_size=5, gamma=0.9)\n",
        "\n",
        " # Função de perda Wasserstein\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return fake_output.mean() - real_output.mean()\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return -fake_output.mean()\n",
        "\n",
        "# Função para calcular Gradient Penalty\n",
        "def gradient_penalty(D, real_samples, fake_samples):\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
        "    interpolated = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "    d_interpolated = D(interpolated)\n",
        "    gradients = torch.autograd.grad(outputs=d_interpolated, inputs=interpolated,\n",
        "                                    grad_outputs=torch.ones_like(d_interpolated),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrMTGrznZNie"
      },
      "source": [
        "#### Treinamento Normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "350e6b1230c84b1a9724475a2d8cd14d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Treinamento:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹 Epoch 1/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "855d0234cfeb4fe6b20bb390bcf4a353",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 [100/469] loss_D_treino: 0.7172 loss_G_treino: 1.0355\n",
            "Epoch 0 [200/469] loss_D_treino: 0.5567 loss_G_treino: 1.0513\n",
            "Epoch 0 [300/469] loss_D_treino: 0.5390 loss_G_treino: 2.0415\n",
            "Epoch 0 [400/469] loss_D_treino: 0.5787 loss_G_treino: 1.0859\n",
            "\n",
            "🔹 Epoch 2/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd268badb2df405cad98649d4b4bed6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 [100/469] loss_D_treino: 0.4553 loss_G_treino: 1.3828\n",
            "Epoch 1 [200/469] loss_D_treino: 0.4056 loss_G_treino: 1.7168\n",
            "Epoch 1 [300/469] loss_D_treino: 0.4521 loss_G_treino: 1.1023\n",
            "Epoch 1 [400/469] loss_D_treino: 0.4181 loss_G_treino: 2.0259\n",
            "\n",
            "🔹 Epoch 3/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8f3364e3e694ab78458a950b52ac420",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 [100/469] loss_D_treino: 0.4117 loss_G_treino: 2.3528\n",
            "Epoch 2 [200/469] loss_D_treino: 0.4865 loss_G_treino: 2.4283\n",
            "Epoch 2 [300/469] loss_D_treino: 0.4765 loss_G_treino: 1.1877\n",
            "Epoch 2 [400/469] loss_D_treino: 0.3930 loss_G_treino: 1.3362\n",
            "\n",
            "🔹 Epoch 4/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bddcae3848b64bf29f052156140ada69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 [100/469] loss_D_treino: 0.3455 loss_G_treino: 1.4572\n",
            "Epoch 3 [200/469] loss_D_treino: 0.6152 loss_G_treino: 1.0659\n",
            "Epoch 3 [300/469] loss_D_treino: 0.5350 loss_G_treino: 1.2728\n",
            "Epoch 3 [400/469] loss_D_treino: 0.4074 loss_G_treino: 1.8394\n",
            "\n",
            "🔹 Epoch 5/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7fe07aa1a894123a4ee4775a6238c4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 [100/469] loss_D_treino: 0.4753 loss_G_treino: 1.1873\n",
            "Epoch 4 [200/469] loss_D_treino: 0.4004 loss_G_treino: 1.1077\n",
            "Epoch 4 [300/469] loss_D_treino: 0.4373 loss_G_treino: 1.4582\n",
            "Epoch 4 [400/469] loss_D_treino: 0.3777 loss_G_treino: 1.3204\n",
            "\n",
            "🔹 Epoch 6/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eacc44c7d4964fbb8a88a569ae07f5a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 [100/469] loss_D_treino: 0.3838 loss_G_treino: 1.2967\n",
            "Epoch 5 [200/469] loss_D_treino: 0.4419 loss_G_treino: 1.5179\n",
            "Epoch 5 [300/469] loss_D_treino: 0.5401 loss_G_treino: 2.0390\n",
            "Epoch 5 [400/469] loss_D_treino: 0.5145 loss_G_treino: 2.0209\n",
            "\n",
            "🔹 Epoch 7/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae65cd77a47b4d80ae5ce44aff3128e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 [100/469] loss_D_treino: 0.4938 loss_G_treino: 1.3961\n",
            "Epoch 6 [200/469] loss_D_treino: 0.5524 loss_G_treino: 1.5449\n",
            "Epoch 6 [300/469] loss_D_treino: 0.5913 loss_G_treino: 0.9399\n",
            "Epoch 6 [400/469] loss_D_treino: 0.5719 loss_G_treino: 1.4465\n",
            "\n",
            "🔹 Epoch 8/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f11fce69d38646598ccca03bf1aa8bfa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 [100/469] loss_D_treino: 0.5467 loss_G_treino: 1.4378\n",
            "Epoch 7 [200/469] loss_D_treino: 0.5788 loss_G_treino: 1.5506\n",
            "Epoch 7 [300/469] loss_D_treino: 0.5075 loss_G_treino: 1.6474\n",
            "Epoch 7 [400/469] loss_D_treino: 0.5595 loss_G_treino: 1.2396\n",
            "\n",
            "🔹 Epoch 9/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62e93158202347efb43623543e4ec2a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 [100/469] loss_D_treino: 0.6096 loss_G_treino: 1.4276\n",
            "Epoch 8 [200/469] loss_D_treino: 0.5336 loss_G_treino: 1.1822\n",
            "Epoch 8 [300/469] loss_D_treino: 0.5510 loss_G_treino: 1.5241\n",
            "Epoch 8 [400/469] loss_D_treino: 0.5710 loss_G_treino: 1.2522\n",
            "\n",
            "🔹 Epoch 10/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e69dfbf244a42e0b6fe2399c2482cf0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 [100/469] loss_D_treino: 0.5359 loss_G_treino: 1.3799\n",
            "Epoch 9 [200/469] loss_D_treino: 0.5499 loss_G_treino: 1.1218\n",
            "Epoch 9 [300/469] loss_D_treino: 0.6207 loss_G_treino: 1.0673\n",
            "Epoch 9 [400/469] loss_D_treino: 0.6025 loss_G_treino: 0.9652\n",
            "\n",
            "🔹 Epoch 11/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a06fe7e3b308445591272f9a70526bd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 [100/469] loss_D_treino: 0.5971 loss_G_treino: 1.0946\n",
            "Epoch 10 [200/469] loss_D_treino: 0.5841 loss_G_treino: 1.1708\n",
            "Epoch 10 [300/469] loss_D_treino: 0.5977 loss_G_treino: 0.9976\n",
            "Epoch 10 [400/469] loss_D_treino: 0.5852 loss_G_treino: 1.0277\n",
            "\n",
            "🔹 Epoch 12/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e23fd2c4831446296cef5b206ef010d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 [100/469] loss_D_treino: 0.6285 loss_G_treino: 1.2069\n",
            "Epoch 11 [200/469] loss_D_treino: 0.5550 loss_G_treino: 1.1061\n",
            "Epoch 11 [300/469] loss_D_treino: 0.5619 loss_G_treino: 1.2196\n",
            "Epoch 11 [400/469] loss_D_treino: 0.5816 loss_G_treino: 1.2839\n",
            "\n",
            "🔹 Epoch 13/30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97eb7a74fa0c4c46aac6bf1713d83ce2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m batch_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(trainloaders[\u001b[38;5;241m0\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 16\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2766\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m   2765\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2766\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2767\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m   2768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2747\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2746\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2747\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:407\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:522\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    520\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m    521\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m--> 522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[14], line 7\u001b[0m, in \u001b[0;36mapply_transforms\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_transforms\u001b[39m(batch):\n\u001b[1;32m----> 7\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mpytorch_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "wgan = False\n",
        "epochs = 30\n",
        "g_losses_batch = []\n",
        "d_losses_batch = []\n",
        "g_losses_epoch = []\n",
        "d_losses_epoch = []\n",
        "\n",
        "epoch_bar = tqdm(range(epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "for epoch in epoch_bar:\n",
        "    print(f\"\\n🔹 Epoch {epoch+1}/{epochs}\")\n",
        "    epoch_g_loss = 0.0\n",
        "    epoch_d_loss = 0.0\n",
        "    num_batches = 0\n",
        "    batch_bar = tqdm(enumerate(trainloaders[0]), desc=\"Batches\", leave=False, position=1)\n",
        "    start_time = time.time()\n",
        "    for batch_idx, batch in batch_bar:\n",
        "        images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "        fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "        z_noise = torch.randn(batch_size, 100, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "\n",
        "        optim_D.zero_grad()\n",
        "\n",
        "        if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            x_fake_labels = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "\n",
        "            # Adicionar labels ao images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = x_fake_labels.view(x_fake_labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            images = torch.cat([images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z_noise = torch.cat([z_noise, x_fake_labels], dim=1)\n",
        "            fake_images = G(z_noise).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            d_loss = discriminator_loss(D(images), D(fake_images)) + 10 * gradient_penalty(D, images, fake_images)\n",
        "\n",
        "        else:\n",
        "            # Train D\n",
        "            y_real = net(images, labels)\n",
        "            d_real_loss = net.loss(y_real, real_ident)\n",
        "            x_fake = gen(z_noise, x_fake_labels).detach()\n",
        "            y_fake_d = net(x_fake, x_fake_labels)\n",
        "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optim_D.step()\n",
        "\n",
        "        optim_G.zero_grad()\n",
        "\n",
        "        if wgan:\n",
        "            fake_images = G(z_noise)\n",
        "            g_loss = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
        "        else:\n",
        "            # Train G\n",
        "            x_fake = gen(z_noise, x_fake_labels)\n",
        "            y_fake_g = net(x_fake, x_fake_labels)\n",
        "            g_loss = gen.loss(y_fake_g, real_ident)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optim_G.step()\n",
        "\n",
        "        g_losses_batch.append(g_loss.item())\n",
        "        d_losses_batch.append(d_loss.item())\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
        "                        epoch, batch_idx, len(trainloaders[0]),\n",
        "                        d_loss.mean().item(),\n",
        "                        g_loss.mean().item()))\n",
        "\n",
        "    # Calcula médias da época\n",
        "    avg_g_loss = epoch_g_loss / num_batches\n",
        "    avg_d_loss = epoch_d_loss / num_batches\n",
        "\n",
        "    # Armazena as médias\n",
        "    g_losses_epoch.append(avg_g_loss)\n",
        "    d_losses_epoch.append(avg_d_loss)\n",
        "\n",
        "\n",
        "    figura = generate_plot(net=gen, device=device, round_number=epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqnZhxxUZaHM"
      },
      "source": [
        "#### Treinamento de Geradora única, após clientes treinarem Discriminadoras por todos os dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdhiGs-HXoC8"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     87\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(gen\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m \u001b[43moptim_G\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Acumula a perda\u001b[39;00m\n\u001b[1;32m     91\u001b[0m round_g_losses\u001b[38;5;241m.\u001b[39mappend(g_loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "g_losses = []  # Perda média do gerador por rodada\n",
        "d_losses = []  # Perda média do discriminador por rodada\n",
        "num_discriminator_epochs = 1  # Épocas de treino do discriminador por rodada\n",
        "num_generator_epochs = 50       # Épocas de treino do gerador por rodada\n",
        "rounds = 50\n",
        "\n",
        "for r in range(rounds):  # 100 rodadas federadas\n",
        "    # ========================================================================\n",
        "    # Treino dos Discriminadores (clientes)\n",
        "    # ========================================================================\n",
        "    round_d_losses = []  # Armazena as perdas dos discriminadores nesta rodada\n",
        "\n",
        "    for i, (net, trainloader) in enumerate(zip(models, trainloaders)):\n",
        "        net.to(device)\n",
        "        optim_D = optim_Ds[i]\n",
        "\n",
        "        for e in range(num_discriminator_epochs):  # Épocas locais\n",
        "            epoch_d_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            for batch in trainloader:\n",
        "                images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "                batch_size = images.size(0)\n",
        "                real_ident = torch.full((batch_size, 1), 1.0, device=device)\n",
        "                fake_ident = torch.full((batch_size, 1), 0.0, device=device)\n",
        "\n",
        "                # Treino do Discriminador\n",
        "                optim_D.zero_grad()\n",
        "\n",
        "                # Dados reais\n",
        "                y_real = net(images, labels)\n",
        "                d_real_loss = net.loss(y_real, real_ident)\n",
        "\n",
        "                # Dados falsos (gerados)\n",
        "                z_noise = torch.randn(batch_size, 100, device=device)\n",
        "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                x_fake = gen(z_noise, x_fake_labels).detach()  # Usa o gerador global\n",
        "                y_fake_d = net(x_fake, x_fake_labels)\n",
        "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "\n",
        "                # Loss total e backprop\n",
        "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                d_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(net.discriminator.parameters(), max_norm=1.0)\n",
        "                optim_D.step()\n",
        "\n",
        "                # Acumula a perda\n",
        "                epoch_d_loss += d_loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            # Média da perda do discriminador nesta época\n",
        "            epoch_d_loss /= num_batches\n",
        "            round_d_losses.append(epoch_d_loss)\n",
        "\n",
        "    # Média da perda dos discriminadores nesta rodada\n",
        "    avg_d_loss = sum(round_d_losses) / len(round_d_losses)\n",
        "    d_losses.append(avg_d_loss)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Treino do Gerador (global)\n",
        "    # ========================================================================\n",
        "    round_g_losses = []  # Armazena as perdas do gerador nesta rodada\n",
        "\n",
        "    for e in range(num_generator_epochs):  # Épocas do gerador\n",
        "        optim_G.zero_grad()\n",
        "\n",
        "        # Gera dados falsos\n",
        "        z_noise = torch.randn(32, 100, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (32,), device=device)\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "        real_ident = torch.full((32, 1), 1.0, device=device)\n",
        "        y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "        Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "\n",
        "        # Calcula a perda do gerador\n",
        "        y_fake_g = Dmax(x_fake, x_fake_labels)\n",
        "        g_loss = gen.loss(y_fake_g, real_ident)\n",
        "        g_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "        optim_G.step()\n",
        "\n",
        "        # Acumula a perda\n",
        "        round_g_losses.append(g_loss.item())\n",
        "\n",
        "    # Média da perda do gerador nesta rodada\n",
        "    avg_g_loss = sum(round_g_losses) / len(round_g_losses)\n",
        "    g_losses.append(avg_g_loss)\n",
        "\n",
        "    # Gera a figura (opcional)\n",
        "    figura = generate_plot(net=gen, device=device, round_number=r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgwSieOWZxdo"
      },
      "source": [
        "#### Treinamento de Geradora única, após cada batch em discriminadoras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n4jz6hpA2dKW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "zJPR2SUg12H2"
      },
      "outputs": [],
      "source": [
        "num_chunks = 5\n",
        "client_chunks = []\n",
        "for train_partition in train_partitions:\n",
        "  chunk_size = math.ceil(len(train_partition)/num_chunks)\n",
        "\n",
        "  chunks = []\n",
        "  for i in range(num_chunks):\n",
        "      start = i * chunk_size\n",
        "      end = min((i + 1) * chunk_size, len(train_partition))\n",
        "      chunks.append(Subset(train_partition, range(start, end)))\n",
        "\n",
        "  client_chunks.append(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'num_partitions' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models \u001b[38;5;241m=\u001b[39m [CGAN() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_partitions\u001b[49m)]\n\u001b[1;32m      2\u001b[0m optim_Ds \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.999\u001b[39m))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m gen \u001b[38;5;241m=\u001b[39m CGAN()\u001b[38;5;241m.\u001b[39mto(device)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_partitions' is not defined"
          ]
        }
      ],
      "source": [
        "models = [CGAN() for i in range(num_partitions)]\n",
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]\n",
        "gen = CGAN().to(device)\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5QYQs60Zwwt",
        "outputId": "f8afa083-4d5e-4a3d-b7d6-03ff46159d6d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7127a5055c4644aeaf2e67609bab7ac9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Treinamento:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "one_hot is only applicable to index tensor of type LongTensor.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[138], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m G(z_noise)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Seleciona o melhor discriminador (Dmax)\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m x_fake_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fake_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    109\u001b[0m image_fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview(x_fake_labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m    110\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([fake_images, image_fake_labels], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor of type LongTensor."
          ]
        }
      ],
      "source": [
        "wgan = True\n",
        "epochs = 20\n",
        "g_losses_chunk = []\n",
        "d_losses_chunk = []\n",
        "g_losses_round = []\n",
        "d_losses_round = []\n",
        "\n",
        "epoch_bar = tqdm(range(epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "\n",
        "batch_size_gen = 128\n",
        "batch_tam = 128\n",
        "extra_g_e = 150\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "  g_loss_c = 0.0\n",
        "  d_loss_c = 0.0\n",
        "  total_d_samples = 0  # Amostras totais processadas pelos discriminadores\n",
        "  total_g_samples = 0  # Amostras totais processadas pelo gerador\n",
        "\n",
        "  for chunk_idx in range(num_chunks):\n",
        "    # ====================================================================\n",
        "    # Treino dos Discriminadores (clientes) no bloco atual\n",
        "    # ====================================================================\n",
        "    d_loss_b = 0\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "    for i, (net, chunks) in enumerate(zip(models, client_chunks)):\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size = batch_tam, shuffle=True)\n",
        "\n",
        "      # Treinar o discriminador no bloco\n",
        "      net.to(device)\n",
        "      optim_D = optim_Ds[i]\n",
        "\n",
        "      for batch in chunk_loader:\n",
        "          images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "          batch_size = images.size(0)\n",
        "          if batch_size == 1:\n",
        "            continue\n",
        "          real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "          fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "          z_noise = torch.randn(batch_size, 128, device=device)\n",
        "          x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "\n",
        "          # Train D\n",
        "          optim_D.zero_grad()\n",
        "\n",
        "          if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            x_fake_labels = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "\n",
        "            # Adicionar labels ao images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = x_fake_labels.view(x_fake_labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            images = torch.cat([images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z_noise = torch.cat([z_noise, x_fake_labels], dim=1)\n",
        "            fake_images = G(z_noise).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            d_loss = discriminator_loss(net(images), net(fake_images)) + 10 * gradient_penalty(net, images, fake_images)\n",
        "          \n",
        "          else:\n",
        "            # Dados Reais\n",
        "            y_real = net(images, labels)\n",
        "            d_real_loss = net.loss(y_real, real_ident)\n",
        "\n",
        "            # Dados Falsos\n",
        "            z_noise = torch.randn(batch_size, 100, device=device)\n",
        "            x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "            x_fake = gen(z_noise, x_fake_labels).detach()\n",
        "            y_fake_d = net(x_fake, x_fake_labels)\n",
        "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "\n",
        "            # Loss total e backprop\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "        \n",
        "          d_loss.backward()\n",
        "          #torch.nn.utils.clip_grad_norm_(net.discriminator.parameters(), max_norm=1.0)\n",
        "          optim_D.step()\n",
        "          d_loss_b += d_loss.item()\n",
        "          total_chunk_samples += 1\n",
        "    # Média da perda dos discriminadores neste chunk\n",
        "    avg_d_loss_chunk = d_loss_b / total_chunk_samples if total_chunk_samples > 0 else 0.0\n",
        "    d_losses_chunk.append(avg_d_loss_chunk)\n",
        "    d_loss_c += avg_d_loss_chunk * total_chunk_samples\n",
        "    total_d_samples += total_chunk_samples\n",
        "\n",
        "    chunk_g_loss = 0.0\n",
        "    for g_epoch in range(extra_g_e):\n",
        "      # Train G\n",
        "      optim_G.zero_grad()\n",
        "\n",
        "      # Gera dados falsos\n",
        "      z_noise = torch.randn(batch_size_gen, 128, device=device)\n",
        "      x_fake_labels = torch.randint(0, 10, (batch_size_gen,), device=device)\n",
        "\n",
        "      if wgan:\n",
        "        x_fake_labels = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "        z_noise = torch.cat([z_noise, x_fake_labels], dim=1)\n",
        "        fake_images = G(z_noise)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        image_fake_labels = torch.view(x_fake_labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "        fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "        y_fake_gs = [model(fake_images.detach()) for model in models]\n",
        "\n",
        "      else:\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "\n",
        "      y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "      Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "\n",
        "      # Calcula a perda do gerador\n",
        "      real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "      if wgan:\n",
        "        y_fake_g = Dmax(fake_images)\n",
        "      \n",
        "      else:\n",
        "        y_fake_g = Dmax(x_fake, x_fake_labels)  # Detach explícito\n",
        "      g_loss = gen.loss(y_fake_g, real_ident)\n",
        "      g_loss.backward()\n",
        "      #torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "      optim_G.step()\n",
        "\n",
        "      chunk_g_loss += g_loss.item()\n",
        "    g_losses_chunk.append(chunk_g_loss / extra_g_e)\n",
        "    g_loss_c += chunk_g_loss /extra_g_e\n",
        "\n",
        "  g_loss_e = g_loss_c/num_chunks\n",
        "  d_loss_e = d_loss_c / total_d_samples if total_d_samples > 0 else 0.0\n",
        "\n",
        "  g_losses_round.append(g_loss_e)\n",
        "  d_losses_round.append(d_loss_e)\n",
        "\n",
        "  figura = generate_plot(net=gen, device=device, round_number=epoch)\n",
        "  print(f\"Época {epoch} completa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBWFA3seZ_-U"
      },
      "source": [
        "#### Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "38ycqSVjaT4q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "J0_nwm1SXoC9"
      },
      "outputs": [],
      "source": [
        "def loss_graph(g_losses: int, d_losses: int) -> None:\n",
        "    \"\"\"Funcao para gerar grafico de evolucao das perdas da geradora e discriminadora\"\"\"\n",
        "\n",
        "    # Número de iterações/épocas para cada lista\n",
        "    epochs_g = range(len(g_losses))  # Eixo x para o gerador\n",
        "    epochs_d = range(len(d_losses))  # Eixo x para o discriminador\n",
        "\n",
        "    # Criar o gráfico\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs_g, g_losses, label='Generator Loss')\n",
        "    plt.plot(epochs_d, d_losses, label='Discriminator Loss')\n",
        "\n",
        "    # Adicionar título e rótulos aos eixos\n",
        "    plt.title('Generator and Discriminator Losses Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Mostrar o gráfico\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "leK6Jt3Manph",
        "outputId": "dbf9d258-2d79-407f-f1e7-6416fec950df"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "loss_graph(g_losses=g_losses_round, d_losses=d_losses_round)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd2fQWWzXoC_"
      },
      "outputs": [],
      "source": [
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "gen.zero_grad()\n",
        "z_noise = torch.randn(128, 100, device=device)\n",
        "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
        "x_fake = gen(z_noise, x_fake_labels)\n",
        "y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "real_ident = torch.full((128, 1), 1., device=device)\n",
        "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "y_fake_g = Dmax(x_fake, x_fake_labels)\n",
        "g_loss = gen.loss(y_fake_g, real_ident)\n",
        "g_loss.backward()\n",
        "optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjrUZkoAXoDD"
      },
      "outputs": [],
      "source": [
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "gen.zero_grad()\n",
        "z_noise = torch.randn(128, 100, device=device)\n",
        "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
        "x_fake = gen(z_noise, x_fake_labels)\n",
        "y_fake_gs = [model(x_fake, x_fake_labels) for model in models]\n",
        "real_ident = torch.full((128, 1), 1., device=device)\n",
        "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "max_idx = y_fake_g_means.index(max(y_fake_g_means))\n",
        "g_loss = gen.loss(y_fake_gs[max_idx], real_ident)\n",
        "g_loss.backward()\n",
        "optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_mj5MokXoDM"
      },
      "outputs": [],
      "source": [
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.common import FitRes, Status, Code, ndarrays_to_parameters, parameters_to_ndarrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDEMtyGKXoDM"
      },
      "outputs": [],
      "source": [
        "params = [[val.cpu().numpy() for _, val in net.state_dict().items()] for net in models]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72r92dCPXoDM"
      },
      "outputs": [],
      "source": [
        "params_converted = [ndarrays_to_parameters(param) for param in params]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is3LfQQLXoDM"
      },
      "outputs": [],
      "source": [
        "results = [(i, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=param, num_examples=len(train_partition), metrics={})) for i, param, train_partition in zip(range(num_partitions), params_converted, train_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxj1CVhoXoDN"
      },
      "outputs": [],
      "source": [
        "from flwr.server.strategy.aggregate import aggregate_inplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvmLuOmPXoDN"
      },
      "outputs": [],
      "source": [
        "aggregated_ndarrays = aggregate_inplace(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-e-StinXoDN"
      },
      "outputs": [],
      "source": [
        "parameters_aggregated_gen = ndarrays_to_parameters(aggregated_ndarrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTzi5zSvXoDP"
      },
      "outputs": [],
      "source": [
        "# Cria uma instância do modelo\n",
        "model = CGAN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wj3iAxHXoDQ"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7ia1SieXoFS",
        "outputId": "bee573da-6d62-4eb0-9314-00c09c3d92f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = next(model.parameters()).device\n",
        "params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
        "state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
        "model.load_state_dict(state_dict, strict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDTkH5S6XoFT"
      },
      "outputs": [],
      "source": [
        "def train_G(net: CGAN, device: str, lr: float, epochs: int, batch_size: int, latent_dim: int):\n",
        "    net.to(device)  # move model to GPU if available\n",
        "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train G\n",
        "        net.zero_grad()\n",
        "        z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "        x_fake = net(z_noise, x_fake_labels)\n",
        "        y_fake_g = net(x_fake, x_fake_labels)\n",
        "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "        g_loss = net.loss(y_fake_g, real_ident)\n",
        "        g_loss.backward()\n",
        "        optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iwNpLftXoFT"
      },
      "outputs": [],
      "source": [
        "train_G(net=model,\n",
        "        device=device,\n",
        "        lr=0.0001,\n",
        "        epochs=2,\n",
        "        batch_size=128,\n",
        "        latent_dim=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoZsab-tXoFU",
        "outputId": "d9cc408c-5563-4d41-bd07-718c97ff8f99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "flwr.common.typing.Parameters"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(parameters_aggregated_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4knrQHgXoFV"
      },
      "outputs": [],
      "source": [
        "params = [val.cpu().numpy() for _, val in model.state_dict().items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZdf9fw0XoFV"
      },
      "outputs": [],
      "source": [
        "param = ndarrays_to_parameters(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnpXx2j_XoFV",
        "outputId": "dcffe647-d5a4-40ea-90b0-390885be8d28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "flwr.common.typing.Parameters"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(param)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gerafed",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006c513f6fd54337835cd1073b081b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ebbb4212b44d70b3c2860c8d98a1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03fe67e2b82e46eda94c5735eb0b3838": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38f5264ea38e490ab1730095821d5732",
            "placeholder": "​",
            "style": "IPY_MODEL_b2c2ad47f5714749b1e233d731137a32",
            "value": "Generating train split: 100%"
          }
        },
        "063a8820ba58452d88fdbc48049680cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f4993d5ce94b90896882f75058ad4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c335ee394e5454688e169253ccc202a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71d3ef1e998342b6a909d098edbe5840",
            "max": 60000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb53eb56d6814e3495324cd1720e403f",
            "value": 60000
          }
        },
        "0c64263c0cbb4d6a8bb03085f87f806b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1964b90585344c1880894924f1d058c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2047512a3b8343ea9b84080d1355820b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2184aff93e5e4ad4a2bcba4e1b352353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bec86a3b96264fa4b742f12438b65c89",
            "placeholder": "​",
            "style": "IPY_MODEL_9ad15303c12b4fed98d8179f75fa6c6d",
            "value": " 60000/60000 [00:01&lt;00:00, 53659.41 examples/s]"
          }
        },
        "28fa1055f85e42d584662c332df28cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c7ed38f9c094e99b04b89a4121085f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e4bf9df1c924c8fb2ac5da26778d5f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "317cc826243e41c6baea198d26fc8ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33c90b7d0dff423bb7bb6b6fe1bcd9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38f5264ea38e490ab1730095821d5732": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ede49350efb4a66bb9d9bd9c50d8402": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5fadde8538c40399d0d6175992a2c08",
            "placeholder": "​",
            "style": "IPY_MODEL_33c90b7d0dff423bb7bb6b6fe1bcd9f5",
            "value": " 15.6M/15.6M [00:00&lt;00:00, 22.7MB/s]"
          }
        },
        "456b4c7409c54888acd47a0c085b57de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46b96767f5e94b1dab5289f7ca493845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe0dac5b8f4b4b2db3b12745a0ea41bb",
              "IPY_MODEL_4c559859b32d4c8e9ff5bdc179a3dbc2",
              "IPY_MODEL_9fe53a5854df445dad0487dfc9035021"
            ],
            "layout": "IPY_MODEL_7f3112f458a1497bafc6dfa40b3873f6"
          }
        },
        "4c559859b32d4c8e9ff5bdc179a3dbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1964b90585344c1880894924f1d058c5",
            "max": 6971,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79be0691cc304fbc974e2463f055b5ad",
            "value": 6971
          }
        },
        "4f105781904045389b9f80fef1422204": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03fe67e2b82e46eda94c5735eb0b3838",
              "IPY_MODEL_0c335ee394e5454688e169253ccc202a",
              "IPY_MODEL_2184aff93e5e4ad4a2bcba4e1b352353"
            ],
            "layout": "IPY_MODEL_d09cd624408c4b12bca2f98fe5cc9e4d"
          }
        },
        "50ba90dce2634ba7bd282722d41eb7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "518ce70cd0104e37a68d62e9b92c6fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5687f3380b0f4371aab7b74c96bf3eae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "582503ab5a96473f974e7e68491f5ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a673e78c8104c17b88cf65efddf4fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71d3ef1e998342b6a909d098edbe5840": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "748f44fc54004f80b40dc65aa3092679": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fac0eb3e353641ce9aab2b90f03a75a1",
              "IPY_MODEL_d8237488423e4f8aab99caecee63f0be",
              "IPY_MODEL_e9655c60cd43415db6a81d0f688b2bf7"
            ],
            "layout": "IPY_MODEL_2047512a3b8343ea9b84080d1355820b"
          }
        },
        "75abc3232c9a4e71aca3ac1cbc333e65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79be0691cc304fbc974e2463f055b5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f3112f458a1497bafc6dfa40b3873f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "856b2392293943a0a31ce86e48abf3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07f4993d5ce94b90896882f75058ad4a",
            "placeholder": "​",
            "style": "IPY_MODEL_0c64263c0cbb4d6a8bb03085f87f806b",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "928472df19414ba380a724a5d986c5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75abc3232c9a4e71aca3ac1cbc333e65",
            "max": 15561616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28fa1055f85e42d584662c332df28cdb",
            "value": 15561616
          }
        },
        "9ad15303c12b4fed98d8179f75fa6c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fe53a5854df445dad0487dfc9035021": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_582503ab5a96473f974e7e68491f5ef2",
            "placeholder": "​",
            "style": "IPY_MODEL_02ebbb4212b44d70b3c2860c8d98a1e1",
            "value": " 6.97k/6.97k [00:00&lt;00:00, 154kB/s]"
          }
        },
        "a8a6950ad39545de9aaab01e502b1c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2c2ad47f5714749b1e233d731137a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5fadde8538c40399d0d6175992a2c08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be0c4ae734eb405ab74533eb8ae6d677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e4bf9df1c924c8fb2ac5da26778d5f0",
            "placeholder": "​",
            "style": "IPY_MODEL_a8a6950ad39545de9aaab01e502b1c7e",
            "value": " 2.60M/2.60M [00:00&lt;00:00, 38.1MB/s]"
          }
        },
        "bec86a3b96264fa4b742f12438b65c89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed1ff516b5e4c33b68312f32fdd7a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf453f0267b841bfbeb8dface06da913": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5687f3380b0f4371aab7b74c96bf3eae",
            "max": 2595890,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50ba90dce2634ba7bd282722d41eb7d6",
            "value": 2595890
          }
        },
        "bfdd786d2c1640728fa4701f3de65680": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c65cbcde25be4b67ad0972777fba7164": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_856b2392293943a0a31ce86e48abf3b4",
              "IPY_MODEL_bf453f0267b841bfbeb8dface06da913",
              "IPY_MODEL_be0c4ae734eb405ab74533eb8ae6d677"
            ],
            "layout": "IPY_MODEL_bfdd786d2c1640728fa4701f3de65680"
          }
        },
        "c7e04c9861094782b045c8eb510f954f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d09cd624408c4b12bca2f98fe5cc9e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a1a9f475f9408e991033c95e4c2a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063a8820ba58452d88fdbc48049680cb",
            "placeholder": "​",
            "style": "IPY_MODEL_456b4c7409c54888acd47a0c085b57de",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "d8237488423e4f8aab99caecee63f0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e04c9861094782b045c8eb510f954f",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_317cc826243e41c6baea198d26fc8ea7",
            "value": 10000
          }
        },
        "e9655c60cd43415db6a81d0f688b2bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_006c513f6fd54337835cd1073b081b0a",
            "placeholder": "​",
            "style": "IPY_MODEL_bed1ff516b5e4c33b68312f32fdd7a6f",
            "value": " 10000/10000 [00:00&lt;00:00, 19212.23 examples/s]"
          }
        },
        "eb53eb56d6814e3495324cd1720e403f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecd6b36466d7420faa39384fbfff990a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7a1a9f475f9408e991033c95e4c2a7d",
              "IPY_MODEL_928472df19414ba380a724a5d986c5f7",
              "IPY_MODEL_3ede49350efb4a66bb9d9bd9c50d8402"
            ],
            "layout": "IPY_MODEL_f1d0fd160d944dba80868e3024371e1b"
          }
        },
        "f1cf09270eb745ea8364b3e44acf920a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1d0fd160d944dba80868e3024371e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac0eb3e353641ce9aab2b90f03a75a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1cf09270eb745ea8364b3e44acf920a",
            "placeholder": "​",
            "style": "IPY_MODEL_518ce70cd0104e37a68d62e9b92c6fc0",
            "value": "Generating test split: 100%"
          }
        },
        "fe0dac5b8f4b4b2db3b12745a0ea41bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a673e78c8104c17b88cf65efddf4fcd",
            "placeholder": "​",
            "style": "IPY_MODEL_2c7ed38f9c094e99b04b89a4121085f0",
            "value": "README.md: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
