{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento modelo classificador e geradora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importacoes, carregamento dos dados, definicao da rede classificadora e treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainloader_reduzido = DataLoader(trainset_reduzido, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Salvando imagens MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a random sample of images\n",
    "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    if classes is None:\n",
    "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
    "    \n",
    "    if balanced:\n",
    "        # Get the number of classes\n",
    "        num_classes = len(classes)\n",
    "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
    "        indices = []\n",
    "        class_counts = {i: 0 for i in classes}\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        shuffled_indices = list(range(len(dataset)))\n",
    "        random.shuffle(shuffled_indices)\n",
    "        \n",
    "        for idx in shuffled_indices:\n",
    "            img = dataset[idx][0]\n",
    "            label = int(dataset[idx][1])\n",
    "            if label in classes and class_counts[label] < samples_per_class:\n",
    "                indices.append(idx)\n",
    "                class_counts[label] += 1\n",
    "            if len(indices) >= num_samples:\n",
    "                break\n",
    "    else:\n",
    "        indices = []\n",
    "        while len(indices) < num_samples:\n",
    "            idx = random.randint(0, len(dataset) - 1)\n",
    "            if int(dataset[idx][1]) in classes:\n",
    "                indices.append(idx)\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx]\n",
    "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
    "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
    "        img = Image.fromarray(img)\n",
    "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_random_samples(trainset, num_samples=2048, balanced=True, classes=[i], folder=filter\"Imagens Testes/mnist_samples_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definicao da GAN e funcoes de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN(nn.Module):\n",
    "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
    "        super(CGAN, self).__init__()\n",
    "        if dataset == \"mnist\":\n",
    "            self.classes = 10\n",
    "            self.channels = 1\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "        self.adv_loss = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
    "            *self._create_layer_gen(128, 256),\n",
    "            *self._create_layer_gen(256, 512),\n",
    "            *self._create_layer_gen(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
    "            *self._create_layer_disc(1024, 512, True, True),\n",
    "            *self._create_layer_disc(512, 256, True, True),\n",
    "            *self._create_layer_disc(256, 128, False, False),\n",
    "            *self._create_layer_disc(128, 1, False, False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(0.4))\n",
    "        if act_func:\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        if input.dim() == 2:\n",
    "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
    "            x = self.generator(z)\n",
    "            x = x.view(x.size(0), *self.img_shape) #Em\n",
    "            return x\n",
    "        elif input.dim() == 4:\n",
    "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)\n",
    "\n",
    "\n",
    "def train_gen(net, trainloader, epochs, lr, device, dataset=\"mnist\", latent_dim=100, f2a: bool = False, cliente: bool = False, D=None):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    if dataset == \"mnist\":\n",
    "      imagem = \"image\"\n",
    "    elif dataset == \"cifar10\":\n",
    "      imagem = \"img\"\n",
    "    \n",
    "    net.to(device)  # move model to GPU if available\n",
    "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "            if not f2a: \n",
    "                # Train G\n",
    "                net.zero_grad()\n",
    "                z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "                x_fake = net(z_noise, x_fake_labels)\n",
    "                y_fake_g = net(x_fake, x_fake_labels)\n",
    "                g_loss = net.loss(y_fake_g, real_ident)\n",
    "                g_loss.backward()\n",
    "                optim_G.step()\n",
    "\n",
    "                # Train D\n",
    "                net.zero_grad()\n",
    "                y_real = net(images, labels)\n",
    "                d_real_loss = net.loss(y_real, real_ident)\n",
    "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optim_D.step()\n",
    "\n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "\n",
    "                if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                    print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
    "                                epoch, batch_idx, len(trainloader),\n",
    "                                d_loss.mean().item(),\n",
    "                                g_loss.mean().item())) \n",
    "\n",
    "            else:\n",
    "                if cliente:\n",
    "                    # Train D\n",
    "                    net.zero_grad()\n",
    "                    y_real = net(images, labels)\n",
    "                    d_real_loss = net.loss(y_real, real_ident)\n",
    "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "                    x_fake = net(z_noise, x_fake_labels)\n",
    "                    y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "                    d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "                    d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                    d_loss.backward()\n",
    "                    optim_D.step()\n",
    "\n",
    "                    d_losses.append(d_loss.item())\n",
    "\n",
    "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                        print('Epoch {} [{}/{}] loss_D_treino: {:.4f}'.format(\n",
    "                                epoch, batch_idx, len(trainloader),\n",
    "                                d_loss.mean().item())) \n",
    "                    \n",
    "                else:\n",
    "                    # Train G\n",
    "                    net.zero_grad()\n",
    "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "                    x_fake = net(z_noise, x_fake_labels)\n",
    "                    y_fake_g = net(x_fake, x_fake_labels)\n",
    "                    g_loss = net.loss(y_fake_g, real_ident)\n",
    "                    g_loss.backward()\n",
    "                    optim_G.step()\n",
    "\n",
    "                    g_losses.append(g_loss.item())\n",
    "\n",
    "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                        print('Epoch {} [{}/{}] loss_G_treino: {:.4f}'.format(\n",
    "                                epoch, batch_idx, len(trainloader),\n",
    "                                g_loss.mean().item())) \n",
    "\n",
    "\n",
    "\n",
    "def test(net, testloader, device, dataset=\"mnist\", latent_dim=100):\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    if dataset == \"mnist\":\n",
    "      imagem = \"image\"\n",
    "    elif dataset == \"cifar10\":\n",
    "      imagem = \"img\"\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(testloader):\n",
    "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "            #Gen loss\n",
    "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "            x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "            x_fake = net(z_noise, x_fake_labels)\n",
    "            y_fake_g = net(x_fake, x_fake_labels)\n",
    "            g_loss = net.loss(y_fake_g, real_ident)\n",
    "\n",
    "            #Disc loss\n",
    "            y_real = net(images, labels)\n",
    "            d_real_loss = net.loss(y_real, real_ident)\n",
    "            y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            g_losses.append(g_loss.item())\n",
    "            d_losses.append(d_loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                print('[{}/{}] loss_D_teste: {:.4f} loss_G_teste: {:.4f}'.format(\n",
    "                            batch_idx, len(testloader),\n",
    "                            d_loss.mean().item(),\n",
    "                            g_loss.mean().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes\n",
    "LATENT_DIM = 128\n",
    "LEARNING_RATE = 0.0002\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.9\n",
    "GP_SCALE = 10\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 50\n",
    "# Camada de Convolu√ß√£o para o Discriminador\n",
    "def conv_block(in_channels, out_channels, kernel_size=5, stride=2, padding=2, use_bn=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "    if use_bn:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Discriminador\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(NUM_CHANNELS + NUM_CLASSES, 64, use_bn=False),\n",
    "            conv_block(64, 128, use_bn=True),\n",
    "            conv_block(128, 256, use_bn=True),\n",
    "            conv_block(256, 512, use_bn=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 2 * 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Camada de upsample para o Gerador\n",
    "def upsample_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_bn=True):\n",
    "    layers = [\n",
    "        nn.Upsample(scale_factor=2),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.BatchNorm2d(out_channels) if use_bn else nn.Identity(),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    ]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Gerador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + NUM_CLASSES, 4 * 4 * 256),\n",
    "            nn.BatchNorm1d(4 * 4 * 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Unflatten(1, (256, 4, 4)),\n",
    "            upsample_block(256, 128),\n",
    "            upsample_block(128, 64),\n",
    "            upsample_block(64, 32),\n",
    "            nn.Conv2d(32, NUM_CHANNELS, kernel_size=5, stride=1, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gera√ß√£o de Dados Sint√©ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
    "        self.generator = generator\n",
    "        self.num_samples = num_samples\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.model = type(self.generator).__name__\n",
    "        self.images, self.labels = self.generate_data()\n",
    "        self.classes = [i for i in range(self.num_classes)]\n",
    "        \n",
    "\n",
    "    def generate_data(self):\n",
    "        self.generator.eval()\n",
    "        labels = torch.tensor([i for i in range(self.num_classes) for _ in range(self.num_samples // self.num_classes)], device=self.device)\n",
    "        if self.model == 'Generator':\n",
    "            labels_one_hot = F.one_hot(labels, self.num_classes).float().to(self.device) #\n",
    "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            if self.model == 'Generator':\n",
    "                gen_imgs = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
    "            elif self.model == 'CGAN':\n",
    "                gen_imgs = self.generator(z, labels)\n",
    "\n",
    "        return gen_imgs.cpu(), labels.cpu()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 10000\n",
    "latent_dim = 128\n",
    "\n",
    "G = Generator(latent_dim=128).to(\"cpu\")\n",
    "G.load_state_dict(torch.load(\"wgan_43e_128b_0.0002lr.pth\", map_location=torch.device('cpu'))[\"generator\"])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "generated_dataset = GeneratedDataset(generator=G, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
    "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 60000\n",
    "latent_dim = 100 \n",
    "\n",
    "gan = CGAN()\n",
    "gan.load_state_dict(torch.load(\"Imagens Testes/FULL_FEDAVG/epochs20/model_round_10_mnist.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "generated_dataset = GeneratedDataset(generator=gan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
    "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando imagens CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_random_samples(generated_dataset, num_samples=2048, balanced=True, folder='Imagens Testes/cgan_samples_niid_0.7acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "net.train()\n",
    "for epoch in range(5):\n",
    "    for data in generated_dataloader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, loss = 0, 0.0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images = batch[0]\n",
    "        labels = batch[1]\n",
    "        outputs = net(images)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "        correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "accuracy = correct / len(testloader.dataset)\n",
    "loss = loss / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CGAN()\n",
    "train(net=net, \n",
    "      trainloader=trainloader, \n",
    "      epochs=50,\n",
    "      learning_rate=0.0001, \n",
    "      device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'CGAN_50epochs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelos\n",
    "D = Discriminator().to(device)\n",
    "G = Generator(latent_dim=LATENT_DIM).to(device)\n",
    "# Otimizadores\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    " \n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
    "\n",
    " # Fun√ß√£o de perda Wasserstein\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return fake_output.mean() - real_output.mean()\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -fake_output.mean()\n",
    "\n",
    "# Fun√ß√£o para calcular Gradient Penalty\n",
    "def gradient_penalty(D, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    interpolated = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolated = D(interpolated)\n",
    "    gradients = torch.autograd.grad(outputs=d_interpolated, inputs=interpolated,\n",
    "                                    grad_outputs=torch.ones_like(d_interpolated),\n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = CGAN(latent_dim=128).to(device)\n",
    "optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "historico_metricas = []\n",
    "wgan = True\n",
    "epoch_bar = tqdm(range(EPOCHS), desc=\"Treinamento\", leave=True, position=0)\n",
    "for epoch in epoch_bar:\n",
    "\n",
    "    print(f\"\\nüîπ Epoch {epoch+1}/{EPOCHS}\")\n",
    "    G_loss = 0\n",
    "    D_loss = 0\n",
    "    batches = 0\n",
    "\n",
    "    batch_bar = tqdm(trainloader_reduzido, desc=\"Batches\", leave=False, position=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for real_images, labels in batch_bar:\n",
    "        real_images = real_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch = real_images.size(0)\n",
    "        fake_labels = torch.randint(0, NUM_CLASSES, (batch,), device=device)\n",
    "        z = torch.randn(batch, LATENT_DIM).to(device)\n",
    "        optimizer_D.zero_grad() \n",
    "        if wgan:\n",
    "            labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
    "            fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
    "\n",
    "            # Adicionar labels ao real_images para treinamento do Discriminador\n",
    "            image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
    "            image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
    "        \n",
    "            real_images = torch.cat([real_images, image_labels], dim=1)\n",
    "\n",
    "            # Treinar Discriminador\n",
    "            z = torch.cat([z, fake_labels], dim=1)\n",
    "            fake_images = G(z).detach()\n",
    "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
    "\n",
    "            D(real_images)\n",
    "            loss_D = discriminator_loss(D(real_images), D(fake_images)) + GP_SCALE * gradient_penalty(D, real_images, fake_images)\n",
    "        \n",
    "        else:\n",
    "            real_ident = torch.full((batch, 1), 1., device=device)\n",
    "            fake_ident = torch.full((batch, 1), 0., device=device)\n",
    "            x_fake = gan(z, fake_labels)\n",
    "\n",
    "            y_real = gan(real_images, labels)\n",
    "            d_real_loss = gan.loss(y_real, real_ident)\n",
    "            y_fake_d = gan(x_fake.detach(), fake_labels)\n",
    "            d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
    "            loss_D = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # z = torch.randn(batch, LATENT_DIM).to(device)\n",
    "        # z = torch.cat([z, fake_labels], dim=1)\n",
    "        if wgan:\n",
    "            fake_images = G(z)\n",
    "            loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
    "        else:\n",
    "            y_fake_g = gan(x_fake, fake_labels)\n",
    "            loss_G = gan.loss(y_fake_g, real_ident)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        G_loss += loss_G.item()\n",
    "        D_loss += loss_D.item()\n",
    "        batches += BATCH_SIZE\n",
    "    \n",
    "    avg_epoch_G_loss = G_loss/batches\n",
    "    avg_epoch_D_loss = D_loss/batches\n",
    "    # Create the dataset and dataloader\n",
    "    if wgan:\n",
    "        generated_dataset = GeneratedDataset(generator=G, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
    "    else:\n",
    "        generated_dataset = GeneratedDataset(generator=gan, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
    "    generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    net = Net()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    net.train()\n",
    "    for _ in range(5):\n",
    "        for data in generated_dataloader:\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    net.eval()\n",
    "    correct, loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    epoch_bar.set_postfix({\n",
    "        \"D_loss\": f\"{avg_epoch_D_loss:.4f}\",\n",
    "        \"G_loss\": f\"{avg_epoch_G_loss:.4f}\",\n",
    "        \"Acc\": f\"{accuracy:.4f}\"\n",
    "    })\n",
    "\n",
    "    with open(\"Treino_GAN.txt\", \"a\") as f:\n",
    "            f.write(f\"Epoca: {epoch+1}, D_loss: {avg_epoch_D_loss:.4f}, G_loss: {avg_epoch_G_loss:.4f}, Acc: {accuracy:.4f}, Tempo: {total_time:.4f}\\n\")\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    #Atualiza o learning_rate\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "    print(f\"Ap√≥s Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
    "    if wgan:\n",
    "         # Salvar modelo a cada √©poca\n",
    "        torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, f\"wgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
    "    else:\n",
    "        torch.save(gan.state_dict(), f\"cgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
    "        \n",
    "print(\"‚úÖ Treinamento Conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, \"wgan_29e_64b_0.002lr.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ajuste de hiperparametro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import optuna\n",
    "from optuna.importance import get_param_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=\"cgan\"\n",
    "EPOCHS = 5\n",
    "# Fun√ß√£o Objetiva (a ser otimizada pelo Optuna)\n",
    "def objective(trial):\n",
    "    # Escolher os hiperpar√¢metros dentro de um intervalo\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 1024)\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 10, 1000)\n",
    "    lr = trial.suggest_float(\"learning_rate\", 0.0001, 0.05, log=True)\n",
    "    beta1 = trial.suggest_float(\"beta1\", 0.0, 0.9)\n",
    "    beta2 = trial.suggest_float(\"beta2\", 0.8, 0.999)\n",
    "    global model\n",
    "    model = model.lower()\n",
    "    if model==\"wgan\":\n",
    "        gp_scale = trial.suggest_int(\"gp_scale\", 0, 100)\n",
    "\n",
    "    # Criar DataLoader com batch_size otimizado\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Criar novos modelos e otimizadores\n",
    "    if model == \"wgan\":\n",
    "        D = Discriminator().to(device)\n",
    "        G = Generator(latent_dim=latent_dim).to(device)\n",
    "        optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "        G.train()\n",
    "        D.train()\n",
    "    elif model == \"cgan\":\n",
    "        gan = CGAN(latent_dim=latent_dim).to(device)\n",
    "        optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    \n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
    "    scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    # Treinar por algumas √©pocas\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_batches = 0\n",
    "        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for real_images, labels in progress_bar:\n",
    "            real_images = real_images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            fake_labels = torch.randint(0, NUM_CLASSES, (real_images.size(0),), device=device)\n",
    "            z = torch.randn(real_images.size(0), latent_dim).to(device)\n",
    "            optimizer_D.zero_grad()\n",
    "            if model==\"wgan\":\n",
    "                labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
    "                fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
    "\n",
    "                # Adicionar labels ao real_images para treinamento do Discriminador\n",
    "                image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
    "                image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
    "        \n",
    "                real_images = torch.cat([real_images, image_labels], dim=1)\n",
    "\n",
    "                # Treinar Discriminador\n",
    "                z = torch.cat([z, labels], dim=1)\n",
    "                fake_images = G(z).detach()\n",
    "                fake_images = torch.cat([fake_images, image_labels], dim=1)\n",
    "\n",
    "                loss_D = discriminator_loss(D(real_images), D(fake_images)) + gp_scale * gradient_penalty(D, real_images, fake_images)\n",
    "           \n",
    "           \n",
    "            else:\n",
    "                real_ident = torch.full((real_images.size(0), 1), 1., device=device)\n",
    "                fake_ident = torch.full((real_images.size(0), 1), 0., device=device)\n",
    "                x_fake = gan(z, fake_labels)\n",
    "\n",
    "                y_real = gan(real_images, labels)\n",
    "                d_real_loss = gan.loss(y_real, real_ident)\n",
    "                y_fake_d = gan(x_fake.detach(), fake_labels)\n",
    "                d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
    "                loss_D = (d_real_loss + d_fake_loss) / 2          \n",
    "           \n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "\n",
    "            # Treinar Gerador\n",
    "            optimizer_G.zero_grad()\n",
    "  \n",
    "            if model==\"wgan\":\n",
    "                fake_images = G(z)\n",
    "                loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
    "            else:\n",
    "                y_fake_g = gan(x_fake, fake_labels)\n",
    "                loss_G = gan.loss(y_fake_g, real_ident)\n",
    "            \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            epoch_loss += loss_G.item()\n",
    "            total_loss += loss_G.item()\n",
    "            total_batches += 1\n",
    "            epoch_batches += 1\n",
    "\n",
    "            progress_bar.set_postfix(d_loss=loss_D.item(), g_loss=loss_G.item())\n",
    "\n",
    "        # Calcular a loss m√©dia dessa √©poca\n",
    "        epoch_avg_loss = epoch_loss / epoch_batches\n",
    "        # Reporta a loss m√©dia da √©poca para pruning\n",
    "        trial.report(epoch_avg_loss, epoch)\n",
    "        if trial.should_prune() and epoch >=3:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "        print(f\"Ap√≥s Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
    "   \n",
    "            \n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "\n",
    "    return avg_loss  # Optuna tentar√° minimizar essa m√©trica\n",
    "\n",
    "# Criar estudo do Optuna e otimizar hiperpar√¢metros\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Exibir os melhores hiperpar√¢metros encontrados\n",
    "print(\"\\nüîπ Melhores Hiperpar√¢metros Encontrados:\")\n",
    "print(study.best_params)\n",
    "\n",
    "importance = get_param_importances(study)\n",
    "print(\"Hyperparameter Importances:\")\n",
    "for param, imp in importance.items():\n",
    "    print(f\"{param}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste para qualidade visual das imagens geradas pelo modelo generativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "latent_dim = 128\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "path = \"\"\n",
    "device = \"cpu\"\n",
    "net = CGAN(dataset=\"mnist\", latent_dim=latent_dim).to(device)\n",
    "net.load_state_dict(torch.load(f'{path}/model_round_5_mnist.pt'))\n",
    "# G = Generator(latent_dim=128)\n",
    "# G.load_state_dict(torch.load(\"wgan_5e_512b_0.002lr_0.5B1_0.9B2_10gp_128_ld.pth\")[\"generator\"])\n",
    "#net = CGAN()\n",
    "#net.load_state_dict(torch.load('CGAN_50epochs.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "#net.eval()\n",
    "G.eval()\n",
    "# Assuming netG is your generator model, classes is the total number of classes, and latent_dim is the latent vector size\n",
    "examples_per_class = 5\n",
    "classes = 10\n",
    "batch_size = examples_per_class * classes  # Generate enough images to have `examples_per_class` for each class\n",
    "\n",
    "# Generate latent vectors and corresponding labels\n",
    "latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
    "labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
    "labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    #generated_images = net(latent_vectors, labels)\n",
    "    generated_images = G(torch.cat([latent_vectors, labels], dim=1))\n",
    "\n",
    "# Criar uma figura com 10 linhas e 5 colunas de subplots\n",
    "fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
    "\n",
    "#fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "fig.text(0.5, 0.98, f\"Round: {5}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "# Exibir as imagens nos subplots\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Ajustar o layout antes de calcular as posi√ß√µes\n",
    "plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
    "\n",
    "# Reduzir espa√ßo entre colunas\n",
    "# plt.subplots_adjust(wspace=0.05)\n",
    "\n",
    "# Adicionar os r√≥tulos das classes corretamente alinhados\n",
    "fig.canvas.draw()  # Atualiza a renderiza√ß√£o para obter posi√ß√µes corretas\n",
    "for row in range(classes):\n",
    "    # Obter posi√ß√£o do subplot em coordenadas da figura\n",
    "    bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
    "    pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
    "    center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
    "\n",
    "    # Adicionar o r√≥tulo\n",
    "    fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
    "    plt.savefig(f\"{path}teste.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(image_files, output_path, duration=200):\n",
    "    \"\"\"\n",
    "    Cria um GIF animado a partir de uma sequ√™ncia de imagens.\n",
    "\n",
    "    Args:\n",
    "        image_files (list): Lista de caminhos das imagens.\n",
    "        output_path (str): Caminho para salvar o GIF.\n",
    "        duration (int): Tempo de exibi√ß√£o de cada frame (em ms).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    frames = [Image.open(img) for img in image_files]  # Carregar imagens\n",
    "    frames[0].save(output_path, format=\"GIF\", save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "    display(IPImage(filename=output_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "image_files = [\"../imagens geradas/mnist_CGAN_r0_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
    "               \"../imagens geradas/mnist_CGAN_r1_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
    "               \"../imagens geradas/mnist_CGAN_r2_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
    "               \"../imagens geradas/mnist_CGAN_r3_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
    "               \"../imagens geradas/mnist_CGAN_r4_100e_64_100z_10c_0.0001lr_niid_01dir.png\"]\n",
    "create_gif(image_files, \"global.gif\", duration=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_federated_collage(\n",
    "    agg_image_paths,       # Lista de caminhos para as imagens grandes (1 por round)\n",
    "    clients_image_paths,   # Lista de listas: para cada round, lista de caminhos de imagens de clientes\n",
    "    big_scale=2,           # Escala da imagem grande em rela√ß√£o √† imagem pequena\n",
    "    small_size=(500, 900),   # Tamanho desejado para cada imagem pequena (largura, altura)\n",
    "    h_gap=0,               # Espa√ßo horizontal entre bloco de imagens\n",
    "    background_color=(255, 255, 255),\n",
    "    save_path=\"collage.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cria um mosaico onde cada round tem:\n",
    "      - 1 imagem \"agregada\" (maior) √† esquerda\n",
    "      - N imagens de cliente empilhadas verticalmente √† direita\n",
    "\n",
    "    Par√¢metros:\n",
    "      agg_image_paths      : lista de strings (caminhos) para as imagens agregadas (1 por round)\n",
    "      clients_image_paths  : lista de listas de strings. Cada sublista √© a lista de caminhos das imagens de cada cliente daquele round\n",
    "      big_scale            : fator de escala da imagem grande em rela√ß√£o √†s pequenas\n",
    "      small_size           : (largura, altura) desejado para cada imagem pequena\n",
    "      background_color     : cor de fundo do mosaico (RGB)\n",
    "      save_path            : caminho do arquivo final a ser salvo\n",
    "\n",
    "    Retorna:\n",
    "      Um objeto PIL.Image com o mosaico criado.\n",
    "    \"\"\"\n",
    "    # Verifica se temos a mesma quantidade de rounds em agg_image_paths e clients_image_paths\n",
    "    assert len(agg_image_paths) == len(clients_image_paths), \\\n",
    "        \"N√∫mero de imagens agregadas deve bater com n√∫mero de listas de clientes.\"\n",
    "\n",
    "    # Carrega todas as imagens agregadas (rounds)\n",
    "    agg_images = []\n",
    "    for path in agg_image_paths:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        agg_images.append(img)\n",
    "\n",
    "    # Carrega todas as imagens de clientes\n",
    "    # clients_image_paths √© lista de listas, cada sublista para um round\n",
    "    client_images = []\n",
    "    for round_paths in clients_image_paths:\n",
    "        imgs = [Image.open(p).convert(\"RGB\") for p in round_paths]\n",
    "        client_images.append(imgs)\n",
    "\n",
    "    # Dimensiona as imagens pequenas para small_size\n",
    "    # e as grandes para (big_scale * small_size)\n",
    "    small_w, small_h = small_size\n",
    "    big_w, big_h = big_scale * small_w, big_scale * small_h\n",
    "\n",
    "    # Faz o resize de todas as imagens\n",
    "    for i, img in enumerate(agg_images):\n",
    "        agg_images[i] = img.resize((big_w, big_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "    for i, imgs in enumerate(client_images):\n",
    "        resized_list = []\n",
    "        for im in imgs:\n",
    "            resized_list.append(im.resize((small_w, small_h), Image.Resampling.LANCZOS))\n",
    "        client_images[i] = resized_list\n",
    "\n",
    "    # Calcula quantos rounds e quantos clientes\n",
    "    num_rounds = len(agg_images)\n",
    "\n",
    "    # Para cada round, vamos colocar:\n",
    "    # - Imagem grande (largura big_w, altura big_h)\n",
    "    # - N clientes empilhados (cada um small_h de altura, total N * small_h)\n",
    "    # A largura de cada \"bloco\" de round = (big_w + small_w)\n",
    "    # A altura do bloco = max(big_h, N * small_h) (para acomodar todas as imagens)\n",
    "\n",
    "    # Descobre o n√∫mero m√°ximo de clientes em qualquer round (para dimensionar corretamente)\n",
    "    max_clients = max(len(imgs) for imgs in client_images)\n",
    "\n",
    "    # Altura total do bloco para cada round\n",
    "    block_h = max(big_h + small_h, max_clients * small_h)\n",
    "    block_w = big_w + small_w  # Largura do bloco do round\n",
    "\n",
    "    # Largura total = num_rounds * block_w\n",
    "    # Altura total = block_h (vamos colocar rounds lado a lado)\n",
    "    total_w = num_rounds * block_w  + h_gap*2*num_rounds-1\n",
    "    total_h = block_h\n",
    "\n",
    "    # Cria imagem de fundo\n",
    "    collage = Image.new(\"RGB\", (total_w, total_h), color=background_color)\n",
    "\n",
    "    # Posiciona cada round\n",
    "    for r in range(num_rounds):\n",
    "        # Posi√ß√£o x para este round\n",
    "        x_offset = r * block_w + 2*r*h_gap\n",
    "\n",
    "        # Coloca a imagem grande (agg)\n",
    "        collage.paste(agg_images[r], (x_offset, small_h))\n",
    "\n",
    "        # Agora empilha as imagens de cliente ao lado (√† direita da imagem grande)\n",
    "        y_offset = 0\n",
    "        for c_img in client_images[r]:\n",
    "            collage.paste(c_img, (x_offset + big_w + h_gap, y_offset))\n",
    "            y_offset += small_h\n",
    "\n",
    "    # Salva o resultado\n",
    "    collage.save(save_path)\n",
    "    print(f\"Mosaico criado e salvo em: {save_path}\")\n",
    "\n",
    "    return collage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_image_paths = [f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir.png\" for i in range(10, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_image_paths = [[f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir_cliente{j}.png\" for j in range(4)] for i in range(11, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_federated_collage(\n",
    "    agg_image_paths=agg_image_paths,\n",
    "    clients_image_paths=client_image_paths,\n",
    "    big_scale=2,\n",
    "    small_size=(500, 900),\n",
    "    h_gap=80,\n",
    "    background_color=(255, 255, 255),\n",
    "    save_path=f\"{path}CGAN_evol.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importacoes, classes e configuracoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_cpus = os.cpu_count()\n",
    "num_workers = min(8, num_cpus,0)\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "dims = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inception_v3(*args, **kwargs):\n",
    "    \"\"\"Wraps `torchvision.models.inception_v3`\"\"\"\n",
    "    try:\n",
    "        version = tuple(map(int, torchvision.__version__.split(\".\")[:2]))\n",
    "    except ValueError:\n",
    "        # Just a caution against weird version strings\n",
    "        version = (0,)\n",
    "\n",
    "    # Skips default weight inititialization if supported by torchvision\n",
    "    # version. See https://github.com/mseitzer/pytorch-fid/issues/28.\n",
    "    if version >= (0, 6):\n",
    "        kwargs[\"init_weights\"] = False\n",
    "\n",
    "    # Backwards compatibility: `weights` argument was handled by `pretrained`\n",
    "    # argument prior to version 0.13.\n",
    "    if version < (0, 13) and \"weights\" in kwargs:\n",
    "        if kwargs[\"weights\"] == \"DEFAULT\":\n",
    "            kwargs[\"pretrained\"] = True\n",
    "        elif kwargs[\"weights\"] is None:\n",
    "            kwargs[\"pretrained\"] = False\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"weights=={} not supported in torchvision {}\".format(\n",
    "                    kwargs[\"weights\"], torchvision.__version__\n",
    "                )\n",
    "            )\n",
    "        del kwargs[\"weights\"]\n",
    "\n",
    "    return torchvision.models.inception_v3(*args, **kwargs)\n",
    "\n",
    "\n",
    "def fid_inception_v3():\n",
    "    \"\"\"Build pretrained Inception model for FID computation\n",
    "\n",
    "    The Inception model for FID computation uses a different set of weights\n",
    "    and has a slightly different structure than torchvision's Inception.\n",
    "\n",
    "    This method first constructs torchvision's Inception and then patches the\n",
    "    necessary parts that are different in the FID Inception model.\n",
    "    \"\"\"\n",
    "    inception = _inception_v3(num_classes=1008, aux_logits=False, weights=None)\n",
    "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
    "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
    "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
    "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
    "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
    "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
    "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
    "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
    "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
    "\n",
    "    state_dict = load_state_dict_from_url(\"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\", progress=True)\n",
    "    inception.load_state_dict(state_dict)\n",
    "    return inception\n",
    "\n",
    "\n",
    "class FIDInceptionA(torchvision.models.inception.InceptionA):\n",
    "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, pool_features):\n",
    "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(\n",
    "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
    "        )\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionC(torchvision.models.inception.InceptionC):\n",
    "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, channels_7x7):\n",
    "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(\n",
    "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
    "        )\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionE_1(torchvision.models.inception.InceptionE):\n",
    "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(\n",
    "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
    "        )\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n",
    "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        # Patch: The FID Inception model uses max pooling instead of average\n",
    "        # pooling. This is likely an error in this specific Inception\n",
    "        # implementation, as other Inception models use average pooling here\n",
    "        # (which matches the description in the paper).\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
    "\n",
    "    # Index of default block of inception to return,\n",
    "    # corresponds to output of final average pooling\n",
    "    DEFAULT_BLOCK_INDEX = 3\n",
    "\n",
    "    # Maps feature dimensionality to their output blocks indices\n",
    "    BLOCK_INDEX_BY_DIM = {\n",
    "        64: 0,  # First max pooling features\n",
    "        192: 1,  # Second max pooling featurs\n",
    "        768: 2,  # Pre-aux classifier features\n",
    "        2048: 3,  # Final average pooling features\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_blocks=(DEFAULT_BLOCK_INDEX,),\n",
    "        resize_input=True,\n",
    "        normalize_input=True,\n",
    "        requires_grad=False,\n",
    "        use_fid_inception=True,\n",
    "    ):\n",
    "        \"\"\"Build pretrained InceptionV3\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_blocks : list of int\n",
    "            Indices of blocks to return features of. Possible values are:\n",
    "                - 0: corresponds to output of first max pooling\n",
    "                - 1: corresponds to output of second max pooling\n",
    "                - 2: corresponds to output which is fed to aux classifier\n",
    "                - 3: corresponds to output of final average pooling\n",
    "        resize_input : bool\n",
    "            If true, bilinearly resizes input to width and height 299 before\n",
    "            feeding input to model. As the network without fully connected\n",
    "            layers is fully convolutional, it should be able to handle inputs\n",
    "            of arbitrary size, so resizing might not be strictly needed\n",
    "        normalize_input : bool\n",
    "            If true, scales the input from range (0, 1) to the range the\n",
    "            pretrained Inception network expects, namely (-1, 1)\n",
    "        requires_grad : bool\n",
    "            If true, parameters of the model require gradients. Possibly useful\n",
    "            for finetuning the network\n",
    "        use_fid_inception : bool\n",
    "            If true, uses the pretrained Inception model used in Tensorflow's\n",
    "            FID implementation. If false, uses the pretrained Inception model\n",
    "            available in torchvision. The FID Inception model has different\n",
    "            weights and a slightly different structure from torchvision's\n",
    "            Inception model. If you want to compute FID scores, you are\n",
    "            strongly advised to set this parameter to true to get comparable\n",
    "            results.\n",
    "        \"\"\"\n",
    "        super(InceptionV3, self).__init__()\n",
    "\n",
    "        self.resize_input = resize_input\n",
    "        self.normalize_input = normalize_input\n",
    "        self.output_blocks = sorted(output_blocks)\n",
    "        self.last_needed_block = max(output_blocks)\n",
    "\n",
    "        assert self.last_needed_block <= 3, \"Last possible output block index is 3\"\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        if use_fid_inception:\n",
    "            inception = fid_inception_v3()\n",
    "        else:\n",
    "            inception = _inception_v3(weights=\"DEFAULT\")\n",
    "\n",
    "        # Block 0: input to maxpool1\n",
    "        block0 = [\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        ]\n",
    "        self.blocks.append(nn.Sequential(*block0))\n",
    "\n",
    "        # Block 1: maxpool1 to maxpool2\n",
    "        if self.last_needed_block >= 1:\n",
    "            block1 = [\n",
    "                inception.Conv2d_3b_1x1,\n",
    "                inception.Conv2d_4a_3x3,\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block1))\n",
    "\n",
    "        # Block 2: maxpool2 to aux classifier\n",
    "        if self.last_needed_block >= 2:\n",
    "            block2 = [\n",
    "                inception.Mixed_5b,\n",
    "                inception.Mixed_5c,\n",
    "                inception.Mixed_5d,\n",
    "                inception.Mixed_6a,\n",
    "                inception.Mixed_6b,\n",
    "                inception.Mixed_6c,\n",
    "                inception.Mixed_6d,\n",
    "                inception.Mixed_6e,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block2))\n",
    "\n",
    "        # Block 3: aux classifier to final avgpool\n",
    "        if self.last_needed_block >= 3:\n",
    "            block3 = [\n",
    "                inception.Mixed_7a,\n",
    "                inception.Mixed_7b,\n",
    "                inception.Mixed_7c,\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block3))\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"Get Inception feature maps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.autograd.Variable\n",
    "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
    "            range (0, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of torch.autograd.Variable, corresponding to the selected output\n",
    "        block, sorted ascending by index\n",
    "        \"\"\"\n",
    "        outp = []\n",
    "        x = inp\n",
    "\n",
    "        if self.resize_input:\n",
    "            x = F.interpolate(x, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
    "\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.output_blocks:\n",
    "                outp.append(x)\n",
    "\n",
    "            if idx == self.last_needed_block:\n",
    "                break\n",
    "\n",
    "        return outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculo da distribuicao gerada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3([block_idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePathDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files, transforms=None):\n",
    "        self.files = files\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por imagens geradas prontas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../imagens geradas/cgan_samples\"\n",
    "path = pathlib.Path(path)\n",
    "files = sorted(file for file in path.glob(\"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = np.empty((len(files), dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImagePathDataset(files, transforms=torchvision.transforms.ToTensor())\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por modelo pre-treinado gerando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN(nn.Module):\n",
    "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
    "        super(CGAN, self).__init__()\n",
    "        if dataset == \"mnist\":\n",
    "            self.classes = 10\n",
    "            self.channels = 1\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "        self.adv_loss = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
    "            *self._create_layer_gen(128, 256),\n",
    "            *self._create_layer_gen(256, 512),\n",
    "            *self._create_layer_gen(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
    "            *self._create_layer_disc(1024, 512, True, True),\n",
    "            *self._create_layer_disc(512, 256, True, True),\n",
    "            *self._create_layer_disc(256, 128, False, False),\n",
    "            *self._create_layer_disc(128, 1, False, False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        #self._initialize_weights()\n",
    "\n",
    "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(0.4))\n",
    "        if act_func:\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Itera sobre todos os m√≥dulos da rede geradora\n",
    "        for m in self.generator:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        if input.dim() == 2:\n",
    "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
    "            x = self.generator(z)\n",
    "            x = x.view(x.size(0), *self.img_shape) #Em\n",
    "            return x\n",
    "        elif input.dim() == 4:\n",
    "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan = CGAN()\n",
    "cgan.load_state_dict(torch.load(\"CGAN_50epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "from datasets import Dataset, Features, ClassLabel\n",
    "from datasets import Image as IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
    "        self.generator = generator\n",
    "        self.num_samples = num_samples\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.model = type(self.generator).__name__\n",
    "        self.images = self.generate_data()\n",
    "        self.classes = [i for i in range(self.num_classes)]\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "        gen_imgs = {}\n",
    "        self.generator.eval()\n",
    "        labels = {c: torch.tensor([c for i in range(self.num_samples)], device=self.device) for c in range(self.num_classes)}\n",
    "        for c, label in labels.items():\n",
    "          if self.model == 'Generator':\n",
    "              labels_one_hot = F.one_hot(label, self.num_classes).float().to(self.device) #\n",
    "          z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
    "          with torch.no_grad():\n",
    "              if self.model == 'Generator':\n",
    "                  gen_imgs_class = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
    "              elif self.model == 'CGAN':\n",
    "                  gen_imgs_class = self.generator(z, label)\n",
    "          gen_imgs[c] = gen_imgs_class\n",
    "\n",
    "        return gen_imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples * self.num_classes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Mapear o √≠ndice global para (classe, √≠ndice interno)\n",
    "        class_idx = idx // self.num_samples\n",
    "        sample_idx = idx % self.num_samples\n",
    "        # Retorna apenas a imagem (sem o r√≥tulo)\n",
    "        return self.images[class_idx][sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 2048\n",
    "latent_dim = 100\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "generated_dataset = GeneratedDataset(generator=cgan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
    "gen_dataset = generated_dataset.images\n",
    "\n",
    "for c in gen_dataset.keys():\n",
    "    gen_dataset[c] = (gen_dataset[c] + 1) / 2\n",
    "    gen_dataset[c] = gen_dataset[c].repeat(1, 3, 1, 1)\n",
    "# # Ajustar para o intervalo [0, 1]\n",
    "# gen_dataset = (gen_dataset + 1) / 2\n",
    "# Expandir o canal para RGB (replicando o canal 1 para 3)\n",
    "# gen_dataset = gen_dataset.repeat(1, 3, 1, 1)  # Agora tem shape [2050, 3, 28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [torch.utils.data.DataLoader(gen_dataset[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_gen = []\n",
    "sigmas_gen = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(10):\n",
    "  pred_arr = np.empty((len(gen_dataset[c]), dims))\n",
    "  start_idx = 0\n",
    "  for batch in tqdm(dataloaders[c]):\n",
    "          batch = batch.to(device)\n",
    "\n",
    "          with torch.no_grad():\n",
    "              pred = model(batch)[0]\n",
    "\n",
    "          # If model output is not scalar, apply global spatial average pooling.\n",
    "          # This happens if you choose a dimensionality not equal 2048.\n",
    "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "\n",
    "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
    "\n",
    "          start_idx = start_idx + pred.shape[0]\n",
    "  mus_gen.append(np.mean(pred_arr, axis=0))\n",
    "  sigmas_gen.append(np.cov(pred_arr, rowvar=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da distribuicao real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
    "# Create data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "trainloader_reduzido = torch.utils.data.DataLoader(trainset_reduzido, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegando imagens sem salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_samples_per_class(dataset, num_samples):\n",
    "    \"\"\"\n",
    "    Selects a specified number of samples per class from the dataset and returns them as tensors.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (torch.utils.data.Dataset): The dataset to select samples from.\n",
    "    num_samples (int): The number of samples to select per class.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where each key corresponds to a class and the value is a tensor of shape [num_samples, 1, 28, 28].\n",
    "    \"\"\"\n",
    "    class_samples = {i: [] for i in range(len(dataset.classes))}\n",
    "    class_counts = {i: 0 for i in range(len(dataset.classes))}\n",
    "\n",
    "    for img, label in dataset:\n",
    "        if class_counts[label] < num_samples:\n",
    "            class_samples[label].append(img)\n",
    "            class_counts[label] += 1\n",
    "        if all(count >= num_samples for count in class_counts.values()):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Warning: Not all classes have the requested number of samples.\")\n",
    "\n",
    "    # Convert lists of tensors to a single tensor per class\n",
    "    for label in class_samples:\n",
    "        if class_samples[label]:  # Check if the list is not empty\n",
    "            class_samples[label] = torch.stack(class_samples[label], dim=0)\n",
    "            class_samples[label] = (class_samples[label] + 1) / 2\n",
    "            class_samples[label] = class_samples[label].repeat(1, 3, 1, 1)\n",
    "        else:\n",
    "            # Handle empty classes if necessary; here we leave an empty tensor\n",
    "            class_samples[label] = torch.Tensor()\n",
    "\n",
    "    return class_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reais = select_samples_per_class(testset, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [torch.utils.data.DataLoader(img_reais[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a random sample of images\n",
    "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    if classes is None:\n",
    "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
    "\n",
    "    if balanced:\n",
    "        # Get the number of classes\n",
    "        num_classes = len(classes)\n",
    "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
    "        indices = []\n",
    "        class_counts = {i: 0 for i in classes}\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        shuffled_indices = list(range(len(dataset)))\n",
    "        random.shuffle(shuffled_indices)\n",
    "\n",
    "        for idx in shuffled_indices:\n",
    "            img = dataset[idx][0]\n",
    "            label = int(dataset[idx][1])\n",
    "            if label in classes and class_counts[label] < samples_per_class:\n",
    "                indices.append(idx)\n",
    "                class_counts[label] += 1\n",
    "            if len(indices) >= num_samples:\n",
    "                break\n",
    "    else:\n",
    "        indices = []\n",
    "        while len(indices) < num_samples:\n",
    "            idx = random.randint(0, len(dataset) - 1)\n",
    "            if int(dataset[idx][1]) in classes:\n",
    "                indices.append(idx)\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx]\n",
    "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
    "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
    "        img = Image.fromarray(img)\n",
    "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  save_random_samples(trainset, num_samples=2050, folder=f'Imagens Testes/mnist_samples_{i}', balanced=True, classes=[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathes = [f\"Imagens Testes/mnist_samples_{i}\" for i in range(10)]\n",
    "pathes = [pathlib.Path(path) for path in pathes]\n",
    "files = [sorted(file for file in path.glob(\"*.png\")) for path in pathes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ImagePathDataset(file, transforms=torchvision.transforms.ToTensor()) for file in files]\n",
    "dataloaders = [torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_real = []\n",
    "sigmas_real = []\n",
    "for c in range(10):\n",
    "  model = InceptionV3([block_idx]).to(device)\n",
    "  model.eval()\n",
    "  pred_arr = np.empty((len(img_reais[0]), dims))\n",
    "  start_idx = 0\n",
    "  for batch in tqdm(dataloaders[c]):\n",
    "          batch = batch.to(device)\n",
    "\n",
    "          with torch.no_grad():\n",
    "              pred = model(batch)[0]\n",
    "\n",
    "          # If model output is not scalar, apply global spatial average pooling.\n",
    "          # This happens if you choose a dimensionality not equal 2048.\n",
    "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "\n",
    "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
    "\n",
    "          start_idx = start_idx + pred.shape[0]\n",
    "  mus_real.append(np.mean(pred_arr, axis=0))\n",
    "  sigmas_real.append(np.cov(pred_arr, rowvar=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_gen = [np.atleast_1d(mu_gen) for mu_gen in mus_gen]\n",
    "mus_real = [np.atleast_1d(mu_real) for mu_real in mus_real]\n",
    "\n",
    "sigmas_gen = [np.atleast_2d(sigma_gen) for sigma_gen in sigmas_gen]\n",
    "sigmas_real = [np.atleast_2d(sigma_real) for sigma_real in sigmas_real]\n",
    "\n",
    "for mu_gen, mu_real, sigma_gen, sigma_real in zip(mus_gen, mus_real, sigmas_gen, sigmas_real):\n",
    "  assert (\n",
    "      mu_gen.shape == mu_real.shape\n",
    "  ), \"Training and test mean vectors have different lengths\"\n",
    "  assert (\n",
    "      sigma_gen.shape == sigma_real.shape\n",
    "  ), \"Training and test covariances have different dimensions\"\n",
    "\n",
    "diffs = [mu_gen - mu_real for mu_gen, mu_real in zip(mus_gen, mus_real)]\n",
    "\n",
    "# Product might be almost singular\n",
    "covmeans = [linalg.sqrtm(sigmas_gen.dot(sigmas_real), disp=False)[0] for sigmas_gen, sigmas_real in zip(sigmas_gen, sigmas_real)]\n",
    "for covmean, sigma_gen, sigma_real in zip(covmeans, sigmas_gen, sigmas_real):\n",
    "  if not np.isfinite(covmean).all():\n",
    "    msg = (\n",
    "        \"fid calculation produces singular product; \"\n",
    "        \"adding %s to diagonal of cov estimates\"\n",
    "    ) % 1e-6\n",
    "    print(msg)\n",
    "    offset = np.eye(sigma_gen.shape[0]) * 1e-6\n",
    "    covmean = linalg.sqrtm((sigma_gen + offset).dot(sigma_real + offset))\n",
    "\n",
    "# Numerical error might give slight imaginary component\n",
    "for i, covmean in enumerate(covmeans):\n",
    "  if np.iscomplexobj(covmean):\n",
    "      if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "          m = np.max(np.abs(covmean.imag))\n",
    "          raise ValueError(\"Imaginary component {}\".format(m))\n",
    "      covmeans[i] = covmean.real\n",
    "\n",
    "tr_covmeans = [np.trace(covmean) for covmean in covmeans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = [diff.dot(diff) + np.trace(sigma_gen) + np.trace(sigma_real) - 2 * tr_covmean for diff, sigma_gen, sigma_real, tr_covmean in zip(diffs, sigmas_gen, sigmas_real, tr_covmeans)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "from flwr_datasets import FederatedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 1\n",
    "alpha_dir = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioner = DirichletPartitioner(\n",
    "    num_partitions=num_partitions,\n",
    "    partition_by=\"label\",\n",
    "    alpha=alpha_dir,\n",
    "    min_partition_size=0,\n",
    "    self_balancing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FederatedDataset(\n",
    "    dataset=\"mnist\",\n",
    "    partitioners={\"train\": partitioner}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_partitions = [fds.load_partition(i, split=\"train\") for i in range(num_partitions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = [int(len(train_partition)/10) for train_partition in train_partitions]\n",
    "train_partitions = [train_partition.select(range(n)) for train_partition, n in zip(train_partitions, num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def apply_transforms(batch):\n",
    "    batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_partitions = [train_partition.with_transform(apply_transforms) for train_partition in train_partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "trainloaders = [DataLoader(train_partition, batch_size=batch_size, shuffle=True) for train_partition in train_partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [CGAN() for i in range(num_partitions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma total dos valores dos par√¢metros do gerador do modelo 0: 1751.9129284620285\n"
     ]
    }
   ],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    generator_params = [param.data.numpy() for param in model.generator.parameters()]\n",
    "    generator_params_sum = sum([param.sum() for param in generator_params])\n",
    "    print(f\"Soma total dos valores dos par√¢metros do gerador do modelo {idx}: {generator_params_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma total dos valores dos par√¢metros do gerador do modelo 0: 328.63008058443666\n",
      "Soma total dos valores dos par√¢metros do gerador do modelo 1: 237.89485466852784\n",
      "Soma total dos valores dos par√¢metros do gerador do modelo 2: 186.06141594797373\n",
      "Soma total dos valores dos par√¢metros do gerador do modelo 3: 222.37551382556558\n"
     ]
    }
   ],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    disc_params = [param.data.numpy() for param in model.discriminator.parameters()]\n",
    "    disc_params_sum = sum([param.sum() for param in disc_params])\n",
    "    print(f\"Soma total dos valores dos par√¢metros do gerador do modelo {idx}: {disc_params_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [100/134] loss_D_treino: 0.0004\n",
      "Epoch 1 [100/134] loss_D_treino: 0.0001\n",
      "Epoch 0 [100/156] loss_D_treino: 0.0004\n",
      "Epoch 1 [100/156] loss_D_treino: 0.0001\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for net, trainloader in zip(models, trainloaders):\n",
    "    train_gen(net=net,\n",
    "              trainloader=trainloader,\n",
    "              epochs=epochs,\n",
    "              lr=0.0001,\n",
    "              device=\"cpu\",\n",
    "              dataset=\"mnist\",\n",
    "              latent_dim=100,\n",
    "              f2a=True,\n",
    "              cliente=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(net, device, round_number, client_id = None, examples_per_class: int=5, classes: int=10, latent_dim: int=100, server: bool=False):\n",
    "    \"\"\"Gera plot de imagens de cada classe\"\"\"\n",
    "    if server:\n",
    "        import matplotlib\n",
    "        matplotlib.use(\"Agg\")\n",
    "        import matplotlib.pyplot as plt\n",
    "    else:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    net.to(device) \n",
    "    net.eval()\n",
    "    batch_size = examples_per_class * classes\n",
    "\n",
    "    latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
    "    labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_images = net(latent_vectors, labels).cpu()\n",
    "\n",
    "    # Criar uma figura com 10 linhas e 5 colunas de subplots\n",
    "    fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
    "\n",
    "    # Adiciona t√≠tulo no topo da figura\n",
    "    if client_id:\n",
    "        fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
    "    else:\n",
    "        fig.text(0.5, 0.98, f\"Round: {round_number-1}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "    # Exibir as imagens nos subplots\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Ajustar o layout antes de calcular as posi√ß√µes\n",
    "    plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
    "\n",
    "    # Reduzir espa√ßo entre colunas\n",
    "    # plt.subplots_adjust(wspace=0.05)\n",
    "\n",
    "    # Adicionar os r√≥tulos das classes corretamente alinhados\n",
    "    fig.canvas.draw()  # Atualiza a renderiza√ß√£o para obter posi√ß√µes corretas\n",
    "    for row in range(classes):\n",
    "        # Obter posi√ß√£o do subplot em coordenadas da figura\n",
    "        bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
    "        pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
    "        center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
    "\n",
    "        # Adicionar o r√≥tulo\n",
    "        fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
    "    \n",
    "    fig.savefig(f\"mnist_CGAN_r{round_number}_{1024}b_{0.0001}lr_f2a.png\")\n",
    "    plt.close(fig)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = CGAN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [100/469] loss_D_treino: 0.4136 loss_G_treino: 1.9666\n",
      "Epoch 0 [200/469] loss_D_treino: 0.4917 loss_G_treino: 1.0213\n",
      "Epoch 0 [300/469] loss_D_treino: 0.4204 loss_G_treino: 0.9824\n",
      "Epoch 0 [400/469] loss_D_treino: 0.4255 loss_G_treino: 0.9698\n",
      "Epoch 1 [100/469] loss_D_treino: 0.3060 loss_G_treino: 1.5628\n",
      "Epoch 1 [200/469] loss_D_treino: 0.1246 loss_G_treino: 2.1974\n",
      "Epoch 1 [300/469] loss_D_treino: 0.2710 loss_G_treino: 2.2661\n",
      "Epoch 1 [400/469] loss_D_treino: 0.3259 loss_G_treino: 1.3751\n",
      "Epoch 2 [100/469] loss_D_treino: 0.2997 loss_G_treino: 1.4453\n",
      "Epoch 2 [200/469] loss_D_treino: 0.3697 loss_G_treino: 2.0212\n",
      "Epoch 2 [300/469] loss_D_treino: 0.3079 loss_G_treino: 1.2303\n",
      "Epoch 2 [400/469] loss_D_treino: 0.2605 loss_G_treino: 2.4553\n",
      "Epoch 3 [100/469] loss_D_treino: 0.2927 loss_G_treino: 1.3952\n",
      "Epoch 3 [200/469] loss_D_treino: 1.0338 loss_G_treino: 5.7651\n",
      "Epoch 3 [300/469] loss_D_treino: 0.3748 loss_G_treino: 3.4611\n",
      "Epoch 3 [400/469] loss_D_treino: 0.1571 loss_G_treino: 2.2099\n",
      "Epoch 4 [100/469] loss_D_treino: 0.3317 loss_G_treino: 2.9111\n",
      "Epoch 4 [200/469] loss_D_treino: 0.2910 loss_G_treino: 2.7685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m d_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2766\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m   2765\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2766\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2767\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m   2768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2747\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2746\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2747\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:407\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:522\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    520\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m    521\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m--> 522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 7\u001b[0m, in \u001b[0;36mapply_transforms\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_transforms\u001b[39m(batch):\n\u001b[1;32m----> 7\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mpytorch_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\55199\\Mestrado\\gerafed_env312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.to(device)  # move model to GPU if available\n",
    "optim_G = torch.optim.Adam(net.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch_idx, batch in enumerate(trainloaders[0]):\n",
    "        images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        batch_size = images.size(0)\n",
    "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "        fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "        # Train G\n",
    "        net.zero_grad()\n",
    "        z_noise = torch.randn(batch_size, 100, device=device)\n",
    "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "        x_fake = net(z_noise, x_fake_labels)\n",
    "        y_fake_g = net(x_fake, x_fake_labels)\n",
    "        g_loss = net.loss(y_fake_g, real_ident)\n",
    "        g_loss.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "        # Train D\n",
    "        net.zero_grad()\n",
    "        y_real = net(images, labels)\n",
    "        d_real_loss = net.loss(y_real, real_ident)\n",
    "        y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "        d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        g_losses.append(g_loss.item())\n",
    "        d_losses.append(d_loss.item())\n",
    "\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
    "                        epoch, batch_idx, len(trainloaders[0]),\n",
    "                        d_loss.mean().item(),\n",
    "                        g_loss.mean().item())) \n",
    "    \n",
    "    figura = generate_plot(net=net, device=device, round_number=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/17] loss_D_treino: 0.0000 loss_G_treino: 0.0004\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(trainloaders[0]):\n",
    "    if batch_idx % 10 == 0 and batch_idx > 0:\n",
    "            print('[{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
    "                        batch_idx, len(trainloaders[0]),\n",
    "                        d_loss.mean().item(),\n",
    "                        g_loss.mean().item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m     optim_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# Acumula a perda\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     round_g_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mg_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# M√©dia da perda do gerador nesta rodada\u001b[39;00m\n\u001b[0;32m     92\u001b[0m avg_g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(round_g_losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(round_g_losses)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g_losses = []  # Perda m√©dia do gerador por rodada\n",
    "d_losses = []  # Perda m√©dia do discriminador por rodada\n",
    "num_discriminator_epochs = 1  # √âpocas de treino do discriminador por rodada\n",
    "num_generator_epochs = 50       # √âpocas de treino do gerador por rodada\n",
    "\n",
    "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optim_Ds = [\n",
    "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "for r in range(50):  # 100 rodadas federadas\n",
    "    # ========================================================================\n",
    "    # Treino dos Discriminadores (clientes)\n",
    "    # ========================================================================\n",
    "    round_d_losses = []  # Armazena as perdas dos discriminadores nesta rodada\n",
    "    \n",
    "    for i, (net, trainloader) in enumerate(zip(models, trainloaders)):\n",
    "        net.to(device)\n",
    "        optim_D = optim_Ds[i]\n",
    "        \n",
    "        for e in range(num_discriminator_epochs):  # √âpocas locais\n",
    "            epoch_d_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch in trainloader:\n",
    "                images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                batch_size = images.size(0)\n",
    "                real_ident = torch.full((batch_size, 1), 1.0, device=device)\n",
    "                fake_ident = torch.full((batch_size, 1), 0.0, device=device)\n",
    "\n",
    "                # Treino do Discriminador\n",
    "                net.zero_grad()\n",
    "                \n",
    "                # Dados reais\n",
    "                y_real = net(images, labels)\n",
    "                d_real_loss = net.loss(y_real, real_ident)\n",
    "                \n",
    "                # Dados falsos (gerados)\n",
    "                z_noise = torch.randn(batch_size, 100, device=device)\n",
    "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "                x_fake = gen(z_noise, x_fake_labels)  # Usa o gerador global\n",
    "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "                \n",
    "                # Loss total e backprop\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optim_D.step()\n",
    "                \n",
    "                # Acumula a perda\n",
    "                epoch_d_loss += d_loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            # M√©dia da perda do discriminador nesta √©poca\n",
    "            epoch_d_loss /= num_batches\n",
    "            round_d_losses.append(epoch_d_loss)\n",
    "    \n",
    "    # M√©dia da perda dos discriminadores nesta rodada\n",
    "    avg_d_loss = sum(round_d_losses) / len(round_d_losses)\n",
    "    d_losses.append(avg_d_loss)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Treino do Gerador (global)\n",
    "    # ========================================================================\n",
    "    round_g_losses = []  # Armazena as perdas do gerador nesta rodada\n",
    "    \n",
    "    for e in range(num_generator_epochs):  # √âpocas do gerador\n",
    "        gen.zero_grad()\n",
    "        \n",
    "        # Gera dados falsos\n",
    "        z_noise = torch.randn(32, 100, device=device)\n",
    "        x_fake_labels = torch.randint(0, 10, (32,), device=device)\n",
    "        x_fake = gen(z_noise, x_fake_labels)\n",
    "        \n",
    "        # Seleciona o melhor discriminador (Dmax)\n",
    "        y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
    "        real_ident = torch.full((32, 1), 1.0, device=device)\n",
    "        y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
    "        Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
    "        \n",
    "        # Calcula a perda do gerador\n",
    "        y_fake_g = Dmax(x_fake, x_fake_labels)\n",
    "        g_loss = gen.loss(y_fake_g, real_ident)\n",
    "        g_loss.backward()\n",
    "        optim_G.step()\n",
    "        \n",
    "        # Acumula a perda\n",
    "        round_g_losses.append(g_loss.item())\n",
    "    \n",
    "    # M√©dia da perda do gerador nesta rodada\n",
    "    avg_g_loss = sum(round_g_losses) / len(round_g_losses)\n",
    "    g_losses.append(avg_g_loss)\n",
    "\n",
    "    # Gera a figura (opcional)\n",
    "    figura = generate_plot(net=gen, device=device, round_number=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# N√∫mero de itera√ß√µes/√©pocas para cada lista\n",
    "epochs_g = range(len(g_losses))  # Eixo x para o gerador\n",
    "epochs_d = range(len(d_losses))  # Eixo x para o discriminador\n",
    "\n",
    "# Criar o gr√°fico\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_g, g_losses, label='Generator Loss')\n",
    "plt.plot(epochs_d, d_losses, label='Discriminator Loss')\n",
    "\n",
    "# Adicionar t√≠tulo e r√≥tulos aos eixos\n",
    "plt.title('Generator and Discriminator Losses Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar o gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "gen.zero_grad()\n",
    "z_noise = torch.randn(128, 100, device=device)\n",
    "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
    "x_fake = gen(z_noise, x_fake_labels)\n",
    "y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
    "real_ident = torch.full((128, 1), 1., device=device)\n",
    "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
    "Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
    "y_fake_g = Dmax(x_fake, x_fake_labels)\n",
    "g_loss = gen.loss(y_fake_g, real_ident)\n",
    "g_loss.backward()\n",
    "optim_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "gen.zero_grad()\n",
    "z_noise = torch.randn(128, 100, device=device)\n",
    "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
    "x_fake = gen(z_noise, x_fake_labels)\n",
    "y_fake_gs = [model(x_fake, x_fake_labels) for model in models]\n",
    "real_ident = torch.full((128, 1), 1., device=device)\n",
    "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
    "max_idx = y_fake_g_means.index(max(y_fake_g_means))\n",
    "g_loss = gen.loss(y_fake_gs[max_idx], real_ident)\n",
    "g_loss.backward()\n",
    "optim_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import FitRes, Status, Code, ndarrays_to_parameters, parameters_to_ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [[val.cpu().numpy() for _, val in net.state_dict().items()] for net in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_converted = [ndarrays_to_parameters(param) for param in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [(i, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=param, num_examples=len(train_partition), metrics={})) for i, param, train_partition in zip(range(num_partitions), params_converted, train_partitions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.server.strategy.aggregate import aggregate_inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_ndarrays = aggregate_inplace(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_aggregated_gen = ndarrays_to_parameters(aggregated_ndarrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma inst√¢ncia do modelo\n",
    "model = CGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
    "state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_G(net: CGAN, device: str, lr: float, epochs: int, batch_size: int, latent_dim: int):\n",
    "    net.to(device)  # move model to GPU if available\n",
    "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train G\n",
    "        net.zero_grad()\n",
    "        z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "        x_fake = net(z_noise, x_fake_labels)\n",
    "        y_fake_g = net(x_fake, x_fake_labels)\n",
    "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "        g_loss = net.loss(y_fake_g, real_ident)\n",
    "        g_loss.backward()\n",
    "        optim_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_G(net=model,\n",
    "        device=device,\n",
    "        lr=0.0001,\n",
    "        epochs=2,\n",
    "        batch_size=128,\n",
    "        latent_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flwr.common.typing.Parameters"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parameters_aggregated_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [val.cpu().numpy() for _, val in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = ndarrays_to_parameters(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flwr.common.typing.Parameters"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gerafed_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
