{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bewK05okXn9u"
      },
      "source": [
        "## Treinamento modelo classificador e geradora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vda6HdlXn90"
      },
      "source": [
        "### Centralizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcekFyKmXn90"
      },
      "source": [
        "#### Importacoes, carregamento dos dados, definicao da rede classificadora e treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yQq3sXpTXn92"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2HQ8Sih1Xn_c"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E7ZYEUmkXn_h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3JP3W75Xn_i"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*4*4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVt9PtdGXn_j"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxtofuOWXn_m"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
        "# Create data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainloader_reduzido = DataLoader(trainset_reduzido, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QISJ5_duXn_n"
      },
      "source": [
        "##### Salvando imagens MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON7L-wrUXn_o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpOrcDK2Xn_o"
      },
      "outputs": [],
      "source": [
        "# Function to save a random sample of images\n",
        "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    if classes is None:\n",
        "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
        "\n",
        "    if balanced:\n",
        "        # Get the number of classes\n",
        "        num_classes = len(classes)\n",
        "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
        "        indices = []\n",
        "        class_counts = {i: 0 for i in classes}\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        shuffled_indices = list(range(len(dataset)))\n",
        "        random.shuffle(shuffled_indices)\n",
        "\n",
        "        for idx in shuffled_indices:\n",
        "            img = dataset[idx][0]\n",
        "            label = int(dataset[idx][1])\n",
        "            if label in classes and class_counts[label] < samples_per_class:\n",
        "                indices.append(idx)\n",
        "                class_counts[label] += 1\n",
        "            if len(indices) >= num_samples:\n",
        "                break\n",
        "    else:\n",
        "        indices = []\n",
        "        while len(indices) < num_samples:\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            if int(dataset[idx][1]) in classes:\n",
        "                indices.append(idx)\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = dataset[idx]\n",
        "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
        "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoanL3q-Xn_p"
      },
      "outputs": [],
      "source": [
        "save_random_samples(trainset, num_samples=2048, balanced=True, classes=[i], folder=filter\"Imagens Testes/mnist_samples_{i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTkej1xYXn_p"
      },
      "source": [
        "#### Definicao da GAN e funcoes de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j9zuYSTUXn_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yihGlxjjXn_p"
      },
      "outputs": [],
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(CGAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        if input.dim() == 2:\n",
        "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "            x = self.generator(z)\n",
        "            x = x.view(x.size(0), *self.img_shape) #Em\n",
        "            return x\n",
        "        elif input.dim() == 4:\n",
        "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)\n",
        "\n",
        "\n",
        "def train_gen(net, trainloader, epochs, lr, device, dataset=\"mnist\", latent_dim=100, f2a: bool = False, cliente: bool = False, D=None):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    if dataset == \"mnist\":\n",
        "      imagem = \"image\"\n",
        "    elif dataset == \"cifar10\":\n",
        "      imagem = \"img\"\n",
        "\n",
        "    net.to(device)  # move model to GPU if available\n",
        "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, batch in enumerate(trainloader):\n",
        "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
        "            batch_size = images.size(0)\n",
        "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "            if not f2a:\n",
        "                # Train G\n",
        "                net.zero_grad()\n",
        "                z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                x_fake = net(z_noise, x_fake_labels)\n",
        "                y_fake_g = net(x_fake, x_fake_labels)\n",
        "                g_loss = net.loss(y_fake_g, real_ident)\n",
        "                g_loss.backward()\n",
        "                optim_G.step()\n",
        "\n",
        "                # Train D\n",
        "                net.zero_grad()\n",
        "                y_real = net(images, labels)\n",
        "                d_real_loss = net.loss(y_real, real_ident)\n",
        "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                d_loss.backward()\n",
        "                optim_D.step()\n",
        "\n",
        "                g_losses.append(g_loss.item())\n",
        "                d_losses.append(d_loss.item())\n",
        "\n",
        "                if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                    print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                d_loss.mean().item(),\n",
        "                                g_loss.mean().item()))\n",
        "\n",
        "            else:\n",
        "                if cliente:\n",
        "                    # Train D\n",
        "                    net.zero_grad()\n",
        "                    y_real = net(images, labels)\n",
        "                    d_real_loss = net.loss(y_real, real_ident)\n",
        "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                    x_fake = net(z_noise, x_fake_labels)\n",
        "                    y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "                    d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "                    d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                    d_loss.backward()\n",
        "                    optim_D.step()\n",
        "\n",
        "                    d_losses.append(d_loss.item())\n",
        "\n",
        "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                        print('Epoch {} [{}/{}] loss_D_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                d_loss.mean().item()))\n",
        "\n",
        "                else:\n",
        "                    # Train G\n",
        "                    net.zero_grad()\n",
        "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                    x_fake = net(z_noise, x_fake_labels)\n",
        "                    y_fake_g = net(x_fake, x_fake_labels)\n",
        "                    g_loss = net.loss(y_fake_g, real_ident)\n",
        "                    g_loss.backward()\n",
        "                    optim_G.step()\n",
        "\n",
        "                    g_losses.append(g_loss.item())\n",
        "\n",
        "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                        print('Epoch {} [{}/{}] loss_G_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                g_loss.mean().item()))\n",
        "\n",
        "\n",
        "\n",
        "def test(net, testloader, device, dataset=\"mnist\", latent_dim=100):\n",
        "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
        "    if dataset == \"mnist\":\n",
        "      imagem = \"image\"\n",
        "    elif dataset == \"cifar10\":\n",
        "      imagem = \"img\"\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(testloader):\n",
        "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
        "            batch_size = images.size(0)\n",
        "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "            #Gen loss\n",
        "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "            x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "            x_fake = net(z_noise, x_fake_labels)\n",
        "            y_fake_g = net(x_fake, x_fake_labels)\n",
        "            g_loss = net.loss(y_fake_g, real_ident)\n",
        "\n",
        "            #Disc loss\n",
        "            y_real = net(images, labels)\n",
        "            d_real_loss = net.loss(y_real, real_ident)\n",
        "            y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "            g_losses.append(g_loss.item())\n",
        "            d_losses.append(d_loss.item())\n",
        "\n",
        "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                print('[{}/{}] loss_D_teste: {:.4f} loss_G_teste: {:.4f}'.format(\n",
        "                            batch_idx, len(testloader),\n",
        "                            d_loss.mean().item(),\n",
        "                            g_loss.mean().item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUPN43l4Xn_r"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CXzJElIeXn_w"
      },
      "outputs": [],
      "source": [
        "# Configura√ß√µes\n",
        "LATENT_DIM = 128\n",
        "LEARNING_RATE = 0.0002\n",
        "BETA1 = 0.5\n",
        "BETA2 = 0.9\n",
        "GP_SCALE = 10\n",
        "NUM_CHANNELS = 1\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 50\n",
        "# Camada de Convolu√ß√£o para o Discriminador\n",
        "def conv_block(in_channels, out_channels, kernel_size=5, stride=2, padding=2, use_bn=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
        "    if use_bn:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Discriminador\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            conv_block(NUM_CHANNELS + NUM_CLASSES, 64, use_bn=False),\n",
        "            conv_block(64, 128, use_bn=True),\n",
        "            conv_block(128, 256, use_bn=True),\n",
        "            conv_block(256, 512, use_bn=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 2 * 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Camada de upsample para o Gerador\n",
        "def upsample_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_bn=True):\n",
        "    layers = [\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels) if use_bn else nn.Identity(),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    ]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Gerador\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + NUM_CLASSES, 4 * 4 * 256),\n",
        "            nn.BatchNorm1d(4 * 4 * 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Unflatten(1, (256, 4, 4)),\n",
        "            upsample_block(256, 128),\n",
        "            upsample_block(128, 64),\n",
        "            upsample_block(64, 32),\n",
        "            nn.Conv2d(32, NUM_CHANNELS, kernel_size=5, stride=1, padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fPY0beuXoBV"
      },
      "source": [
        "#### Gera√ß√£o de Dados Sint√©ticos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-l-6raaXoBW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GeneratedDataset(Dataset):\n",
        "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.model = type(self.generator).__name__\n",
        "        self.images, self.labels = self.generate_data()\n",
        "        self.classes = [i for i in range(self.num_classes)]\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        self.generator.eval()\n",
        "        labels = torch.tensor([i for i in range(self.num_classes) for _ in range(self.num_samples // self.num_classes)], device=self.device)\n",
        "        if self.model == 'Generator':\n",
        "            labels_one_hot = F.one_hot(labels, self.num_classes).float().to(self.device) #\n",
        "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "        with torch.no_grad():\n",
        "            if self.model == 'Generator':\n",
        "                gen_imgs = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
        "            elif self.model == 'CGAN':\n",
        "                gen_imgs = self.generator(z, labels)\n",
        "\n",
        "        return gen_imgs.cpu(), labels.cpu()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abkIz6SPXoBW"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cEGGzm2XoBW"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 10000\n",
        "latent_dim = 128\n",
        "\n",
        "G = Generator(latent_dim=128).to(\"cpu\")\n",
        "G.load_state_dict(torch.load(\"wgan_43e_128b_0.0002lr.pth\", map_location=torch.device('cpu'))[\"generator\"])\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=G, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGU4X9cFXoBX"
      },
      "source": [
        "##### CGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR8Apn1XXoBX"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 60000\n",
        "latent_dim = 100\n",
        "\n",
        "gan = CGAN()\n",
        "gan.load_state_dict(torch.load(\"Imagens Testes/FULL_FEDAVG/epochs20/model_round_10_mnist.pt\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "neterated_dataset = GeneratedDataset(generator=gan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9407SFvSXoBX"
      },
      "source": [
        "Salvando imagens CGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zEozbb5XoBX"
      },
      "outputs": [],
      "source": [
        "save_random_samples(generated_dataset, num_samples=2048, balanced=True, folder='Imagens Testes/cgan_samples_niid_0.7acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjkOWq3mXoBY"
      },
      "outputs": [],
      "source": [
        "net = Net()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "net.train()\n",
        "for epoch in range(5):\n",
        "    for data in generated_dataloader:\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSQ_NYB_XoBY"
      },
      "outputs": [],
      "source": [
        "correct, loss = 0, 0.0\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in testloader:\n",
        "        images = batch[0]\n",
        "        labels = batch[1]\n",
        "        outputs = net(images)\n",
        "        loss += criterion(outputs, labels).item()\n",
        "        correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "accuracy = correct / len(testloader.dataset)\n",
        "loss = loss / len(testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er2sYOlBXoBY"
      },
      "outputs": [],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFt_vSA_XoBY"
      },
      "source": [
        "#### Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jW6ykXwXoBZ"
      },
      "outputs": [],
      "source": [
        "net = CGAN()\n",
        "train(net=net,\n",
        "      trainloader=trainloader,\n",
        "      epochs=50,\n",
        "      learning_rate=0.0001,\n",
        "      device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOjPqx2oXoBZ"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), 'CGAN_50epochs.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCs7kMrjXoBZ"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xfvrkD1XoBa"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihvzrqgrXoBa"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "445r_BCSXoBa"
      },
      "outputs": [],
      "source": [
        "# Inicializar modelos\n",
        "D = Discriminator().to(device)\n",
        "G = Generator(latent_dim=LATENT_DIM).to(device)\n",
        "# Otimizadores\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
        "\n",
        " # Fun√ß√£o de perda Wasserstein\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return fake_output.mean() - real_output.mean()\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return -fake_output.mean()\n",
        "\n",
        "# Fun√ß√£o para calcular Gradient Penalty\n",
        "def gradient_penalty(D, real_samples, fake_samples):\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
        "    interpolated = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "    d_interpolated = D(interpolated)\n",
        "    gradients = torch.autograd.grad(outputs=d_interpolated, inputs=interpolated,\n",
        "                                    grad_outputs=torch.ones_like(d_interpolated),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0HlgddkXoBb"
      },
      "outputs": [],
      "source": [
        "gan = CGAN(latent_dim=128).to(device)\n",
        "optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUGhPPWsXoBb"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7M7HXKbXoBc"
      },
      "outputs": [],
      "source": [
        "# Treinamento\n",
        "historico_metricas = []\n",
        "wgan = True\n",
        "epoch_bar = tqdm(range(EPOCHS), desc=\"Treinamento\", leave=True, position=0)\n",
        "for epoch in epoch_bar:\n",
        "\n",
        "    print(f\"\\nüîπ Epoch {epoch+1}/{EPOCHS}\")\n",
        "    G_loss = 0\n",
        "    D_loss = 0\n",
        "    batches = 0\n",
        "\n",
        "    batch_bar = tqdm(trainloader_reduzido, desc=\"Batches\", leave=False, position=1)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for real_images, labels in batch_bar:\n",
        "        real_images = real_images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch = real_images.size(0)\n",
        "        fake_labels = torch.randint(0, NUM_CLASSES, (batch,), device=device)\n",
        "        z = torch.randn(batch, LATENT_DIM).to(device)\n",
        "        optimizer_D.zero_grad()\n",
        "        if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "            fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
        "\n",
        "            # Adicionar labels ao real_images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            real_images = torch.cat([real_images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z = torch.cat([z, fake_labels], dim=1)\n",
        "            fake_images = G(z).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            D(real_images)\n",
        "            loss_D = discriminator_loss(D(real_images), D(fake_images)) + GP_SCALE * gradient_penalty(D, real_images, fake_images)\n",
        "\n",
        "        else:\n",
        "            real_ident = torch.full((batch, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch, 1), 0., device=device)\n",
        "            x_fake = gan(z, fake_labels)\n",
        "\n",
        "            y_real = gan(real_images, labels)\n",
        "            d_real_loss = gan.loss(y_real, real_ident)\n",
        "            y_fake_d = gan(x_fake.detach(), fake_labels)\n",
        "            d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
        "            loss_D = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # z = torch.randn(batch, LATENT_DIM).to(device)\n",
        "        # z = torch.cat([z, fake_labels], dim=1)\n",
        "        if wgan:\n",
        "            fake_images = G(z)\n",
        "            loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
        "        else:\n",
        "            y_fake_g = gan(x_fake, fake_labels)\n",
        "            loss_G = gan.loss(y_fake_g, real_ident)\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        G_loss += loss_G.item()\n",
        "        D_loss += loss_D.item()\n",
        "        batches += BATCH_SIZE\n",
        "\n",
        "    avg_epoch_G_loss = G_loss/batches\n",
        "    avg_epoch_D_loss = D_loss/batches\n",
        "    # Create the dataset and dataloader\n",
        "    if wgan:\n",
        "        generated_dataset = GeneratedDataset(generator=G, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
        "    else:\n",
        "        generated_dataset = GeneratedDataset(generator=gan, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
        "    generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    net = Net()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "    net.train()\n",
        "    for _ in range(5):\n",
        "        for data in generated_dataloader:\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    net.eval()\n",
        "    correct, loss = 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images = batch[0]\n",
        "            labels = batch[1]\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    epoch_bar.set_postfix({\n",
        "        \"D_loss\": f\"{avg_epoch_D_loss:.4f}\",\n",
        "        \"G_loss\": f\"{avg_epoch_G_loss:.4f}\",\n",
        "        \"Acc\": f\"{accuracy:.4f}\"\n",
        "    })\n",
        "\n",
        "    with open(\"Treino_GAN.txt\", \"a\") as f:\n",
        "            f.write(f\"Epoca: {epoch+1}, D_loss: {avg_epoch_D_loss:.4f}, G_loss: {avg_epoch_G_loss:.4f}, Acc: {accuracy:.4f}, Tempo: {total_time:.4f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Atualiza o learning_rate\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "    print(f\"Ap√≥s Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
        "    if wgan:\n",
        "         # Salvar modelo a cada √©poca\n",
        "        torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, f\"wgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
        "    else:\n",
        "        torch.save(gan.state_dict(), f\"cgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
        "\n",
        "print(\"‚úÖ Treinamento Conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8n9eIqhXoCF"
      },
      "outputs": [],
      "source": [
        "torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, \"wgan_29e_64b_0.002lr.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyhBKXfHXoCI"
      },
      "source": [
        "##### Ajuste de hiperparametro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMuI-AbbXoCI"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8lgBKXqXoCI"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import optuna\n",
        "from optuna.importance import get_param_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89xbKVJMXoCJ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=\"cgan\"\n",
        "EPOCHS = 5\n",
        "# Fun√ß√£o Objetiva (a ser otimizada pelo Optuna)\n",
        "def objective(trial):\n",
        "    # Escolher os hiperpar√¢metros dentro de um intervalo\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 16, 1024)\n",
        "    latent_dim = trial.suggest_int(\"latent_dim\", 10, 1000)\n",
        "    lr = trial.suggest_float(\"learning_rate\", 0.0001, 0.05, log=True)\n",
        "    beta1 = trial.suggest_float(\"beta1\", 0.0, 0.9)\n",
        "    beta2 = trial.suggest_float(\"beta2\", 0.8, 0.999)\n",
        "    global model\n",
        "    model = model.lower()\n",
        "    if model==\"wgan\":\n",
        "        gp_scale = trial.suggest_int(\"gp_scale\", 0, 100)\n",
        "\n",
        "    # Criar DataLoader com batch_size otimizado\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Criar novos modelos e otimizadores\n",
        "    if model == \"wgan\":\n",
        "        D = Discriminator().to(device)\n",
        "        G = Generator(latent_dim=latent_dim).to(device)\n",
        "        optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "        optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "        G.train()\n",
        "        D.train()\n",
        "    elif model == \"cgan\":\n",
        "        gan = CGAN(latent_dim=latent_dim).to(device)\n",
        "        optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "        optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "    scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "    scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    # Treinar por algumas √©pocas\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_batches = 0\n",
        "        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for real_images, labels in progress_bar:\n",
        "            real_images = real_images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            fake_labels = torch.randint(0, NUM_CLASSES, (real_images.size(0),), device=device)\n",
        "            z = torch.randn(real_images.size(0), latent_dim).to(device)\n",
        "            optimizer_D.zero_grad()\n",
        "            if model==\"wgan\":\n",
        "                labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "                fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
        "\n",
        "                # Adicionar labels ao real_images para treinamento do Discriminador\n",
        "                image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "                image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "                real_images = torch.cat([real_images, image_labels], dim=1)\n",
        "\n",
        "                # Treinar Discriminador\n",
        "                z = torch.cat([z, labels], dim=1)\n",
        "                fake_images = G(z).detach()\n",
        "                fake_images = torch.cat([fake_images, image_labels], dim=1)\n",
        "\n",
        "                loss_D = discriminator_loss(D(real_images), D(fake_images)) + gp_scale * gradient_penalty(D, real_images, fake_images)\n",
        "\n",
        "\n",
        "            else:\n",
        "                real_ident = torch.full((real_images.size(0), 1), 1., device=device)\n",
        "                fake_ident = torch.full((real_images.size(0), 1), 0., device=device)\n",
        "                x_fake = gan(z, fake_labels)\n",
        "\n",
        "                y_real = gan(real_images, labels)\n",
        "                d_real_loss = gan.loss(y_real, real_ident)\n",
        "                y_fake_d = gan(x_fake.detach(), fake_labels)\n",
        "                d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
        "                loss_D = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "\n",
        "            # Treinar Gerador\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            if model==\"wgan\":\n",
        "                fake_images = G(z)\n",
        "                loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
        "            else:\n",
        "                y_fake_g = gan(x_fake, fake_labels)\n",
        "                loss_G = gan.loss(y_fake_g, real_ident)\n",
        "\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            epoch_loss += loss_G.item()\n",
        "            total_loss += loss_G.item()\n",
        "            total_batches += 1\n",
        "            epoch_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix(d_loss=loss_D.item(), g_loss=loss_G.item())\n",
        "\n",
        "        # Calcular a loss m√©dia dessa √©poca\n",
        "        epoch_avg_loss = epoch_loss / epoch_batches\n",
        "        # Reporta a loss m√©dia da √©poca para pruning\n",
        "        trial.report(epoch_avg_loss, epoch)\n",
        "        if trial.should_prune() and epoch >=3:\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        scheduler_G.step()\n",
        "        scheduler_D.step()\n",
        "        print(f\"Ap√≥s Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "\n",
        "    return avg_loss  # Optuna tentar√° minimizar essa m√©trica\n",
        "\n",
        "# Criar estudo do Optuna e otimizar hiperpar√¢metros\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Exibir os melhores hiperpar√¢metros encontrados\n",
        "print(\"\\nüîπ Melhores Hiperpar√¢metros Encontrados:\")\n",
        "print(study.best_params)\n",
        "\n",
        "importance = get_param_importances(study)\n",
        "print(\"Hyperparameter Importances:\")\n",
        "for param, imp in importance.items():\n",
        "    print(f\"{param}: {imp:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt2EiBMCXoCL"
      },
      "source": [
        "#### Teste para qualidade visual das imagens geradas pelo modelo generativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hdas7twXoCM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByyDvVxcXoCM"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "latent_dim = 128\n",
        "# random.seed(seed)\n",
        "# np.random.seed(seed)\n",
        "#torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "path = \"\"\n",
        "device = \"cpu\"\n",
        "net = CGAN(dataset=\"mnist\", latent_dim=latent_dim).to(device)\n",
        "net.load_state_dict(torch.load(f'{path}/model_round_5_mnist.pt'))\n",
        "# G = Generator(latent_dim=128)\n",
        "# G.load_state_dict(torch.load(\"wgan_5e_512b_0.002lr_0.5B1_0.9B2_10gp_128_ld.pth\")[\"generator\"])\n",
        "#net = CGAN()\n",
        "#net.load_state_dict(torch.load('CGAN_50epochs.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "#net.eval()\n",
        "G.eval()\n",
        "# Assuming netG is your generator model, classes is the total number of classes, and latent_dim is the latent vector size\n",
        "examples_per_class = 5\n",
        "classes = 10\n",
        "batch_size = examples_per_class * classes  # Generate enough images to have `examples_per_class` for each class\n",
        "\n",
        "# Generate latent vectors and corresponding labels\n",
        "latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
        "labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
        "labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "\n",
        "# Generate images\n",
        "with torch.no_grad():\n",
        "    #generated_images = net(latent_vectors, labels)\n",
        "    generated_images = G(torch.cat([latent_vectors, labels], dim=1))\n",
        "\n",
        "# Criar uma figura com 10 linhas e 5 colunas de subplots\n",
        "fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
        "\n",
        "#fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "fig.text(0.5, 0.98, f\"Round: {5}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "# Exibir as imagens nos subplots\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Ajustar o layout antes de calcular as posi√ß√µes\n",
        "plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "# Reduzir espa√ßo entre colunas\n",
        "# plt.subplots_adjust(wspace=0.05)\n",
        "\n",
        "# Adicionar os r√≥tulos das classes corretamente alinhados\n",
        "fig.canvas.draw()  # Atualiza a renderiza√ß√£o para obter posi√ß√µes corretas\n",
        "for row in range(classes):\n",
        "    # Obter posi√ß√£o do subplot em coordenadas da figura\n",
        "    bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "    pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "    center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "    # Adicionar o r√≥tulo\n",
        "    fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
        "    plt.savefig(f\"{path}teste.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtY4KCiWXoCO"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import Image as IPImage, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6PlY_TYXoCP"
      },
      "outputs": [],
      "source": [
        "def create_gif(image_files, output_path, duration=200):\n",
        "    \"\"\"\n",
        "    Cria um GIF animado a partir de uma sequ√™ncia de imagens.\n",
        "\n",
        "    Args:\n",
        "        image_files (list): Lista de caminhos das imagens.\n",
        "        output_path (str): Caminho para salvar o GIF.\n",
        "        duration (int): Tempo de exibi√ß√£o de cada frame (em ms).\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    frames = [Image.open(img) for img in image_files]  # Carregar imagens\n",
        "    frames[0].save(output_path, format=\"GIF\", save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
        "    display(IPImage(filename=output_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txiXYjHEXoCP"
      },
      "outputs": [],
      "source": [
        "# Exemplo de uso\n",
        "image_files = [\"../imagens geradas/mnist_CGAN_r0_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r1_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r2_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r3_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r4_100e_64_100z_10c_0.0001lr_niid_01dir.png\"]\n",
        "create_gif(image_files, \"global.gif\", duration=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra5PcELgXoCQ"
      },
      "outputs": [],
      "source": [
        "def create_federated_collage(\n",
        "    agg_image_paths,       # Lista de caminhos para as imagens grandes (1 por round)\n",
        "    clients_image_paths,   # Lista de listas: para cada round, lista de caminhos de imagens de clientes\n",
        "    big_scale=2,           # Escala da imagem grande em rela√ß√£o √† imagem pequena\n",
        "    small_size=(500, 900),   # Tamanho desejado para cada imagem pequena (largura, altura)\n",
        "    h_gap=0,               # Espa√ßo horizontal entre bloco de imagens\n",
        "    background_color=(255, 255, 255),\n",
        "    save_path=\"collage.png\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Cria um mosaico onde cada round tem:\n",
        "      - 1 imagem \"agregada\" (maior) √† esquerda\n",
        "      - N imagens de cliente empilhadas verticalmente √† direita\n",
        "\n",
        "    Par√¢metros:\n",
        "      agg_image_paths      : lista de strings (caminhos) para as imagens agregadas (1 por round)\n",
        "      clients_image_paths  : lista de listas de strings. Cada sublista √© a lista de caminhos das imagens de cada cliente daquele round\n",
        "      big_scale            : fator de escala da imagem grande em rela√ß√£o √†s pequenas\n",
        "      small_size           : (largura, altura) desejado para cada imagem pequena\n",
        "      background_color     : cor de fundo do mosaico (RGB)\n",
        "      save_path            : caminho do arquivo final a ser salvo\n",
        "\n",
        "    Retorna:\n",
        "      Um objeto PIL.Image com o mosaico criado.\n",
        "    \"\"\"\n",
        "    # Verifica se temos a mesma quantidade de rounds em agg_image_paths e clients_image_paths\n",
        "    assert len(agg_image_paths) == len(clients_image_paths), \\\n",
        "        \"N√∫mero de imagens agregadas deve bater com n√∫mero de listas de clientes.\"\n",
        "\n",
        "    # Carrega todas as imagens agregadas (rounds)\n",
        "    agg_images = []\n",
        "    for path in agg_image_paths:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        agg_images.append(img)\n",
        "\n",
        "    # Carrega todas as imagens de clientes\n",
        "    # clients_image_paths √© lista de listas, cada sublista para um round\n",
        "    client_images = []\n",
        "    for round_paths in clients_image_paths:\n",
        "        imgs = [Image.open(p).convert(\"RGB\") for p in round_paths]\n",
        "        client_images.append(imgs)\n",
        "\n",
        "    # Dimensiona as imagens pequenas para small_size\n",
        "    # e as grandes para (big_scale * small_size)\n",
        "    small_w, small_h = small_size\n",
        "    big_w, big_h = big_scale * small_w, big_scale * small_h\n",
        "\n",
        "    # Faz o resize de todas as imagens\n",
        "    for i, img in enumerate(agg_images):\n",
        "        agg_images[i] = img.resize((big_w, big_h), Image.Resampling.LANCZOS)\n",
        "\n",
        "    for i, imgs in enumerate(client_images):\n",
        "        resized_list = []\n",
        "        for im in imgs:\n",
        "            resized_list.append(im.resize((small_w, small_h), Image.Resampling.LANCZOS))\n",
        "        client_images[i] = resized_list\n",
        "\n",
        "    # Calcula quantos rounds e quantos clientes\n",
        "    num_rounds = len(agg_images)\n",
        "\n",
        "    # Para cada round, vamos colocar:\n",
        "    # - Imagem grande (largura big_w, altura big_h)\n",
        "    # - N clientes empilhados (cada um small_h de altura, total N * small_h)\n",
        "    # A largura de cada \"bloco\" de round = (big_w + small_w)\n",
        "    # A altura do bloco = max(big_h, N * small_h) (para acomodar todas as imagens)\n",
        "\n",
        "    # Descobre o n√∫mero m√°ximo de clientes em qualquer round (para dimensionar corretamente)\n",
        "    max_clients = max(len(imgs) for imgs in client_images)\n",
        "\n",
        "    # Altura total do bloco para cada round\n",
        "    block_h = max(big_h + small_h, max_clients * small_h)\n",
        "    block_w = big_w + small_w  # Largura do bloco do round\n",
        "\n",
        "    # Largura total = num_rounds * block_w\n",
        "    # Altura total = block_h (vamos colocar rounds lado a lado)\n",
        "    total_w = num_rounds * block_w  + h_gap*2*num_rounds-1\n",
        "    total_h = block_h\n",
        "\n",
        "    # Cria imagem de fundo\n",
        "    collage = Image.new(\"RGB\", (total_w, total_h), color=background_color)\n",
        "\n",
        "    # Posiciona cada round\n",
        "    for r in range(num_rounds):\n",
        "        # Posi√ß√£o x para este round\n",
        "        x_offset = r * block_w + 2*r*h_gap\n",
        "\n",
        "        # Coloca a imagem grande (agg)\n",
        "        collage.paste(agg_images[r], (x_offset, small_h))\n",
        "\n",
        "        # Agora empilha as imagens de cliente ao lado (√† direita da imagem grande)\n",
        "        y_offset = 0\n",
        "        for c_img in client_images[r]:\n",
        "            collage.paste(c_img, (x_offset + big_w + h_gap, y_offset))\n",
        "            y_offset += small_h\n",
        "\n",
        "    # Salva o resultado\n",
        "    collage.save(save_path)\n",
        "    print(f\"Mosaico criado e salvo em: {save_path}\")\n",
        "\n",
        "    return collage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoP558u6XoCR"
      },
      "outputs": [],
      "source": [
        "path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p8mhqvHXoCR"
      },
      "outputs": [],
      "source": [
        "agg_image_paths = [f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir.png\" for i in range(10, 20)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wWETQNqXoCS"
      },
      "outputs": [],
      "source": [
        "client_image_paths = [[f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir_cliente{j}.png\" for j in range(4)] for i in range(11, 21)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJztr1mtXoCT"
      },
      "outputs": [],
      "source": [
        "create_federated_collage(\n",
        "    agg_image_paths=agg_image_paths,\n",
        "    clients_image_paths=client_image_paths,\n",
        "    big_scale=2,\n",
        "    small_size=(500, 900),\n",
        "    h_gap=80,\n",
        "    background_color=(255, 255, 255),\n",
        "    save_path=f\"{path}CGAN_evol.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu8-avfIXoCT"
      },
      "source": [
        "### FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI2FOV2rXoCU"
      },
      "source": [
        "### Importacoes, classes e configuracoes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7zeFBY8XoCV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjVnx-3FXoCX"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "num_cpus = os.cpu_count()\n",
        "num_workers = min(8, num_cpus,0)\n",
        "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "dims = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz1K-r-OXoCY"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhtBYnxkXoCY"
      },
      "outputs": [],
      "source": [
        "def _inception_v3(*args, **kwargs):\n",
        "    \"\"\"Wraps `torchvision.models.inception_v3`\"\"\"\n",
        "    try:\n",
        "        version = tuple(map(int, torchvision.__version__.split(\".\")[:2]))\n",
        "    except ValueError:\n",
        "        # Just a caution against weird version strings\n",
        "        version = (0,)\n",
        "\n",
        "    # Skips default weight inititialization if supported by torchvision\n",
        "    # version. See https://github.com/mseitzer/pytorch-fid/issues/28.\n",
        "    if version >= (0, 6):\n",
        "        kwargs[\"init_weights\"] = False\n",
        "\n",
        "    # Backwards compatibility: `weights` argument was handled by `pretrained`\n",
        "    # argument prior to version 0.13.\n",
        "    if version < (0, 13) and \"weights\" in kwargs:\n",
        "        if kwargs[\"weights\"] == \"DEFAULT\":\n",
        "            kwargs[\"pretrained\"] = True\n",
        "        elif kwargs[\"weights\"] is None:\n",
        "            kwargs[\"pretrained\"] = False\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"weights=={} not supported in torchvision {}\".format(\n",
        "                    kwargs[\"weights\"], torchvision.__version__\n",
        "                )\n",
        "            )\n",
        "        del kwargs[\"weights\"]\n",
        "\n",
        "    return torchvision.models.inception_v3(*args, **kwargs)\n",
        "\n",
        "\n",
        "def fid_inception_v3():\n",
        "    \"\"\"Build pretrained Inception model for FID computation\n",
        "\n",
        "    The Inception model for FID computation uses a different set of weights\n",
        "    and has a slightly different structure than torchvision's Inception.\n",
        "\n",
        "    This method first constructs torchvision's Inception and then patches the\n",
        "    necessary parts that are different in the FID Inception model.\n",
        "    \"\"\"\n",
        "    inception = _inception_v3(num_classes=1008, aux_logits=False, weights=None)\n",
        "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
        "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
        "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
        "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
        "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
        "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
        "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
        "\n",
        "    state_dict = load_state_dict_from_url(\"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\", progress=True)\n",
        "    inception.load_state_dict(state_dict)\n",
        "    return inception\n",
        "\n",
        "\n",
        "class FIDInceptionA(torchvision.models.inception.InceptionA):\n",
        "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, pool_features):\n",
        "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionC(torchvision.models.inception.InceptionC):\n",
        "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, channels_7x7):\n",
        "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_1(torchvision.models.inception.InceptionE):\n",
        "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n",
        "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: The FID Inception model uses max pooling instead of average\n",
        "        # pooling. This is likely an error in this specific Inception\n",
        "        # implementation, as other Inception models use average pooling here\n",
        "        # (which matches the description in the paper).\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgZPF8WdXoCZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC2LlVt4XoCa"
      },
      "outputs": [],
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,  # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3,  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_blocks=(DEFAULT_BLOCK_INDEX,),\n",
        "        resize_input=True,\n",
        "        normalize_input=True,\n",
        "        requires_grad=False,\n",
        "        use_fid_inception=True,\n",
        "    ):\n",
        "        \"\"\"Build pretrained InceptionV3\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_blocks : list of int\n",
        "            Indices of blocks to return features of. Possible values are:\n",
        "                - 0: corresponds to output of first max pooling\n",
        "                - 1: corresponds to output of second max pooling\n",
        "                - 2: corresponds to output which is fed to aux classifier\n",
        "                - 3: corresponds to output of final average pooling\n",
        "        resize_input : bool\n",
        "            If true, bilinearly resizes input to width and height 299 before\n",
        "            feeding input to model. As the network without fully connected\n",
        "            layers is fully convolutional, it should be able to handle inputs\n",
        "            of arbitrary size, so resizing might not be strictly needed\n",
        "        normalize_input : bool\n",
        "            If true, scales the input from range (0, 1) to the range the\n",
        "            pretrained Inception network expects, namely (-1, 1)\n",
        "        requires_grad : bool\n",
        "            If true, parameters of the model require gradients. Possibly useful\n",
        "            for finetuning the network\n",
        "        use_fid_inception : bool\n",
        "            If true, uses the pretrained Inception model used in Tensorflow's\n",
        "            FID implementation. If false, uses the pretrained Inception model\n",
        "            available in torchvision. The FID Inception model has different\n",
        "            weights and a slightly different structure from torchvision's\n",
        "            Inception model. If you want to compute FID scores, you are\n",
        "            strongly advised to set this parameter to true to get comparable\n",
        "            results.\n",
        "        \"\"\"\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \"Last possible output block index is 3\"\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        if use_fid_inception:\n",
        "            inception = fid_inception_v3()\n",
        "        else:\n",
        "            inception = _inception_v3(weights=\"DEFAULT\")\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tOV_I6AXoCc"
      },
      "source": [
        "#### Calculo da distribuicao gerada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snRXJWMYXoCc"
      },
      "outputs": [],
      "source": [
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36YTdjuXXoCc"
      },
      "outputs": [],
      "source": [
        "model = InceptionV3([block_idx]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIXrWCu6XoCd"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDvQF-u3XoCd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2Ia0an8XoCd"
      },
      "outputs": [],
      "source": [
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYgFFYEcXoCd"
      },
      "outputs": [],
      "source": [
        "class ImagePathDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, transforms=None):\n",
        "        self.files = files\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path = self.files[i]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3bQrqaXoCd"
      },
      "source": [
        "Por imagens geradas prontas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOUTdiayXoCe"
      },
      "outputs": [],
      "source": [
        "path = \"../imagens geradas/cgan_samples\"\n",
        "path = pathlib.Path(path)\n",
        "files = sorted(file for file in path.glob(\"*.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCrv8my3XoCe"
      },
      "outputs": [],
      "source": [
        "pred_arr = np.empty((len(files), dims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0LGyDp9XoCe"
      },
      "outputs": [],
      "source": [
        "dataset = ImagePathDataset(files, transforms=torchvision.transforms.ToTensor())\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtFXwOHQXoCe"
      },
      "source": [
        "Por modelo pre-treinado gerando imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiwYtDuvXoCe"
      },
      "outputs": [],
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(CGAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        #self._initialize_weights()\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # Itera sobre todos os m√≥dulos da rede geradora\n",
        "        for m in self.generator:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        if input.dim() == 2:\n",
        "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "            x = self.generator(z)\n",
        "            x = x.view(x.size(0), *self.img_shape) #Em\n",
        "            return x\n",
        "        elif input.dim() == 4:\n",
        "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UnF0P-uXoCf"
      },
      "outputs": [],
      "source": [
        "cgan = CGAN()\n",
        "cgan.load_state_dict(torch.load(\"CGAN_50epochs.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU6PYq0JXoCf"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "from datasets import Dataset, Features, ClassLabel\n",
        "from datasets import Image as IMG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YmuBLHNXoCf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GeneratedDataset(Dataset):\n",
        "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.model = type(self.generator).__name__\n",
        "        self.images = self.generate_data()\n",
        "        self.classes = [i for i in range(self.num_classes)]\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        gen_imgs = {}\n",
        "        self.generator.eval()\n",
        "        labels = {c: torch.tensor([c for i in range(self.num_samples)], device=self.device) for c in range(self.num_classes)}\n",
        "        for c, label in labels.items():\n",
        "          if self.model == 'Generator':\n",
        "              labels_one_hot = F.one_hot(label, self.num_classes).float().to(self.device) #\n",
        "          z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "          with torch.no_grad():\n",
        "              if self.model == 'Generator':\n",
        "                  gen_imgs_class = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
        "              elif self.model == 'CGAN':\n",
        "                  gen_imgs_class = self.generator(z, label)\n",
        "          gen_imgs[c] = gen_imgs_class\n",
        "\n",
        "        return gen_imgs\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples * self.num_classes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Mapear o √≠ndice global para (classe, √≠ndice interno)\n",
        "        class_idx = idx // self.num_samples\n",
        "        sample_idx = idx % self.num_samples\n",
        "        # Retorna apenas a imagem (sem o r√≥tulo)\n",
        "        return self.images[class_idx][sample_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIh5F0qqXoCf"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 2048\n",
        "latent_dim = 100\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=cgan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "gen_dataset = generated_dataset.images\n",
        "\n",
        "for c in gen_dataset.keys():\n",
        "    gen_dataset[c] = (gen_dataset[c] + 1) / 2\n",
        "    gen_dataset[c] = gen_dataset[c].repeat(1, 3, 1, 1)\n",
        "# # Ajustar para o intervalo [0, 1]\n",
        "# gen_dataset = (gen_dataset + 1) / 2\n",
        "# Expandir o canal para RGB (replicando o canal 1 para 3)\n",
        "# gen_dataset = gen_dataset.repeat(1, 3, 1, 1)  # Agora tem shape [2050, 3, 28, 28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceinxLQKXoCf"
      },
      "outputs": [],
      "source": [
        "dataloaders = [torch.utils.data.DataLoader(gen_dataset[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwsjJ5fsXoCh"
      },
      "source": [
        "Calculo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQweQMlEXoCh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2x0DraxXoCi"
      },
      "outputs": [],
      "source": [
        "mus_gen = []\n",
        "sigmas_gen = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgtpBE95XoCl"
      },
      "outputs": [],
      "source": [
        "for c in range(10):\n",
        "  pred_arr = np.empty((len(gen_dataset[c]), dims))\n",
        "  start_idx = 0\n",
        "  for batch in tqdm(dataloaders[c]):\n",
        "          batch = batch.to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              pred = model(batch)[0]\n",
        "\n",
        "          # If model output is not scalar, apply global spatial average pooling.\n",
        "          # This happens if you choose a dimensionality not equal 2048.\n",
        "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "\n",
        "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
        "\n",
        "          start_idx = start_idx + pred.shape[0]\n",
        "  mus_gen.append(np.mean(pred_arr, axis=0))\n",
        "  sigmas_gen.append(np.cov(pred_arr, rowvar=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_w71nZ8XoCl"
      },
      "source": [
        "Calculo da distribuicao real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsx4vJ9BXoCm"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWEAlYaGXoCm"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
        "# Create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "trainloader_reduzido = torch.utils.data.DataLoader(trainset_reduzido, batch_size=128, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ODa5VTrXoCm"
      },
      "outputs": [],
      "source": [
        "testset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8js9LM5qXoCm"
      },
      "source": [
        "Pegando imagens sem salvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgwQDuAyXoCn"
      },
      "outputs": [],
      "source": [
        "def select_samples_per_class(dataset, num_samples):\n",
        "    \"\"\"\n",
        "    Selects a specified number of samples per class from the dataset and returns them as tensors.\n",
        "\n",
        "    Parameters:\n",
        "    dataset (torch.utils.data.Dataset): The dataset to select samples from.\n",
        "    num_samples (int): The number of samples to select per class.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where each key corresponds to a class and the value is a tensor of shape [num_samples, 1, 28, 28].\n",
        "    \"\"\"\n",
        "    class_samples = {i: [] for i in range(len(dataset.classes))}\n",
        "    class_counts = {i: 0 for i in range(len(dataset.classes))}\n",
        "\n",
        "    for img, label in dataset:\n",
        "        if class_counts[label] < num_samples:\n",
        "            class_samples[label].append(img)\n",
        "            class_counts[label] += 1\n",
        "        if all(count >= num_samples for count in class_counts.values()):\n",
        "            break\n",
        "    else:\n",
        "        print(\"Warning: Not all classes have the requested number of samples.\")\n",
        "\n",
        "    # Convert lists of tensors to a single tensor per class\n",
        "    for label in class_samples:\n",
        "        if class_samples[label]:  # Check if the list is not empty\n",
        "            class_samples[label] = torch.stack(class_samples[label], dim=0)\n",
        "            class_samples[label] = (class_samples[label] + 1) / 2\n",
        "            class_samples[label] = class_samples[label].repeat(1, 3, 1, 1)\n",
        "        else:\n",
        "            # Handle empty classes if necessary; here we leave an empty tensor\n",
        "            class_samples[label] = torch.Tensor()\n",
        "\n",
        "    return class_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxtmTmD6XoCn"
      },
      "outputs": [],
      "source": [
        "img_reais = select_samples_per_class(testset, 800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_r4SNe9XoCv"
      },
      "outputs": [],
      "source": [
        "dataloaders = [torch.utils.data.DataLoader(img_reais[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXFM_KcOXoCw"
      },
      "source": [
        "Salvando imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4HPBo8iXoCw"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRy47-hWXoCw"
      },
      "outputs": [],
      "source": [
        "# Function to save a random sample of images\n",
        "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    if classes is None:\n",
        "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
        "\n",
        "    if balanced:\n",
        "        # Get the number of classes\n",
        "        num_classes = len(classes)\n",
        "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
        "        indices = []\n",
        "        class_counts = {i: 0 for i in classes}\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        shuffled_indices = list(range(len(dataset)))\n",
        "        random.shuffle(shuffled_indices)\n",
        "\n",
        "        for idx in shuffled_indices:\n",
        "            img = dataset[idx][0]\n",
        "            label = int(dataset[idx][1])\n",
        "            if label in classes and class_counts[label] < samples_per_class:\n",
        "                indices.append(idx)\n",
        "                class_counts[label] += 1\n",
        "            if len(indices) >= num_samples:\n",
        "                break\n",
        "    else:\n",
        "        indices = []\n",
        "        while len(indices) < num_samples:\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            if int(dataset[idx][1]) in classes:\n",
        "                indices.append(idx)\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = dataset[idx]\n",
        "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
        "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhEt3LNsXoCx"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  save_random_samples(trainset, num_samples=2050, folder=f'Imagens Testes/mnist_samples_{i}', balanced=True, classes=[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXpIFPDtXoCx"
      },
      "outputs": [],
      "source": [
        "pathes = [f\"Imagens Testes/mnist_samples_{i}\" for i in range(10)]\n",
        "pathes = [pathlib.Path(path) for path in pathes]\n",
        "files = [sorted(file for file in path.glob(\"*.png\")) for path in pathes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U4U3uYZXoCx"
      },
      "outputs": [],
      "source": [
        "datasets = [ImagePathDataset(file, transforms=torchvision.transforms.ToTensor()) for file in files]\n",
        "dataloaders = [torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False) for dataset in datasets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVRjN1RbXoCx"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OHeQeC6XoCx"
      },
      "source": [
        "Calulo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeivOPmZXoCy"
      },
      "outputs": [],
      "source": [
        "mus_real = []\n",
        "sigmas_real = []\n",
        "for c in range(10):\n",
        "  model = InceptionV3([block_idx]).to(device)\n",
        "  model.eval()\n",
        "  pred_arr = np.empty((len(img_reais[0]), dims))\n",
        "  start_idx = 0\n",
        "  for batch in tqdm(dataloaders[c]):\n",
        "          batch = batch.to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              pred = model(batch)[0]\n",
        "\n",
        "          # If model output is not scalar, apply global spatial average pooling.\n",
        "          # This happens if you choose a dimensionality not equal 2048.\n",
        "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "\n",
        "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
        "\n",
        "          start_idx = start_idx + pred.shape[0]\n",
        "  mus_real.append(np.mean(pred_arr, axis=0))\n",
        "  sigmas_real.append(np.cov(pred_arr, rowvar=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXa0TM2lXoCy"
      },
      "source": [
        "Calculo FID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XtoaLpPXoCy"
      },
      "outputs": [],
      "source": [
        "from scipy import linalg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAGo9hN6XoCz"
      },
      "outputs": [],
      "source": [
        "mus_gen = [np.atleast_1d(mu_gen) for mu_gen in mus_gen]\n",
        "mus_real = [np.atleast_1d(mu_real) for mu_real in mus_real]\n",
        "\n",
        "sigmas_gen = [np.atleast_2d(sigma_gen) for sigma_gen in sigmas_gen]\n",
        "sigmas_real = [np.atleast_2d(sigma_real) for sigma_real in sigmas_real]\n",
        "\n",
        "for mu_gen, mu_real, sigma_gen, sigma_real in zip(mus_gen, mus_real, sigmas_gen, sigmas_real):\n",
        "  assert (\n",
        "      mu_gen.shape == mu_real.shape\n",
        "  ), \"Training and test mean vectors have different lengths\"\n",
        "  assert (\n",
        "      sigma_gen.shape == sigma_real.shape\n",
        "  ), \"Training and test covariances have different dimensions\"\n",
        "\n",
        "diffs = [mu_gen - mu_real for mu_gen, mu_real in zip(mus_gen, mus_real)]\n",
        "\n",
        "# Product might be almost singular\n",
        "covmeans = [linalg.sqrtm(sigmas_gen.dot(sigmas_real), disp=False)[0] for sigmas_gen, sigmas_real in zip(sigmas_gen, sigmas_real)]\n",
        "for covmean, sigma_gen, sigma_real in zip(covmeans, sigmas_gen, sigmas_real):\n",
        "  if not np.isfinite(covmean).all():\n",
        "    msg = (\n",
        "        \"fid calculation produces singular product; \"\n",
        "        \"adding %s to diagonal of cov estimates\"\n",
        "    ) % 1e-6\n",
        "    print(msg)\n",
        "    offset = np.eye(sigma_gen.shape[0]) * 1e-6\n",
        "    covmean = linalg.sqrtm((sigma_gen + offset).dot(sigma_real + offset))\n",
        "\n",
        "# Numerical error might give slight imaginary component\n",
        "for i, covmean in enumerate(covmeans):\n",
        "  if np.iscomplexobj(covmean):\n",
        "      if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "          m = np.max(np.abs(covmean.imag))\n",
        "          raise ValueError(\"Imaginary component {}\".format(m))\n",
        "      covmeans[i] = covmean.real\n",
        "\n",
        "tr_covmeans = [np.trace(covmean) for covmean in covmeans]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlYmUB0RXoCz"
      },
      "outputs": [],
      "source": [
        "fids = [diff.dot(diff) + np.trace(sigma_gen) + np.trace(sigma_real) - 2 * tr_covmean for diff, sigma_gen, sigma_real, tr_covmean in zip(diffs, sigmas_gen, sigmas_real, tr_covmeans)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UAzyULzXoCz"
      },
      "outputs": [],
      "source": [
        "fids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYo0rAs8XoCz"
      },
      "source": [
        "### Federado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CGAN duas classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Gen(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(Gen, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "    \n",
        "    def forward(self, input, labels):\n",
        "        z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "        x = self.generator(z)\n",
        "        x = x.view(x.size(0), *self.img_shape) #Em\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Disc(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(Disc, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "        return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jgO5dbyOX9IB",
        "outputId": "a3b2b562-b7db-4f9e-b6b9-306668fa6d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flwr_datasets\n",
            "  Downloading flwr_datasets-0.5.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting datasets<=3.1.0,>=2.14.6 (from flwr_datasets)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.7.5 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (1.26.4)\n",
            "Requirement already satisfied: seaborn<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (0.13.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets<=3.1.0,>=2.14.6->flwr_datasets) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1.31)\n",
            "Downloading flwr_datasets-0.5.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m87.0/87.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, flwr_datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 flwr_datasets-0.5.0 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flwr_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nBvMjRnOXoCz"
      },
      "outputs": [],
      "source": [
        "from flwr_datasets.partitioner import DirichletPartitioner\n",
        "from flwr_datasets import FederatedDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "85htNladXoC0"
      },
      "outputs": [],
      "source": [
        "num_partitions = 1\n",
        "alpha_dir = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R2Xjlca_XoC0"
      },
      "outputs": [],
      "source": [
        "partitioner = DirichletPartitioner(\n",
        "    num_partitions=num_partitions,\n",
        "    partition_by=\"label\",\n",
        "    alpha=alpha_dir,\n",
        "    min_partition_size=0,\n",
        "    self_balancing=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "227p665iXoC0"
      },
      "outputs": [],
      "source": [
        "fds = FederatedDataset(\n",
        "    dataset=\"mnist\",\n",
        "    partitioners={\"train\": partitioner}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "46b96767f5e94b1dab5289f7ca493845",
            "fe0dac5b8f4b4b2db3b12745a0ea41bb",
            "4c559859b32d4c8e9ff5bdc179a3dbc2",
            "9fe53a5854df445dad0487dfc9035021",
            "7f3112f458a1497bafc6dfa40b3873f6",
            "5a673e78c8104c17b88cf65efddf4fcd",
            "2c7ed38f9c094e99b04b89a4121085f0",
            "1964b90585344c1880894924f1d058c5",
            "79be0691cc304fbc974e2463f055b5ad",
            "582503ab5a96473f974e7e68491f5ef2",
            "02ebbb4212b44d70b3c2860c8d98a1e1",
            "ecd6b36466d7420faa39384fbfff990a",
            "d7a1a9f475f9408e991033c95e4c2a7d",
            "928472df19414ba380a724a5d986c5f7",
            "3ede49350efb4a66bb9d9bd9c50d8402",
            "f1d0fd160d944dba80868e3024371e1b",
            "063a8820ba58452d88fdbc48049680cb",
            "456b4c7409c54888acd47a0c085b57de",
            "75abc3232c9a4e71aca3ac1cbc333e65",
            "28fa1055f85e42d584662c332df28cdb",
            "b5fadde8538c40399d0d6175992a2c08",
            "33c90b7d0dff423bb7bb6b6fe1bcd9f5",
            "c65cbcde25be4b67ad0972777fba7164",
            "856b2392293943a0a31ce86e48abf3b4",
            "bf453f0267b841bfbeb8dface06da913",
            "be0c4ae734eb405ab74533eb8ae6d677",
            "bfdd786d2c1640728fa4701f3de65680",
            "07f4993d5ce94b90896882f75058ad4a",
            "0c64263c0cbb4d6a8bb03085f87f806b",
            "5687f3380b0f4371aab7b74c96bf3eae",
            "50ba90dce2634ba7bd282722d41eb7d6",
            "2e4bf9df1c924c8fb2ac5da26778d5f0",
            "a8a6950ad39545de9aaab01e502b1c7e",
            "4f105781904045389b9f80fef1422204",
            "03fe67e2b82e46eda94c5735eb0b3838",
            "0c335ee394e5454688e169253ccc202a",
            "2184aff93e5e4ad4a2bcba4e1b352353",
            "d09cd624408c4b12bca2f98fe5cc9e4d",
            "38f5264ea38e490ab1730095821d5732",
            "b2c2ad47f5714749b1e233d731137a32",
            "71d3ef1e998342b6a909d098edbe5840",
            "eb53eb56d6814e3495324cd1720e403f",
            "bec86a3b96264fa4b742f12438b65c89",
            "9ad15303c12b4fed98d8179f75fa6c6d",
            "748f44fc54004f80b40dc65aa3092679",
            "fac0eb3e353641ce9aab2b90f03a75a1",
            "d8237488423e4f8aab99caecee63f0be",
            "e9655c60cd43415db6a81d0f688b2bf7",
            "2047512a3b8343ea9b84080d1355820b",
            "f1cf09270eb745ea8364b3e44acf920a",
            "518ce70cd0104e37a68d62e9b92c6fc0",
            "c7e04c9861094782b045c8eb510f954f",
            "317cc826243e41c6baea198d26fc8ea7",
            "006c513f6fd54337835cd1073b081b0a",
            "bed1ff516b5e4c33b68312f32fdd7a6f"
          ]
        },
        "id": "jw5CezaqXoC0",
        "outputId": "ce63cdcf-8d4d-48b2-962f-f906f96cda02"
      },
      "outputs": [],
      "source": [
        "train_partitions = [fds.load_partition(i, split=\"train\") for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ05wNS_ZSi3"
      },
      "source": [
        "##### Rodar proxima celula somente se quiser testar com dataset reduzido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-DfNUiSXoC0"
      },
      "outputs": [],
      "source": [
        "num_samples = [int(len(train_partition)/10) for train_partition in train_partitions]\n",
        "train_partitions = [train_partition.select(range(n)) for train_partition, n in zip(train_partitions, num_samples)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BkqcHp6eXoC1"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LIroIscqXoC1"
      },
      "outputs": [],
      "source": [
        "pytorch_transforms = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nPXZ9XAjXoC1"
      },
      "outputs": [],
      "source": [
        "train_partitions = [train_partition.with_transform(apply_transforms) for train_partition in train_partitions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kF7BB2O-XoC1"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "trainloaders = [DataLoader(train_partition, batch_size=batch_size, shuffle=True) for train_partition in train_partitions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "00vLeJqCXoC2"
      },
      "outputs": [],
      "source": [
        "models = [CGAN() for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [Disc() for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrgGRhGmXoC2",
        "outputId": "7b2c21d9-7715-442a-985a-861b784b1e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soma total dos valores dos par√¢metros do gerador do modelo 0: 1751.9129284620285\n"
          ]
        }
      ],
      "source": [
        "for idx, model in enumerate(models):\n",
        "    generator_params = [param.data.numpy() for param in model.generator.parameters()]\n",
        "    generator_params_sum = sum([param.sum() for param in generator_params])\n",
        "    print(f\"Soma total dos valores dos par√¢metros do gerador do modelo {idx}: {generator_params_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aafji_4jXoC3",
        "outputId": "d95ffb4a-7fb7-47a8-9a87-e863bed21e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soma total dos valores dos par√¢metros do gerador do modelo 0: 328.63008058443666\n",
            "Soma total dos valores dos par√¢metros do gerador do modelo 1: 237.89485466852784\n",
            "Soma total dos valores dos par√¢metros do gerador do modelo 2: 186.06141594797373\n",
            "Soma total dos valores dos par√¢metros do gerador do modelo 3: 222.37551382556558\n"
          ]
        }
      ],
      "source": [
        "for idx, model in enumerate(models):\n",
        "    disc_params = [param.data.numpy() for param in model.discriminator.parameters()]\n",
        "    disc_params_sum = sum([param.sum() for param in disc_params])\n",
        "    print(f\"Soma total dos valores dos par√¢metros do gerador do modelo {idx}: {disc_params_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iKBK-xyXoC3",
        "outputId": "5f9fe266-99bd-446f-a567-1d05ea02f2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 [100/134] loss_D_treino: 0.0004\n",
            "Epoch 1 [100/134] loss_D_treino: 0.0001\n",
            "Epoch 0 [100/156] loss_D_treino: 0.0004\n",
            "Epoch 1 [100/156] loss_D_treino: 0.0001\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "for net, trainloader in zip(models, trainloaders):\n",
        "    train_gen(net=net,\n",
        "              trainloader=trainloader,\n",
        "              epochs=epochs,\n",
        "              lr=0.0001,\n",
        "              device=\"cpu\",\n",
        "              dataset=\"mnist\",\n",
        "              latent_dim=100,\n",
        "              f2a=True,\n",
        "              cliente=True\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3NZ6Ak66XoC4"
      },
      "outputs": [],
      "source": [
        "def generate_plot(net, device, round_number, client_id = None, examples_per_class: int=5, classes: int=10, latent_dim: int=100, server: bool=False):\n",
        "    \"\"\"Gera plot de imagens de cada classe\"\"\"\n",
        "    if server:\n",
        "        import matplotlib\n",
        "        matplotlib.use(\"Agg\")\n",
        "        import matplotlib.pyplot as plt\n",
        "    else:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    batch_size = examples_per_class * classes\n",
        "\n",
        "    latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
        "    labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_images = net(latent_vectors, labels).cpu()\n",
        "\n",
        "    # Criar uma figura com 10 linhas e 5 colunas de subplots\n",
        "    fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
        "\n",
        "    # Adiciona t√≠tulo no topo da figura\n",
        "    if client_id:\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
        "    else:\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "    # Exibir as imagens nos subplots\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    # Ajustar o layout antes de calcular as posi√ß√µes\n",
        "    plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "    # Reduzir espa√ßo entre colunas\n",
        "    # plt.subplots_adjust(wspace=0.05)\n",
        "\n",
        "    # Adicionar os r√≥tulos das classes corretamente alinhados\n",
        "    fig.canvas.draw()  # Atualiza a renderiza√ß√£o para obter posi√ß√µes corretas\n",
        "    for row in range(classes):\n",
        "        # Obter posi√ß√£o do subplot em coordenadas da figura\n",
        "        bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "        pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "        center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "        # Adicionar o r√≥tulo\n",
        "        fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
        "\n",
        "    fig.savefig(f\"mnist_CGAN_r{round_number}_f2a.png\")\n",
        "    plt.close(fig)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VZuTSmgYXoC4"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7L_24AOXoC5"
      },
      "outputs": [],
      "source": [
        "gen = CGAN().to(device)\n",
        "optimizer_D_gen = torch.optim.Adam(gen.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optimizer_G_gen = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "# scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "# scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = Disc().to(device)\n",
        "optimizer_D = torch.optim.Adam(net.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Z8SN_A1oXoC5"
      },
      "outputs": [],
      "source": [
        "net = CGAN().to(device)\n",
        "optimizer_D = torch.optim.Adam(net.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optimizer_G = torch.optim.Adam(net.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "# scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "# scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen = Gen().to(device)\n",
        "optimizer_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "D = Discriminator().to(device)\n",
        "G = Generator().to(device)\n",
        "\n",
        "# Otimizadores\n",
        "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "# scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "# scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
        "\n",
        " # Fun√ß√£o de perda Wasserstein\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return fake_output.mean() - real_output.mean()\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return -fake_output.mean()\n",
        "\n",
        "# Fun√ß√£o para calcular Gradient Penalty\n",
        "def gradient_penalty(D, real_samples, fake_samples):\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
        "    interpolated = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "    d_interpolated = D(interpolated)\n",
        "    gradients = torch.autograd.grad(outputs=d_interpolated, inputs=interpolated,\n",
        "                                    grad_outputs=torch.ones_like(d_interpolated),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrMTGrznZNie"
      },
      "source": [
        "#### Treinamento Normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5e15fe4f89444da95475e53e0ed6521",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Treinamento:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Epoch 1/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae1857db953643cfb9ea00bb98f0602d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 [100/469] loss_D_treino: 0.7231 loss_G_treino: 0.7497\n",
            "Epoch 0 [200/469] loss_D_treino: 0.6370 loss_G_treino: 0.8519\n",
            "Epoch 0 [300/469] loss_D_treino: 0.6112 loss_G_treino: 1.7622\n",
            "Epoch 0 [400/469] loss_D_treino: 0.5447 loss_G_treino: 1.3501\n",
            "\n",
            "üîπ Epoch 2/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2be77324ebac451eab5a47055ae2c7d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 [100/469] loss_D_treino: 0.5171 loss_G_treino: 1.7068\n",
            "Epoch 1 [200/469] loss_D_treino: 0.5881 loss_G_treino: 0.8900\n",
            "Epoch 1 [300/469] loss_D_treino: 0.7096 loss_G_treino: 1.1687\n",
            "Epoch 1 [400/469] loss_D_treino: 0.3222 loss_G_treino: 2.5462\n",
            "\n",
            "üîπ Epoch 3/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d2587d2ef414442ace4cd9c86d60b65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 [100/469] loss_D_treino: 0.4525 loss_G_treino: 2.0984\n",
            "Epoch 2 [200/469] loss_D_treino: 0.3967 loss_G_treino: 2.4146\n",
            "Epoch 2 [300/469] loss_D_treino: 0.3842 loss_G_treino: 1.9833\n",
            "Epoch 2 [400/469] loss_D_treino: 0.5920 loss_G_treino: 4.0998\n",
            "\n",
            "üîπ Epoch 4/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15850ca1f78a42629f4e1d6f779b24c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 [100/469] loss_D_treino: 0.4037 loss_G_treino: 1.5290\n",
            "Epoch 3 [200/469] loss_D_treino: 0.3887 loss_G_treino: 2.3713\n",
            "Epoch 3 [300/469] loss_D_treino: 1.0298 loss_G_treino: 0.7663\n",
            "Epoch 3 [400/469] loss_D_treino: 0.3923 loss_G_treino: 1.3169\n",
            "\n",
            "üîπ Epoch 5/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b98ca95973d4036badd2f683f5704ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 [100/469] loss_D_treino: 0.4275 loss_G_treino: 1.3646\n",
            "Epoch 4 [200/469] loss_D_treino: 0.4272 loss_G_treino: 1.4226\n",
            "Epoch 4 [300/469] loss_D_treino: 0.5610 loss_G_treino: 1.1324\n",
            "Epoch 4 [400/469] loss_D_treino: 0.4061 loss_G_treino: 1.2114\n",
            "\n",
            "üîπ Epoch 6/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbcf9e95cd5e43f1a183e066226394ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 [100/469] loss_D_treino: 0.5364 loss_G_treino: 0.9555\n",
            "Epoch 5 [200/469] loss_D_treino: 0.4289 loss_G_treino: 1.5250\n",
            "Epoch 5 [300/469] loss_D_treino: 0.4179 loss_G_treino: 1.1756\n",
            "Epoch 5 [400/469] loss_D_treino: 0.4825 loss_G_treino: 1.4234\n",
            "\n",
            "üîπ Epoch 7/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a035e1858544650995922bce1207493",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 [100/469] loss_D_treino: 0.5625 loss_G_treino: 1.2706\n",
            "Epoch 6 [200/469] loss_D_treino: 0.4637 loss_G_treino: 1.5901\n",
            "Epoch 6 [300/469] loss_D_treino: 0.4951 loss_G_treino: 1.2918\n",
            "Epoch 6 [400/469] loss_D_treino: 0.4600 loss_G_treino: 1.1470\n",
            "\n",
            "üîπ Epoch 8/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11d65a197eeb48b7b09f933e2f179580",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 [100/469] loss_D_treino: 0.5117 loss_G_treino: 1.3274\n",
            "Epoch 7 [200/469] loss_D_treino: 0.5663 loss_G_treino: 1.9842\n",
            "Epoch 7 [300/469] loss_D_treino: 0.4805 loss_G_treino: 0.9685\n",
            "Epoch 7 [400/469] loss_D_treino: 0.6072 loss_G_treino: 0.9271\n",
            "\n",
            "üîπ Epoch 9/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "948b2cb3c9694b01936caffa5152462c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 [100/469] loss_D_treino: 0.5458 loss_G_treino: 1.5693\n",
            "Epoch 8 [200/469] loss_D_treino: 0.4890 loss_G_treino: 1.3664\n",
            "Epoch 8 [300/469] loss_D_treino: 0.5871 loss_G_treino: 1.8632\n",
            "Epoch 8 [400/469] loss_D_treino: 0.5446 loss_G_treino: 0.9717\n",
            "\n",
            "üîπ Epoch 10/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fee56e3971214bb58015cc0c1dbae6c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 [100/469] loss_D_treino: 0.5302 loss_G_treino: 1.1317\n",
            "Epoch 9 [200/469] loss_D_treino: 0.5517 loss_G_treino: 1.1455\n",
            "Epoch 9 [300/469] loss_D_treino: 0.5295 loss_G_treino: 0.9624\n",
            "Epoch 9 [400/469] loss_D_treino: 0.5778 loss_G_treino: 1.0638\n",
            "\n",
            "üîπ Epoch 11/50\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d82df92d22c4b348b2836dc194ca0a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 [100/469] loss_D_treino: 0.5848 loss_G_treino: 0.9494\n",
            "Epoch 10 [200/469] loss_D_treino: 0.5997 loss_G_treino: 1.0939\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m d_real_loss \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mloss(y_real, real_ident)\n\u001b[1;32m     49\u001b[0m x_fake \u001b[38;5;241m=\u001b[39m gen(z_noise, x_fake_labels)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 50\u001b[0m y_fake_d \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_fake_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m d_fake_loss \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mloss(y_fake_d, fake_ident)\n\u001b[1;32m     52\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (d_real_loss \u001b[38;5;241m+\u001b[39m d_fake_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[20], line 31\u001b[0m, in \u001b[0;36mDisc.forward\u001b[0;34m(self, input, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, labels):\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(x)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "wgan = False\n",
        "epochs = 50\n",
        "g_losses_batch = []\n",
        "d_losses_batch = []\n",
        "g_losses_epoch = []\n",
        "d_losses_epoch = []\n",
        "\n",
        "epoch_bar = tqdm(range(epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "for epoch in epoch_bar:\n",
        "    print(f\"\\nüîπ Epoch {epoch+1}/{epochs}\")\n",
        "    epoch_g_loss = 0.0\n",
        "    epoch_d_loss = 0.0\n",
        "    num_batches = 0\n",
        "    batch_bar = tqdm(enumerate(trainloaders[0]), desc=\"Batches\", leave=False, position=1)\n",
        "    start_time = time.time()\n",
        "    for batch_idx, batch in batch_bar:\n",
        "        images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "        fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "        z_noise = torch.randn(batch_size, 100, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            x_fake_labels = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "\n",
        "            # Adicionar labels ao images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = x_fake_labels.view(x_fake_labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            images = torch.cat([images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z_noise = torch.cat([z_noise, x_fake_labels], dim=1)\n",
        "            fake_images = G(z_noise).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            D(images)\n",
        "            d_loss = discriminator_loss(D(images), D(fake_images)) + 10 * gradient_penalty(D, images, fake_images)\n",
        "\n",
        "        else:\n",
        "            # Train D\n",
        "            y_real = net(images, labels)\n",
        "            d_real_loss = net.loss(y_real, real_ident)\n",
        "            x_fake = gen(z_noise, x_fake_labels).detach()\n",
        "            y_fake_d = net(x_fake, x_fake_labels)\n",
        "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        if wgan:\n",
        "            fake_images = G(z_noise)\n",
        "            g_loss = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
        "        else:\n",
        "            # Train G\n",
        "            x_fake = gen(z_noise, x_fake_labels)\n",
        "            y_fake_g = net(x_fake, x_fake_labels)\n",
        "            g_loss = gen.loss(y_fake_g, real_ident)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        g_losses_batch.append(g_loss.item())\n",
        "        d_losses_batch.append(d_loss.item())\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
        "                        epoch, batch_idx, len(trainloaders[0]),\n",
        "                        d_loss.mean().item(),\n",
        "                        g_loss.mean().item()))\n",
        "\n",
        "    # Calcula m√©dias da √©poca\n",
        "    avg_g_loss = epoch_g_loss / num_batches\n",
        "    avg_d_loss = epoch_d_loss / num_batches\n",
        "\n",
        "    # Armazena as m√©dias\n",
        "    g_losses_epoch.append(avg_g_loss)\n",
        "    d_losses_epoch.append(avg_d_loss)\n",
        "\n",
        "\n",
        "    figura = generate_plot(net=gen, device=device, round_number=epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqnZhxxUZaHM"
      },
      "source": [
        "#### Treinamento de Geradora √∫nica, ap√≥s clientes treinarem Discriminadoras por todos os dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdhiGs-HXoC8"
      },
      "outputs": [],
      "source": [
        "g_losses = []  # Perda m√©dia do gerador por rodada\n",
        "d_losses = []  # Perda m√©dia do discriminador por rodada\n",
        "num_discriminator_epochs = 1  # √âpocas de treino do discriminador por rodada\n",
        "num_generator_epochs = 50       # √âpocas de treino do gerador por rodada\n",
        "\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "for r in range(50):  # 100 rodadas federadas\n",
        "    # ========================================================================\n",
        "    # Treino dos Discriminadores (clientes)\n",
        "    # ========================================================================\n",
        "    round_d_losses = []  # Armazena as perdas dos discriminadores nesta rodada\n",
        "\n",
        "    for i, (net, trainloader) in enumerate(zip(models, trainloaders)):\n",
        "        net.to(device)\n",
        "        optim_D = optim_Ds[i]\n",
        "\n",
        "        for e in range(num_discriminator_epochs):  # √âpocas locais\n",
        "            epoch_d_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            for batch in trainloader:\n",
        "                images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "                batch_size = images.size(0)\n",
        "                real_ident = torch.full((batch_size, 1), 1.0, device=device)\n",
        "                fake_ident = torch.full((batch_size, 1), 0.0, device=device)\n",
        "\n",
        "                # Treino do Discriminador\n",
        "                net.zero_grad()\n",
        "\n",
        "                # Dados reais\n",
        "                y_real = net(images, labels)\n",
        "                d_real_loss = net.loss(y_real, real_ident)\n",
        "\n",
        "                # Dados falsos (gerados)\n",
        "                z_noise = torch.randn(batch_size, 100, device=device)\n",
        "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                x_fake = gen(z_noise, x_fake_labels)  # Usa o gerador global\n",
        "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "\n",
        "                # Loss total e backprop\n",
        "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                d_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(net.discriminator.parameters(), max_norm=1.0)\n",
        "                optim_D.step()\n",
        "\n",
        "                # Acumula a perda\n",
        "                epoch_d_loss += d_loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            # M√©dia da perda do discriminador nesta √©poca\n",
        "            epoch_d_loss /= num_batches\n",
        "            round_d_losses.append(epoch_d_loss)\n",
        "\n",
        "    # M√©dia da perda dos discriminadores nesta rodada\n",
        "    avg_d_loss = sum(round_d_losses) / len(round_d_losses)\n",
        "    d_losses.append(avg_d_loss)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Treino do Gerador (global)\n",
        "    # ========================================================================\n",
        "    round_g_losses = []  # Armazena as perdas do gerador nesta rodada\n",
        "\n",
        "    for e in range(num_generator_epochs):  # √âpocas do gerador\n",
        "        gen.zero_grad()\n",
        "\n",
        "        # Gera dados falsos\n",
        "        z_noise = torch.randn(32, 100, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (32,), device=device)\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "        real_ident = torch.full((32, 1), 1.0, device=device)\n",
        "        y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "        Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "\n",
        "        # Calcula a perda do gerador\n",
        "        y_fake_g = Dmax(x_fake, x_fake_labels)\n",
        "        g_loss = gen.loss(y_fake_g, real_ident)\n",
        "        g_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "        optim_G.step()\n",
        "\n",
        "        # Acumula a perda\n",
        "        round_g_losses.append(g_loss.item())\n",
        "\n",
        "    # M√©dia da perda do gerador nesta rodada\n",
        "    avg_g_loss = sum(round_g_losses) / len(round_g_losses)\n",
        "    g_losses.append(avg_g_loss)\n",
        "\n",
        "    # Gera a figura (opcional)\n",
        "    figura = generate_plot(net=gen, device=device, round_number=r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgwSieOWZxdo"
      },
      "source": [
        "#### Treinamento de Geradora √∫nica, ap√≥s cada batch em discriminadoras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n4jz6hpA2dKW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zJPR2SUg12H2"
      },
      "outputs": [],
      "source": [
        "num_chunks = 5\n",
        "client_chunks = []\n",
        "for train_partition in train_partitions:\n",
        "  chunk_size = math.ceil(len(train_partition)/num_chunks)\n",
        "\n",
        "  chunks = []\n",
        "  for i in range(num_chunks):\n",
        "      start = i * chunk_size\n",
        "      end = min((i + 1) * chunk_size, len(train_partition))\n",
        "      chunks.append(Subset(train_partition, range(start, end)))\n",
        "\n",
        "  client_chunks.append(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5QYQs60Zwwt",
        "outputId": "f8afa083-4d5e-4a3d-b7d6-03ff46159d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "√âpoca 0 completa\n",
            "√âpoca 1 completa\n",
            "√âpoca 2 completa\n",
            "√âpoca 3 completa\n",
            "√âpoca 4 completa\n",
            "√âpoca 5 completa\n",
            "√âpoca 6 completa\n",
            "√âpoca 7 completa\n",
            "√âpoca 8 completa\n",
            "√âpoca 9 completa\n",
            "√âpoca 10 completa\n",
            "√âpoca 11 completa\n",
            "√âpoca 12 completa\n",
            "√âpoca 13 completa\n",
            "√âpoca 14 completa\n",
            "√âpoca 15 completa\n",
            "√âpoca 16 completa\n",
            "√âpoca 17 completa\n",
            "√âpoca 18 completa\n",
            "√âpoca 19 completa\n",
            "√âpoca 20 completa\n",
            "√âpoca 21 completa\n",
            "√âpoca 22 completa\n",
            "√âpoca 23 completa\n",
            "√âpoca 24 completa\n",
            "√âpoca 25 completa\n",
            "√âpoca 26 completa\n",
            "√âpoca 27 completa\n",
            "√âpoca 28 completa\n",
            "√âpoca 29 completa\n",
            "√âpoca 30 completa\n",
            "√âpoca 31 completa\n",
            "√âpoca 32 completa\n",
            "√âpoca 33 completa\n",
            "√âpoca 34 completa\n",
            "√âpoca 35 completa\n",
            "√âpoca 36 completa\n",
            "√âpoca 37 completa\n",
            "√âpoca 38 completa\n",
            "√âpoca 39 completa\n",
            "√âpoca 40 completa\n",
            "√âpoca 41 completa\n",
            "√âpoca 42 completa\n",
            "√âpoca 43 completa\n",
            "√âpoca 44 completa\n",
            "√âpoca 45 completa\n",
            "√âpoca 46 completa\n",
            "√âpoca 47 completa\n",
            "√âpoca 48 completa\n",
            "√âpoca 49 completa\n"
          ]
        }
      ],
      "source": [
        "g_losses_chunk = []\n",
        "d_losses_chunk = []\n",
        "g_losses_round = []\n",
        "d_losses_round = []\n",
        "\n",
        "\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "batch_size_gen = 128\n",
        "batch_tam = 128\n",
        "extra_g_e = 5\n",
        "\n",
        "for epoch in range(50):\n",
        "  g_loss_c = 0.0\n",
        "  d_loss_c = 0.0\n",
        "  total_d_samples = 0  # Amostras totais processadas pelos discriminadores\n",
        "  total_g_samples = 0  # Amostras totais processadas pelo gerador\n",
        "\n",
        "  for chunk_idx in range(num_chunks):\n",
        "    # ====================================================================\n",
        "    # Treino dos Discriminadores (clientes) no bloco atual\n",
        "    # ====================================================================\n",
        "    d_loss_b = 0\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "    for i, (net, chunks) in enumerate(zip(models, client_chunks)):\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size = batch_tam, shuffle=True)\n",
        "\n",
        "      # Treinar o discriminador no bloco\n",
        "      net.to(device)\n",
        "      optim_D = optim_Ds[i]\n",
        "\n",
        "      for batch in chunk_loader:\n",
        "          images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "          batch_size = images.size(0)\n",
        "          real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "          fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "          # Train D\n",
        "          net.zero_grad()\n",
        "\n",
        "          # Dados Reais\n",
        "          y_real = net(images, labels)\n",
        "          d_real_loss = net.loss(y_real, real_ident)\n",
        "\n",
        "          # Dados Falsos\n",
        "          z_noise = torch.randn(batch_size, 100, device=device)\n",
        "          x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "          x_fake = gen(z_noise, x_fake_labels)\n",
        "          y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "          d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "\n",
        "          # Loss total e backprop\n",
        "          d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "          d_loss.backward()\n",
        "          # torch.nn.utils.clip_grad_norm_(net.discriminator.parameters(), max_norm=1.0)\n",
        "          optim_D.step()\n",
        "          d_loss_b += d_loss.item() * batch_size\n",
        "          total_chunk_samples += 1\n",
        "    # M√©dia da perda dos discriminadores neste chunk\n",
        "    avg_d_loss_chunk = d_loss_b / total_chunk_samples if total_chunk_samples > 0 else 0.0\n",
        "    d_losses_chunk.append(avg_d_loss_chunk)\n",
        "    d_loss_c += avg_d_loss_chunk * total_chunk_samples\n",
        "    total_d_samples += total_chunk_samples\n",
        "\n",
        "    chunk_g_loss = 0.0\n",
        "    for g_epoch in range(extra_g_e):\n",
        "      # Train G\n",
        "      gen.zero_grad()\n",
        "\n",
        "      # Gera dados falsos\n",
        "      z_noise = torch.randn(batch_size_gen, 100, device=device)\n",
        "      x_fake_labels = torch.randint(0, 10, (batch_size_gen,), device=device)\n",
        "      x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "      # Seleciona o melhor discriminador (Dmax)\n",
        "      y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "      y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "      Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "\n",
        "      # Calcula a perda do gerador\n",
        "      real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "      y_fake_g = Dmax(x_fake, x_fake_labels)  # Detach expl√≠cito\n",
        "      g_loss = gen.loss(y_fake_g, real_ident)\n",
        "      g_loss.backward()\n",
        "      #torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "      optim_G.step()\n",
        "\n",
        "      chunk_g_loss += g_loss.item()\n",
        "    g_losses_chunk.append(chunk_g_loss /extra_g_e)\n",
        "    g_loss_c += chunk_g_loss /extra_g_e\n",
        "\n",
        "  g_loss_e = g_loss_c/num_chunks\n",
        "  d_loss_e = d_loss_c / total_d_samples if total_d_samples > 0 else 0.0\n",
        "\n",
        "  g_losses_round.append(g_loss_e)\n",
        "  d_losses_round.append(d_loss_e)\n",
        "\n",
        "  figura = generate_plot(net=net, device=device, round_number=epoch)\n",
        "  print(f\"√âpoca {epoch} completa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBWFA3seZ_-U"
      },
      "source": [
        "#### Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "38ycqSVjaT4q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "J0_nwm1SXoC9"
      },
      "outputs": [],
      "source": [
        "def loss_graph(g_losses: int, d_losses: int) -> None:\n",
        "    \"\"\"Funcao para gerar grafico de evolucao das perdas da geradora e discriminadora\"\"\"\n",
        "\n",
        "    # N√∫mero de itera√ß√µes/√©pocas para cada lista\n",
        "    epochs_g = range(len(g_losses))  # Eixo x para o gerador\n",
        "    epochs_d = range(len(d_losses))  # Eixo x para o discriminador\n",
        "\n",
        "    # Criar o gr√°fico\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs_g, g_losses, label='Generator Loss')\n",
        "    plt.plot(epochs_d, d_losses, label='Discriminator Loss')\n",
        "\n",
        "    # Adicionar t√≠tulo e r√≥tulos aos eixos\n",
        "    plt.title('Generator and Discriminator Losses Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Mostrar o gr√°fico\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "leK6Jt3Manph",
        "outputId": "dbf9d258-2d79-407f-f1e7-6416fec950df"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "loss_graph(g_losses=g_losses_epoch, d_losses=d_losses_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd2fQWWzXoC_"
      },
      "outputs": [],
      "source": [
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "gen.zero_grad()\n",
        "z_noise = torch.randn(128, 100, device=device)\n",
        "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
        "x_fake = gen(z_noise, x_fake_labels)\n",
        "y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "real_ident = torch.full((128, 1), 1., device=device)\n",
        "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "y_fake_g = Dmax(x_fake, x_fake_labels)\n",
        "g_loss = gen.loss(y_fake_g, real_ident)\n",
        "g_loss.backward()\n",
        "optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjrUZkoAXoDD"
      },
      "outputs": [],
      "source": [
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "gen.zero_grad()\n",
        "z_noise = torch.randn(128, 100, device=device)\n",
        "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
        "x_fake = gen(z_noise, x_fake_labels)\n",
        "y_fake_gs = [model(x_fake, x_fake_labels) for model in models]\n",
        "real_ident = torch.full((128, 1), 1., device=device)\n",
        "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "max_idx = y_fake_g_means.index(max(y_fake_g_means))\n",
        "g_loss = gen.loss(y_fake_gs[max_idx], real_ident)\n",
        "g_loss.backward()\n",
        "optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_mj5MokXoDM"
      },
      "outputs": [],
      "source": [
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.common import FitRes, Status, Code, ndarrays_to_parameters, parameters_to_ndarrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDEMtyGKXoDM"
      },
      "outputs": [],
      "source": [
        "params = [[val.cpu().numpy() for _, val in net.state_dict().items()] for net in models]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72r92dCPXoDM"
      },
      "outputs": [],
      "source": [
        "params_converted = [ndarrays_to_parameters(param) for param in params]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is3LfQQLXoDM"
      },
      "outputs": [],
      "source": [
        "results = [(i, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=param, num_examples=len(train_partition), metrics={})) for i, param, train_partition in zip(range(num_partitions), params_converted, train_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxj1CVhoXoDN"
      },
      "outputs": [],
      "source": [
        "from flwr.server.strategy.aggregate import aggregate_inplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvmLuOmPXoDN"
      },
      "outputs": [],
      "source": [
        "aggregated_ndarrays = aggregate_inplace(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-e-StinXoDN"
      },
      "outputs": [],
      "source": [
        "parameters_aggregated_gen = ndarrays_to_parameters(aggregated_ndarrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTzi5zSvXoDP"
      },
      "outputs": [],
      "source": [
        "# Cria uma inst√¢ncia do modelo\n",
        "model = CGAN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wj3iAxHXoDQ"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7ia1SieXoFS",
        "outputId": "bee573da-6d62-4eb0-9314-00c09c3d92f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = next(model.parameters()).device\n",
        "params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
        "state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
        "model.load_state_dict(state_dict, strict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDTkH5S6XoFT"
      },
      "outputs": [],
      "source": [
        "def train_G(net: CGAN, device: str, lr: float, epochs: int, batch_size: int, latent_dim: int):\n",
        "    net.to(device)  # move model to GPU if available\n",
        "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train G\n",
        "        net.zero_grad()\n",
        "        z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "        x_fake = net(z_noise, x_fake_labels)\n",
        "        y_fake_g = net(x_fake, x_fake_labels)\n",
        "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "        g_loss = net.loss(y_fake_g, real_ident)\n",
        "        g_loss.backward()\n",
        "        optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iwNpLftXoFT"
      },
      "outputs": [],
      "source": [
        "train_G(net=model,\n",
        "        device=device,\n",
        "        lr=0.0001,\n",
        "        epochs=2,\n",
        "        batch_size=128,\n",
        "        latent_dim=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoZsab-tXoFU",
        "outputId": "d9cc408c-5563-4d41-bd07-718c97ff8f99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "flwr.common.typing.Parameters"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(parameters_aggregated_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4knrQHgXoFV"
      },
      "outputs": [],
      "source": [
        "params = [val.cpu().numpy() for _, val in model.state_dict().items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZdf9fw0XoFV"
      },
      "outputs": [],
      "source": [
        "param = ndarrays_to_parameters(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnpXx2j_XoFV",
        "outputId": "dcffe647-d5a4-40ea-90b0-390885be8d28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "flwr.common.typing.Parameters"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(param)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gerafed",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006c513f6fd54337835cd1073b081b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ebbb4212b44d70b3c2860c8d98a1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03fe67e2b82e46eda94c5735eb0b3838": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38f5264ea38e490ab1730095821d5732",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b2c2ad47f5714749b1e233d731137a32",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
          }
        },
        "063a8820ba58452d88fdbc48049680cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f4993d5ce94b90896882f75058ad4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c335ee394e5454688e169253ccc202a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71d3ef1e998342b6a909d098edbe5840",
            "max": 60000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb53eb56d6814e3495324cd1720e403f",
            "value": 60000
          }
        },
        "0c64263c0cbb4d6a8bb03085f87f806b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1964b90585344c1880894924f1d058c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2047512a3b8343ea9b84080d1355820b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2184aff93e5e4ad4a2bcba4e1b352353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bec86a3b96264fa4b742f12438b65c89",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9ad15303c12b4fed98d8179f75fa6c6d",
            "value": "‚Äá60000/60000‚Äá[00:01&lt;00:00,‚Äá53659.41‚Äáexamples/s]"
          }
        },
        "28fa1055f85e42d584662c332df28cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c7ed38f9c094e99b04b89a4121085f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e4bf9df1c924c8fb2ac5da26778d5f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "317cc826243e41c6baea198d26fc8ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33c90b7d0dff423bb7bb6b6fe1bcd9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38f5264ea38e490ab1730095821d5732": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ede49350efb4a66bb9d9bd9c50d8402": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5fadde8538c40399d0d6175992a2c08",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_33c90b7d0dff423bb7bb6b6fe1bcd9f5",
            "value": "‚Äá15.6M/15.6M‚Äá[00:00&lt;00:00,‚Äá22.7MB/s]"
          }
        },
        "456b4c7409c54888acd47a0c085b57de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46b96767f5e94b1dab5289f7ca493845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe0dac5b8f4b4b2db3b12745a0ea41bb",
              "IPY_MODEL_4c559859b32d4c8e9ff5bdc179a3dbc2",
              "IPY_MODEL_9fe53a5854df445dad0487dfc9035021"
            ],
            "layout": "IPY_MODEL_7f3112f458a1497bafc6dfa40b3873f6"
          }
        },
        "4c559859b32d4c8e9ff5bdc179a3dbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1964b90585344c1880894924f1d058c5",
            "max": 6971,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79be0691cc304fbc974e2463f055b5ad",
            "value": 6971
          }
        },
        "4f105781904045389b9f80fef1422204": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03fe67e2b82e46eda94c5735eb0b3838",
              "IPY_MODEL_0c335ee394e5454688e169253ccc202a",
              "IPY_MODEL_2184aff93e5e4ad4a2bcba4e1b352353"
            ],
            "layout": "IPY_MODEL_d09cd624408c4b12bca2f98fe5cc9e4d"
          }
        },
        "50ba90dce2634ba7bd282722d41eb7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "518ce70cd0104e37a68d62e9b92c6fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5687f3380b0f4371aab7b74c96bf3eae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "582503ab5a96473f974e7e68491f5ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a673e78c8104c17b88cf65efddf4fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71d3ef1e998342b6a909d098edbe5840": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "748f44fc54004f80b40dc65aa3092679": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fac0eb3e353641ce9aab2b90f03a75a1",
              "IPY_MODEL_d8237488423e4f8aab99caecee63f0be",
              "IPY_MODEL_e9655c60cd43415db6a81d0f688b2bf7"
            ],
            "layout": "IPY_MODEL_2047512a3b8343ea9b84080d1355820b"
          }
        },
        "75abc3232c9a4e71aca3ac1cbc333e65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79be0691cc304fbc974e2463f055b5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f3112f458a1497bafc6dfa40b3873f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "856b2392293943a0a31ce86e48abf3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07f4993d5ce94b90896882f75058ad4a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0c64263c0cbb4d6a8bb03085f87f806b",
            "value": "test-00000-of-00001.parquet:‚Äá100%"
          }
        },
        "928472df19414ba380a724a5d986c5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75abc3232c9a4e71aca3ac1cbc333e65",
            "max": 15561616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28fa1055f85e42d584662c332df28cdb",
            "value": 15561616
          }
        },
        "9ad15303c12b4fed98d8179f75fa6c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fe53a5854df445dad0487dfc9035021": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_582503ab5a96473f974e7e68491f5ef2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_02ebbb4212b44d70b3c2860c8d98a1e1",
            "value": "‚Äá6.97k/6.97k‚Äá[00:00&lt;00:00,‚Äá154kB/s]"
          }
        },
        "a8a6950ad39545de9aaab01e502b1c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2c2ad47f5714749b1e233d731137a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5fadde8538c40399d0d6175992a2c08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be0c4ae734eb405ab74533eb8ae6d677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e4bf9df1c924c8fb2ac5da26778d5f0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a8a6950ad39545de9aaab01e502b1c7e",
            "value": "‚Äá2.60M/2.60M‚Äá[00:00&lt;00:00,‚Äá38.1MB/s]"
          }
        },
        "bec86a3b96264fa4b742f12438b65c89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed1ff516b5e4c33b68312f32fdd7a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf453f0267b841bfbeb8dface06da913": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5687f3380b0f4371aab7b74c96bf3eae",
            "max": 2595890,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50ba90dce2634ba7bd282722d41eb7d6",
            "value": 2595890
          }
        },
        "bfdd786d2c1640728fa4701f3de65680": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c65cbcde25be4b67ad0972777fba7164": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_856b2392293943a0a31ce86e48abf3b4",
              "IPY_MODEL_bf453f0267b841bfbeb8dface06da913",
              "IPY_MODEL_be0c4ae734eb405ab74533eb8ae6d677"
            ],
            "layout": "IPY_MODEL_bfdd786d2c1640728fa4701f3de65680"
          }
        },
        "c7e04c9861094782b045c8eb510f954f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d09cd624408c4b12bca2f98fe5cc9e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a1a9f475f9408e991033c95e4c2a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063a8820ba58452d88fdbc48049680cb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_456b4c7409c54888acd47a0c085b57de",
            "value": "train-00000-of-00001.parquet:‚Äá100%"
          }
        },
        "d8237488423e4f8aab99caecee63f0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e04c9861094782b045c8eb510f954f",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_317cc826243e41c6baea198d26fc8ea7",
            "value": 10000
          }
        },
        "e9655c60cd43415db6a81d0f688b2bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_006c513f6fd54337835cd1073b081b0a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bed1ff516b5e4c33b68312f32fdd7a6f",
            "value": "‚Äá10000/10000‚Äá[00:00&lt;00:00,‚Äá19212.23‚Äáexamples/s]"
          }
        },
        "eb53eb56d6814e3495324cd1720e403f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecd6b36466d7420faa39384fbfff990a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7a1a9f475f9408e991033c95e4c2a7d",
              "IPY_MODEL_928472df19414ba380a724a5d986c5f7",
              "IPY_MODEL_3ede49350efb4a66bb9d9bd9c50d8402"
            ],
            "layout": "IPY_MODEL_f1d0fd160d944dba80868e3024371e1b"
          }
        },
        "f1cf09270eb745ea8364b3e44acf920a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1d0fd160d944dba80868e3024371e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac0eb3e353641ce9aab2b90f03a75a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1cf09270eb745ea8364b3e44acf920a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_518ce70cd0104e37a68d62e9b92c6fc0",
            "value": "Generating‚Äátest‚Äásplit:‚Äá100%"
          }
        },
        "fe0dac5b8f4b4b2db3b12745a0ea41bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a673e78c8104c17b88cf65efddf4fcd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2c7ed38f9c094e99b04b89a4121085f0",
            "value": "README.md:‚Äá100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
