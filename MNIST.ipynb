{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento modelo classificador e geradora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importacoes, carregamento dos dados, definicao da rede classificadora e treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainloader_reduzido = DataLoader(trainset_reduzido, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Salvando imagens MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a random sample of images\n",
    "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    if classes is None:\n",
    "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
    "    \n",
    "    if balanced:\n",
    "        # Get the number of classes\n",
    "        num_classes = len(classes)\n",
    "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
    "        indices = []\n",
    "        class_counts = {i: 0 for i in classes}\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        shuffled_indices = list(range(len(dataset)))\n",
    "        random.shuffle(shuffled_indices)\n",
    "        \n",
    "        for idx in shuffled_indices:\n",
    "            img = dataset[idx][0]\n",
    "            label = int(dataset[idx][1])\n",
    "            if label in classes and class_counts[label] < samples_per_class:\n",
    "                indices.append(idx)\n",
    "                class_counts[label] += 1\n",
    "            if len(indices) >= num_samples:\n",
    "                break\n",
    "    else:\n",
    "        indices = []\n",
    "        while len(indices) < num_samples:\n",
    "            idx = random.randint(0, len(dataset) - 1)\n",
    "            if int(dataset[idx][1]) in classes:\n",
    "                indices.append(idx)\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx]\n",
    "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
    "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
    "        img = Image.fromarray(img)\n",
    "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_random_samples(trainset, num_samples=2048, balanced=True, classes=[i], folder=filter\"Imagens Testes/mnist_samples_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definicao da GAN e funcoes de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN(nn.Module):\n",
    "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
    "        super(CGAN, self).__init__()\n",
    "        if dataset == \"mnist\":\n",
    "            self.classes = 10\n",
    "            self.channels = 1\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "        self.adv_loss = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
    "            *self._create_layer_gen(128, 256),\n",
    "            *self._create_layer_gen(256, 512),\n",
    "            *self._create_layer_gen(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
    "            *self._create_layer_disc(1024, 512, True, True),\n",
    "            *self._create_layer_disc(512, 256, True, True),\n",
    "            *self._create_layer_disc(256, 128, False, False),\n",
    "            *self._create_layer_disc(128, 1, False, False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(0.4))\n",
    "        if act_func:\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        if input.dim() == 2:\n",
    "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
    "            x = self.generator(z)\n",
    "            x = x.view(x.size(0), *self.img_shape) #Em\n",
    "            return x\n",
    "        elif input.dim() == 4:\n",
    "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)\n",
    "\n",
    "\n",
    "def train_gen(net, trainloader, epochs, lr, device, dataset=\"mnist\", latent_dim=100, f2a: bool = False, cliente: bool = False, D=None):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    if dataset == \"mnist\":\n",
    "      imagem = \"image\"\n",
    "    elif dataset == \"cifar10\":\n",
    "      imagem = \"img\"\n",
    "    \n",
    "    net.to(device)  # move model to GPU if available\n",
    "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "            if not f2a: \n",
    "                # Train G\n",
    "                net.zero_grad()\n",
    "                z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "                x_fake = net(z_noise, x_fake_labels)\n",
    "                y_fake_g = net(x_fake, x_fake_labels)\n",
    "                g_loss = net.loss(y_fake_g, real_ident)\n",
    "                g_loss.backward()\n",
    "                optim_G.step()\n",
    "\n",
    "                # Train D\n",
    "                net.zero_grad()\n",
    "                y_real = net(images, labels)\n",
    "                d_real_loss = net.loss(y_real, real_ident)\n",
    "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optim_D.step()\n",
    "\n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "\n",
    "                if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                    print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
    "                                epoch, batch_idx, len(trainloader),\n",
    "                                d_loss.mean().item(),\n",
    "                                g_loss.mean().item())) \n",
    "\n",
    "            else:\n",
    "                if cliente:\n",
    "                    # Train D\n",
    "                    net.zero_grad()\n",
    "                    y_real = net(images, labels)\n",
    "                    d_real_loss = net.loss(y_real, real_ident)\n",
    "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "                    x_fake = net(z_noise, x_fake_labels)\n",
    "                    y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "                    d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "                    d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                    d_loss.backward()\n",
    "                    optim_D.step()\n",
    "\n",
    "                    d_losses.append(d_loss.item())\n",
    "\n",
    "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                        print('Epoch {} [{}/{}] loss_D_treino: {:.4f}'.format(\n",
    "                                epoch, batch_idx, len(trainloader),\n",
    "                                d_loss.mean().item())) \n",
    "                    \n",
    "                else:\n",
    "                    # Train G\n",
    "                    net.zero_grad()\n",
    "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "                    x_fake = net(z_noise, x_fake_labels)\n",
    "                    y_fake_g = net(x_fake, x_fake_labels)\n",
    "                    g_loss = net.loss(y_fake_g, real_ident)\n",
    "                    g_loss.backward()\n",
    "                    optim_G.step()\n",
    "\n",
    "                    g_losses.append(g_loss.item())\n",
    "\n",
    "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                        print('Epoch {} [{}/{}] loss_G_treino: {:.4f}'.format(\n",
    "                                epoch, batch_idx, len(trainloader),\n",
    "                                g_loss.mean().item())) \n",
    "\n",
    "\n",
    "\n",
    "def test(net, testloader, device, dataset=\"mnist\", latent_dim=100):\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    if dataset == \"mnist\":\n",
    "      imagem = \"image\"\n",
    "    elif dataset == \"cifar10\":\n",
    "      imagem = \"img\"\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(testloader):\n",
    "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "            #Gen loss\n",
    "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "            x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "            x_fake = net(z_noise, x_fake_labels)\n",
    "            y_fake_g = net(x_fake, x_fake_labels)\n",
    "            g_loss = net.loss(y_fake_g, real_ident)\n",
    "\n",
    "            #Disc loss\n",
    "            y_real = net(images, labels)\n",
    "            d_real_loss = net.loss(y_real, real_ident)\n",
    "            y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            g_losses.append(g_loss.item())\n",
    "            d_losses.append(d_loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                print('[{}/{}] loss_D_teste: {:.4f} loss_G_teste: {:.4f}'.format(\n",
    "                            batch_idx, len(testloader),\n",
    "                            d_loss.mean().item(),\n",
    "                            g_loss.mean().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações\n",
    "LATENT_DIM = 128\n",
    "LEARNING_RATE = 0.0002\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.9\n",
    "GP_SCALE = 10\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 50\n",
    "# Camada de Convolução para o Discriminador\n",
    "def conv_block(in_channels, out_channels, kernel_size=5, stride=2, padding=2, use_bn=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "    if use_bn:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Discriminador\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(NUM_CHANNELS + NUM_CLASSES, 64, use_bn=False),\n",
    "            conv_block(64, 128, use_bn=True),\n",
    "            conv_block(128, 256, use_bn=True),\n",
    "            conv_block(256, 512, use_bn=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 2 * 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Camada de upsample para o Gerador\n",
    "def upsample_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_bn=True):\n",
    "    layers = [\n",
    "        nn.Upsample(scale_factor=2),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.BatchNorm2d(out_channels) if use_bn else nn.Identity(),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    ]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Gerador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + NUM_CLASSES, 4 * 4 * 256),\n",
    "            nn.BatchNorm1d(4 * 4 * 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Unflatten(1, (256, 4, 4)),\n",
    "            upsample_block(256, 128),\n",
    "            upsample_block(128, 64),\n",
    "            upsample_block(64, 32),\n",
    "            nn.Conv2d(32, NUM_CHANNELS, kernel_size=5, stride=1, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geração de Dados Sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
    "        self.generator = generator\n",
    "        self.num_samples = num_samples\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.model = type(self.generator).__name__\n",
    "        self.images, self.labels = self.generate_data()\n",
    "        self.classes = [i for i in range(self.num_classes)]\n",
    "        \n",
    "\n",
    "    def generate_data(self):\n",
    "        self.generator.eval()\n",
    "        labels = torch.tensor([i for i in range(self.num_classes) for _ in range(self.num_samples // self.num_classes)], device=self.device)\n",
    "        if self.model == 'Generator':\n",
    "            labels_one_hot = F.one_hot(labels, self.num_classes).float().to(self.device) #\n",
    "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            if self.model == 'Generator':\n",
    "                gen_imgs = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
    "            elif self.model == 'CGAN':\n",
    "                gen_imgs = self.generator(z, labels)\n",
    "\n",
    "        return gen_imgs.cpu(), labels.cpu()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 10000\n",
    "latent_dim = 128\n",
    "\n",
    "G = Generator(latent_dim=128).to(\"cpu\")\n",
    "G.load_state_dict(torch.load(\"wgan_43e_128b_0.0002lr.pth\", map_location=torch.device('cpu'))[\"generator\"])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "generated_dataset = GeneratedDataset(generator=G, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
    "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 60000\n",
    "latent_dim = 100 \n",
    "\n",
    "gan = CGAN()\n",
    "gan.load_state_dict(torch.load(\"Imagens Testes/FULL_FEDAVG/epochs20/model_round_10_mnist.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "generated_dataset = GeneratedDataset(generator=gan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
    "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando imagens CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_random_samples(generated_dataset, num_samples=2048, balanced=True, folder='Imagens Testes/cgan_samples_niid_0.7acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "net.train()\n",
    "for epoch in range(5):\n",
    "    for data in generated_dataloader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, loss = 0, 0.0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images = batch[0]\n",
    "        labels = batch[1]\n",
    "        outputs = net(images)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "        correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "accuracy = correct / len(testloader.dataset)\n",
    "loss = loss / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CGAN()\n",
    "train(net=net, \n",
    "      trainloader=trainloader, \n",
    "      epochs=50,\n",
    "      learning_rate=0.0001, \n",
    "      device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'CGAN_50epochs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelos\n",
    "D = Discriminator().to(device)\n",
    "G = Generator(latent_dim=LATENT_DIM).to(device)\n",
    "# Otimizadores\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    " \n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
    "\n",
    " # Função de perda Wasserstein\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return fake_output.mean() - real_output.mean()\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -fake_output.mean()\n",
    "\n",
    "# Função para calcular Gradient Penalty\n",
    "def gradient_penalty(D, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    interpolated = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolated = D(interpolated)\n",
    "    gradients = torch.autograd.grad(outputs=d_interpolated, inputs=interpolated,\n",
    "                                    grad_outputs=torch.ones_like(d_interpolated),\n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = CGAN(latent_dim=128).to(device)\n",
    "optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "historico_metricas = []\n",
    "wgan = True\n",
    "epoch_bar = tqdm(range(EPOCHS), desc=\"Treinamento\", leave=True, position=0)\n",
    "for epoch in epoch_bar:\n",
    "\n",
    "    print(f\"\\n🔹 Epoch {epoch+1}/{EPOCHS}\")\n",
    "    G_loss = 0\n",
    "    D_loss = 0\n",
    "    batches = 0\n",
    "\n",
    "    batch_bar = tqdm(trainloader_reduzido, desc=\"Batches\", leave=False, position=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for real_images, labels in batch_bar:\n",
    "        real_images = real_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch = real_images.size(0)\n",
    "        fake_labels = torch.randint(0, NUM_CLASSES, (batch,), device=device)\n",
    "        z = torch.randn(batch, LATENT_DIM).to(device)\n",
    "        optimizer_D.zero_grad() \n",
    "        if wgan:\n",
    "            labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
    "            fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
    "\n",
    "            # Adicionar labels ao real_images para treinamento do Discriminador\n",
    "            image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
    "            image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
    "        \n",
    "            real_images = torch.cat([real_images, image_labels], dim=1)\n",
    "\n",
    "            # Treinar Discriminador\n",
    "            z = torch.cat([z, fake_labels], dim=1)\n",
    "            fake_images = G(z).detach()\n",
    "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
    "\n",
    "            D(real_images)\n",
    "            loss_D = discriminator_loss(D(real_images), D(fake_images)) + GP_SCALE * gradient_penalty(D, real_images, fake_images)\n",
    "        \n",
    "        else:\n",
    "            real_ident = torch.full((batch, 1), 1., device=device)\n",
    "            fake_ident = torch.full((batch, 1), 0., device=device)\n",
    "            x_fake = gan(z, fake_labels)\n",
    "\n",
    "            y_real = gan(real_images, labels)\n",
    "            d_real_loss = gan.loss(y_real, real_ident)\n",
    "            y_fake_d = gan(x_fake.detach(), fake_labels)\n",
    "            d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
    "            loss_D = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # z = torch.randn(batch, LATENT_DIM).to(device)\n",
    "        # z = torch.cat([z, fake_labels], dim=1)\n",
    "        if wgan:\n",
    "            fake_images = G(z)\n",
    "            loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
    "        else:\n",
    "            y_fake_g = gan(x_fake, fake_labels)\n",
    "            loss_G = gan.loss(y_fake_g, real_ident)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        G_loss += loss_G.item()\n",
    "        D_loss += loss_D.item()\n",
    "        batches += BATCH_SIZE\n",
    "    \n",
    "    avg_epoch_G_loss = G_loss/batches\n",
    "    avg_epoch_D_loss = D_loss/batches\n",
    "    # Create the dataset and dataloader\n",
    "    if wgan:\n",
    "        generated_dataset = GeneratedDataset(generator=G, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
    "    else:\n",
    "        generated_dataset = GeneratedDataset(generator=gan, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
    "    generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    net = Net()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    net.train()\n",
    "    for _ in range(5):\n",
    "        for data in generated_dataloader:\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    net.eval()\n",
    "    correct, loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    epoch_bar.set_postfix({\n",
    "        \"D_loss\": f\"{avg_epoch_D_loss:.4f}\",\n",
    "        \"G_loss\": f\"{avg_epoch_G_loss:.4f}\",\n",
    "        \"Acc\": f\"{accuracy:.4f}\"\n",
    "    })\n",
    "\n",
    "    with open(\"Treino_GAN.txt\", \"a\") as f:\n",
    "            f.write(f\"Epoca: {epoch+1}, D_loss: {avg_epoch_D_loss:.4f}, G_loss: {avg_epoch_G_loss:.4f}, Acc: {accuracy:.4f}, Tempo: {total_time:.4f}\\n\")\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    #Atualiza o learning_rate\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "    print(f\"Após Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
    "    if wgan:\n",
    "         # Salvar modelo a cada época\n",
    "        torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, f\"wgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
    "    else:\n",
    "        torch.save(gan.state_dict(), f\"cgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
    "        \n",
    "print(\"✅ Treinamento Concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, \"wgan_29e_64b_0.002lr.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ajuste de hiperparametro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import optuna\n",
    "from optuna.importance import get_param_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=\"cgan\"\n",
    "EPOCHS = 5\n",
    "# Função Objetiva (a ser otimizada pelo Optuna)\n",
    "def objective(trial):\n",
    "    # Escolher os hiperparâmetros dentro de um intervalo\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 1024)\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 10, 1000)\n",
    "    lr = trial.suggest_float(\"learning_rate\", 0.0001, 0.05, log=True)\n",
    "    beta1 = trial.suggest_float(\"beta1\", 0.0, 0.9)\n",
    "    beta2 = trial.suggest_float(\"beta2\", 0.8, 0.999)\n",
    "    global model\n",
    "    model = model.lower()\n",
    "    if model==\"wgan\":\n",
    "        gp_scale = trial.suggest_int(\"gp_scale\", 0, 100)\n",
    "\n",
    "    # Criar DataLoader com batch_size otimizado\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Criar novos modelos e otimizadores\n",
    "    if model == \"wgan\":\n",
    "        D = Discriminator().to(device)\n",
    "        G = Generator(latent_dim=latent_dim).to(device)\n",
    "        optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "        G.train()\n",
    "        D.train()\n",
    "    elif model == \"cgan\":\n",
    "        gan = CGAN(latent_dim=latent_dim).to(device)\n",
    "        optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    \n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
    "    scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    # Treinar por algumas épocas\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_batches = 0\n",
    "        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for real_images, labels in progress_bar:\n",
    "            real_images = real_images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            fake_labels = torch.randint(0, NUM_CLASSES, (real_images.size(0),), device=device)\n",
    "            z = torch.randn(real_images.size(0), latent_dim).to(device)\n",
    "            optimizer_D.zero_grad()\n",
    "            if model==\"wgan\":\n",
    "                labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
    "                fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
    "\n",
    "                # Adicionar labels ao real_images para treinamento do Discriminador\n",
    "                image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
    "                image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
    "        \n",
    "                real_images = torch.cat([real_images, image_labels], dim=1)\n",
    "\n",
    "                # Treinar Discriminador\n",
    "                z = torch.cat([z, labels], dim=1)\n",
    "                fake_images = G(z).detach()\n",
    "                fake_images = torch.cat([fake_images, image_labels], dim=1)\n",
    "\n",
    "                loss_D = discriminator_loss(D(real_images), D(fake_images)) + gp_scale * gradient_penalty(D, real_images, fake_images)\n",
    "           \n",
    "           \n",
    "            else:\n",
    "                real_ident = torch.full((real_images.size(0), 1), 1., device=device)\n",
    "                fake_ident = torch.full((real_images.size(0), 1), 0., device=device)\n",
    "                x_fake = gan(z, fake_labels)\n",
    "\n",
    "                y_real = gan(real_images, labels)\n",
    "                d_real_loss = gan.loss(y_real, real_ident)\n",
    "                y_fake_d = gan(x_fake.detach(), fake_labels)\n",
    "                d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
    "                loss_D = (d_real_loss + d_fake_loss) / 2          \n",
    "           \n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "\n",
    "            # Treinar Gerador\n",
    "            optimizer_G.zero_grad()\n",
    "  \n",
    "            if model==\"wgan\":\n",
    "                fake_images = G(z)\n",
    "                loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
    "            else:\n",
    "                y_fake_g = gan(x_fake, fake_labels)\n",
    "                loss_G = gan.loss(y_fake_g, real_ident)\n",
    "            \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            epoch_loss += loss_G.item()\n",
    "            total_loss += loss_G.item()\n",
    "            total_batches += 1\n",
    "            epoch_batches += 1\n",
    "\n",
    "            progress_bar.set_postfix(d_loss=loss_D.item(), g_loss=loss_G.item())\n",
    "\n",
    "        # Calcular a loss média dessa época\n",
    "        epoch_avg_loss = epoch_loss / epoch_batches\n",
    "        # Reporta a loss média da época para pruning\n",
    "        trial.report(epoch_avg_loss, epoch)\n",
    "        if trial.should_prune() and epoch >=3:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "        print(f\"Após Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
    "   \n",
    "            \n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "\n",
    "    return avg_loss  # Optuna tentará minimizar essa métrica\n",
    "\n",
    "# Criar estudo do Optuna e otimizar hiperparâmetros\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Exibir os melhores hiperparâmetros encontrados\n",
    "print(\"\\n🔹 Melhores Hiperparâmetros Encontrados:\")\n",
    "print(study.best_params)\n",
    "\n",
    "importance = get_param_importances(study)\n",
    "print(\"Hyperparameter Importances:\")\n",
    "for param, imp in importance.items():\n",
    "    print(f\"{param}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste para qualidade visual das imagens geradas pelo modelo generativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "latent_dim = 128\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "path = \"\"\n",
    "device = \"cpu\"\n",
    "net = CGAN(dataset=\"mnist\", latent_dim=latent_dim).to(device)\n",
    "net.load_state_dict(torch.load(f'{path}/model_round_5_mnist.pt'))\n",
    "# G = Generator(latent_dim=128)\n",
    "# G.load_state_dict(torch.load(\"wgan_5e_512b_0.002lr_0.5B1_0.9B2_10gp_128_ld.pth\")[\"generator\"])\n",
    "#net = CGAN()\n",
    "#net.load_state_dict(torch.load('CGAN_50epochs.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "#net.eval()\n",
    "G.eval()\n",
    "# Assuming netG is your generator model, classes is the total number of classes, and latent_dim is the latent vector size\n",
    "examples_per_class = 5\n",
    "classes = 10\n",
    "batch_size = examples_per_class * classes  # Generate enough images to have `examples_per_class` for each class\n",
    "\n",
    "# Generate latent vectors and corresponding labels\n",
    "latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
    "labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
    "labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    #generated_images = net(latent_vectors, labels)\n",
    "    generated_images = G(torch.cat([latent_vectors, labels], dim=1))\n",
    "\n",
    "# Criar uma figura com 10 linhas e 5 colunas de subplots\n",
    "fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
    "\n",
    "#fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "fig.text(0.5, 0.98, f\"Round: {5}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "# Exibir as imagens nos subplots\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Ajustar o layout antes de calcular as posições\n",
    "plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
    "\n",
    "# Reduzir espaço entre colunas\n",
    "# plt.subplots_adjust(wspace=0.05)\n",
    "\n",
    "# Adicionar os rótulos das classes corretamente alinhados\n",
    "fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
    "for row in range(classes):\n",
    "    # Obter posição do subplot em coordenadas da figura\n",
    "    bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
    "    pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
    "    center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
    "\n",
    "    # Adicionar o rótulo\n",
    "    fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
    "    plt.savefig(f\"{path}teste.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(image_files, output_path, duration=200):\n",
    "    \"\"\"\n",
    "    Cria um GIF animado a partir de uma sequência de imagens.\n",
    "\n",
    "    Args:\n",
    "        image_files (list): Lista de caminhos das imagens.\n",
    "        output_path (str): Caminho para salvar o GIF.\n",
    "        duration (int): Tempo de exibição de cada frame (em ms).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    frames = [Image.open(img) for img in image_files]  # Carregar imagens\n",
    "    frames[0].save(output_path, format=\"GIF\", save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "    display(IPImage(filename=output_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "image_files = [\"../imagens geradas/mnist_CGAN_r0_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
    "               \"../imagens geradas/mnist_CGAN_r1_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
    "               \"../imagens geradas/mnist_CGAN_r2_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
    "               \"../imagens geradas/mnist_CGAN_r3_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
    "               \"../imagens geradas/mnist_CGAN_r4_100e_64_100z_10c_0.0001lr_niid_01dir.png\"]\n",
    "create_gif(image_files, \"global.gif\", duration=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_federated_collage(\n",
    "    agg_image_paths,       # Lista de caminhos para as imagens grandes (1 por round)\n",
    "    clients_image_paths,   # Lista de listas: para cada round, lista de caminhos de imagens de clientes\n",
    "    big_scale=2,           # Escala da imagem grande em relação à imagem pequena\n",
    "    small_size=(500, 900),   # Tamanho desejado para cada imagem pequena (largura, altura)\n",
    "    h_gap=0,               # Espaço horizontal entre bloco de imagens\n",
    "    background_color=(255, 255, 255),\n",
    "    save_path=\"collage.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cria um mosaico onde cada round tem:\n",
    "      - 1 imagem \"agregada\" (maior) à esquerda\n",
    "      - N imagens de cliente empilhadas verticalmente à direita\n",
    "\n",
    "    Parâmetros:\n",
    "      agg_image_paths      : lista de strings (caminhos) para as imagens agregadas (1 por round)\n",
    "      clients_image_paths  : lista de listas de strings. Cada sublista é a lista de caminhos das imagens de cada cliente daquele round\n",
    "      big_scale            : fator de escala da imagem grande em relação às pequenas\n",
    "      small_size           : (largura, altura) desejado para cada imagem pequena\n",
    "      background_color     : cor de fundo do mosaico (RGB)\n",
    "      save_path            : caminho do arquivo final a ser salvo\n",
    "\n",
    "    Retorna:\n",
    "      Um objeto PIL.Image com o mosaico criado.\n",
    "    \"\"\"\n",
    "    # Verifica se temos a mesma quantidade de rounds em agg_image_paths e clients_image_paths\n",
    "    assert len(agg_image_paths) == len(clients_image_paths), \\\n",
    "        \"Número de imagens agregadas deve bater com número de listas de clientes.\"\n",
    "\n",
    "    # Carrega todas as imagens agregadas (rounds)\n",
    "    agg_images = []\n",
    "    for path in agg_image_paths:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        agg_images.append(img)\n",
    "\n",
    "    # Carrega todas as imagens de clientes\n",
    "    # clients_image_paths é lista de listas, cada sublista para um round\n",
    "    client_images = []\n",
    "    for round_paths in clients_image_paths:\n",
    "        imgs = [Image.open(p).convert(\"RGB\") for p in round_paths]\n",
    "        client_images.append(imgs)\n",
    "\n",
    "    # Dimensiona as imagens pequenas para small_size\n",
    "    # e as grandes para (big_scale * small_size)\n",
    "    small_w, small_h = small_size\n",
    "    big_w, big_h = big_scale * small_w, big_scale * small_h\n",
    "\n",
    "    # Faz o resize de todas as imagens\n",
    "    for i, img in enumerate(agg_images):\n",
    "        agg_images[i] = img.resize((big_w, big_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "    for i, imgs in enumerate(client_images):\n",
    "        resized_list = []\n",
    "        for im in imgs:\n",
    "            resized_list.append(im.resize((small_w, small_h), Image.Resampling.LANCZOS))\n",
    "        client_images[i] = resized_list\n",
    "\n",
    "    # Calcula quantos rounds e quantos clientes\n",
    "    num_rounds = len(agg_images)\n",
    "\n",
    "    # Para cada round, vamos colocar:\n",
    "    # - Imagem grande (largura big_w, altura big_h)\n",
    "    # - N clientes empilhados (cada um small_h de altura, total N * small_h)\n",
    "    # A largura de cada \"bloco\" de round = (big_w + small_w)\n",
    "    # A altura do bloco = max(big_h, N * small_h) (para acomodar todas as imagens)\n",
    "\n",
    "    # Descobre o número máximo de clientes em qualquer round (para dimensionar corretamente)\n",
    "    max_clients = max(len(imgs) for imgs in client_images)\n",
    "\n",
    "    # Altura total do bloco para cada round\n",
    "    block_h = max(big_h + small_h, max_clients * small_h)\n",
    "    block_w = big_w + small_w  # Largura do bloco do round\n",
    "\n",
    "    # Largura total = num_rounds * block_w\n",
    "    # Altura total = block_h (vamos colocar rounds lado a lado)\n",
    "    total_w = num_rounds * block_w  + h_gap*2*num_rounds-1\n",
    "    total_h = block_h\n",
    "\n",
    "    # Cria imagem de fundo\n",
    "    collage = Image.new(\"RGB\", (total_w, total_h), color=background_color)\n",
    "\n",
    "    # Posiciona cada round\n",
    "    for r in range(num_rounds):\n",
    "        # Posição x para este round\n",
    "        x_offset = r * block_w + 2*r*h_gap\n",
    "\n",
    "        # Coloca a imagem grande (agg)\n",
    "        collage.paste(agg_images[r], (x_offset, small_h))\n",
    "\n",
    "        # Agora empilha as imagens de cliente ao lado (à direita da imagem grande)\n",
    "        y_offset = 0\n",
    "        for c_img in client_images[r]:\n",
    "            collage.paste(c_img, (x_offset + big_w + h_gap, y_offset))\n",
    "            y_offset += small_h\n",
    "\n",
    "    # Salva o resultado\n",
    "    collage.save(save_path)\n",
    "    print(f\"Mosaico criado e salvo em: {save_path}\")\n",
    "\n",
    "    return collage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_image_paths = [f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir.png\" for i in range(10, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_image_paths = [[f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir_cliente{j}.png\" for j in range(4)] for i in range(11, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_federated_collage(\n",
    "    agg_image_paths=agg_image_paths,\n",
    "    clients_image_paths=client_image_paths,\n",
    "    big_scale=2,\n",
    "    small_size=(500, 900),\n",
    "    h_gap=80,\n",
    "    background_color=(255, 255, 255),\n",
    "    save_path=f\"{path}CGAN_evol.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importacoes, classes e configuracoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_cpus = os.cpu_count()\n",
    "num_workers = min(8, num_cpus,0)\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "dims = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inception_v3(*args, **kwargs):\n",
    "    \"\"\"Wraps `torchvision.models.inception_v3`\"\"\"\n",
    "    try:\n",
    "        version = tuple(map(int, torchvision.__version__.split(\".\")[:2]))\n",
    "    except ValueError:\n",
    "        # Just a caution against weird version strings\n",
    "        version = (0,)\n",
    "\n",
    "    # Skips default weight inititialization if supported by torchvision\n",
    "    # version. See https://github.com/mseitzer/pytorch-fid/issues/28.\n",
    "    if version >= (0, 6):\n",
    "        kwargs[\"init_weights\"] = False\n",
    "\n",
    "    # Backwards compatibility: `weights` argument was handled by `pretrained`\n",
    "    # argument prior to version 0.13.\n",
    "    if version < (0, 13) and \"weights\" in kwargs:\n",
    "        if kwargs[\"weights\"] == \"DEFAULT\":\n",
    "            kwargs[\"pretrained\"] = True\n",
    "        elif kwargs[\"weights\"] is None:\n",
    "            kwargs[\"pretrained\"] = False\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"weights=={} not supported in torchvision {}\".format(\n",
    "                    kwargs[\"weights\"], torchvision.__version__\n",
    "                )\n",
    "            )\n",
    "        del kwargs[\"weights\"]\n",
    "\n",
    "    return torchvision.models.inception_v3(*args, **kwargs)\n",
    "\n",
    "\n",
    "def fid_inception_v3():\n",
    "    \"\"\"Build pretrained Inception model for FID computation\n",
    "\n",
    "    The Inception model for FID computation uses a different set of weights\n",
    "    and has a slightly different structure than torchvision's Inception.\n",
    "\n",
    "    This method first constructs torchvision's Inception and then patches the\n",
    "    necessary parts that are different in the FID Inception model.\n",
    "    \"\"\"\n",
    "    inception = _inception_v3(num_classes=1008, aux_logits=False, weights=None)\n",
    "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
    "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
    "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
    "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
    "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
    "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
    "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
    "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
    "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
    "\n",
    "    state_dict = load_state_dict_from_url(\"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\", progress=True)\n",
    "    inception.load_state_dict(state_dict)\n",
    "    return inception\n",
    "\n",
    "\n",
    "class FIDInceptionA(torchvision.models.inception.InceptionA):\n",
    "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, pool_features):\n",
    "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(\n",
    "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
    "        )\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionC(torchvision.models.inception.InceptionC):\n",
    "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, channels_7x7):\n",
    "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(\n",
    "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
    "        )\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionE_1(torchvision.models.inception.InceptionE):\n",
    "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
    "        # its average calculation\n",
    "        branch_pool = F.avg_pool2d(\n",
    "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
    "        )\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n",
    "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        # Patch: The FID Inception model uses max pooling instead of average\n",
    "        # pooling. This is likely an error in this specific Inception\n",
    "        # implementation, as other Inception models use average pooling here\n",
    "        # (which matches the description in the paper).\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
    "\n",
    "    # Index of default block of inception to return,\n",
    "    # corresponds to output of final average pooling\n",
    "    DEFAULT_BLOCK_INDEX = 3\n",
    "\n",
    "    # Maps feature dimensionality to their output blocks indices\n",
    "    BLOCK_INDEX_BY_DIM = {\n",
    "        64: 0,  # First max pooling features\n",
    "        192: 1,  # Second max pooling featurs\n",
    "        768: 2,  # Pre-aux classifier features\n",
    "        2048: 3,  # Final average pooling features\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_blocks=(DEFAULT_BLOCK_INDEX,),\n",
    "        resize_input=True,\n",
    "        normalize_input=True,\n",
    "        requires_grad=False,\n",
    "        use_fid_inception=True,\n",
    "    ):\n",
    "        \"\"\"Build pretrained InceptionV3\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_blocks : list of int\n",
    "            Indices of blocks to return features of. Possible values are:\n",
    "                - 0: corresponds to output of first max pooling\n",
    "                - 1: corresponds to output of second max pooling\n",
    "                - 2: corresponds to output which is fed to aux classifier\n",
    "                - 3: corresponds to output of final average pooling\n",
    "        resize_input : bool\n",
    "            If true, bilinearly resizes input to width and height 299 before\n",
    "            feeding input to model. As the network without fully connected\n",
    "            layers is fully convolutional, it should be able to handle inputs\n",
    "            of arbitrary size, so resizing might not be strictly needed\n",
    "        normalize_input : bool\n",
    "            If true, scales the input from range (0, 1) to the range the\n",
    "            pretrained Inception network expects, namely (-1, 1)\n",
    "        requires_grad : bool\n",
    "            If true, parameters of the model require gradients. Possibly useful\n",
    "            for finetuning the network\n",
    "        use_fid_inception : bool\n",
    "            If true, uses the pretrained Inception model used in Tensorflow's\n",
    "            FID implementation. If false, uses the pretrained Inception model\n",
    "            available in torchvision. The FID Inception model has different\n",
    "            weights and a slightly different structure from torchvision's\n",
    "            Inception model. If you want to compute FID scores, you are\n",
    "            strongly advised to set this parameter to true to get comparable\n",
    "            results.\n",
    "        \"\"\"\n",
    "        super(InceptionV3, self).__init__()\n",
    "\n",
    "        self.resize_input = resize_input\n",
    "        self.normalize_input = normalize_input\n",
    "        self.output_blocks = sorted(output_blocks)\n",
    "        self.last_needed_block = max(output_blocks)\n",
    "\n",
    "        assert self.last_needed_block <= 3, \"Last possible output block index is 3\"\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        if use_fid_inception:\n",
    "            inception = fid_inception_v3()\n",
    "        else:\n",
    "            inception = _inception_v3(weights=\"DEFAULT\")\n",
    "\n",
    "        # Block 0: input to maxpool1\n",
    "        block0 = [\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        ]\n",
    "        self.blocks.append(nn.Sequential(*block0))\n",
    "\n",
    "        # Block 1: maxpool1 to maxpool2\n",
    "        if self.last_needed_block >= 1:\n",
    "            block1 = [\n",
    "                inception.Conv2d_3b_1x1,\n",
    "                inception.Conv2d_4a_3x3,\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block1))\n",
    "\n",
    "        # Block 2: maxpool2 to aux classifier\n",
    "        if self.last_needed_block >= 2:\n",
    "            block2 = [\n",
    "                inception.Mixed_5b,\n",
    "                inception.Mixed_5c,\n",
    "                inception.Mixed_5d,\n",
    "                inception.Mixed_6a,\n",
    "                inception.Mixed_6b,\n",
    "                inception.Mixed_6c,\n",
    "                inception.Mixed_6d,\n",
    "                inception.Mixed_6e,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block2))\n",
    "\n",
    "        # Block 3: aux classifier to final avgpool\n",
    "        if self.last_needed_block >= 3:\n",
    "            block3 = [\n",
    "                inception.Mixed_7a,\n",
    "                inception.Mixed_7b,\n",
    "                inception.Mixed_7c,\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block3))\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"Get Inception feature maps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.autograd.Variable\n",
    "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
    "            range (0, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of torch.autograd.Variable, corresponding to the selected output\n",
    "        block, sorted ascending by index\n",
    "        \"\"\"\n",
    "        outp = []\n",
    "        x = inp\n",
    "\n",
    "        if self.resize_input:\n",
    "            x = F.interpolate(x, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
    "\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.output_blocks:\n",
    "                outp.append(x)\n",
    "\n",
    "            if idx == self.last_needed_block:\n",
    "                break\n",
    "\n",
    "        return outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculo da distribuicao gerada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3([block_idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePathDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files, transforms=None):\n",
    "        self.files = files\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por imagens geradas prontas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../imagens geradas/cgan_samples\"\n",
    "path = pathlib.Path(path)\n",
    "files = sorted(file for file in path.glob(\"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = np.empty((len(files), dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImagePathDataset(files, transforms=torchvision.transforms.ToTensor())\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por modelo pre-treinado gerando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN(nn.Module):\n",
    "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
    "        super(CGAN, self).__init__()\n",
    "        if dataset == \"mnist\":\n",
    "            self.classes = 10\n",
    "            self.channels = 1\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
    "        self.adv_loss = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
    "            *self._create_layer_gen(128, 256),\n",
    "            *self._create_layer_gen(256, 512),\n",
    "            *self._create_layer_gen(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
    "            *self._create_layer_disc(1024, 512, True, True),\n",
    "            *self._create_layer_disc(512, 256, True, True),\n",
    "            *self._create_layer_disc(256, 128, False, False),\n",
    "            *self._create_layer_disc(128, 1, False, False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        #self._initialize_weights()\n",
    "\n",
    "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
    "        layers = [nn.Linear(size_in, size_out)]\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(0.4))\n",
    "        if act_func:\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Itera sobre todos os módulos da rede geradora\n",
    "        for m in self.generator:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        if input.dim() == 2:\n",
    "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
    "            x = self.generator(z)\n",
    "            x = x.view(x.size(0), *self.img_shape) #Em\n",
    "            return x\n",
    "        elif input.dim() == 4:\n",
    "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan = CGAN()\n",
    "cgan.load_state_dict(torch.load(\"CGAN_50epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "from datasets import Dataset, Features, ClassLabel\n",
    "from datasets import Image as IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
    "        self.generator = generator\n",
    "        self.num_samples = num_samples\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.model = type(self.generator).__name__\n",
    "        self.images = self.generate_data()\n",
    "        self.classes = [i for i in range(self.num_classes)]\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "        gen_imgs = {}\n",
    "        self.generator.eval()\n",
    "        labels = {c: torch.tensor([c for i in range(self.num_samples)], device=self.device) for c in range(self.num_classes)}\n",
    "        for c, label in labels.items():\n",
    "          if self.model == 'Generator':\n",
    "              labels_one_hot = F.one_hot(label, self.num_classes).float().to(self.device) #\n",
    "          z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
    "          with torch.no_grad():\n",
    "              if self.model == 'Generator':\n",
    "                  gen_imgs_class = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
    "              elif self.model == 'CGAN':\n",
    "                  gen_imgs_class = self.generator(z, label)\n",
    "          gen_imgs[c] = gen_imgs_class\n",
    "\n",
    "        return gen_imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples * self.num_classes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Mapear o índice global para (classe, índice interno)\n",
    "        class_idx = idx // self.num_samples\n",
    "        sample_idx = idx % self.num_samples\n",
    "        # Retorna apenas a imagem (sem o rótulo)\n",
    "        return self.images[class_idx][sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 2048\n",
    "latent_dim = 100\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "generated_dataset = GeneratedDataset(generator=cgan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
    "gen_dataset = generated_dataset.images\n",
    "\n",
    "for c in gen_dataset.keys():\n",
    "    gen_dataset[c] = (gen_dataset[c] + 1) / 2\n",
    "    gen_dataset[c] = gen_dataset[c].repeat(1, 3, 1, 1)\n",
    "# # Ajustar para o intervalo [0, 1]\n",
    "# gen_dataset = (gen_dataset + 1) / 2\n",
    "# Expandir o canal para RGB (replicando o canal 1 para 3)\n",
    "# gen_dataset = gen_dataset.repeat(1, 3, 1, 1)  # Agora tem shape [2050, 3, 28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [torch.utils.data.DataLoader(gen_dataset[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_gen = []\n",
    "sigmas_gen = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(10):\n",
    "  pred_arr = np.empty((len(gen_dataset[c]), dims))\n",
    "  start_idx = 0\n",
    "  for batch in tqdm(dataloaders[c]):\n",
    "          batch = batch.to(device)\n",
    "\n",
    "          with torch.no_grad():\n",
    "              pred = model(batch)[0]\n",
    "\n",
    "          # If model output is not scalar, apply global spatial average pooling.\n",
    "          # This happens if you choose a dimensionality not equal 2048.\n",
    "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "\n",
    "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
    "\n",
    "          start_idx = start_idx + pred.shape[0]\n",
    "  mus_gen.append(np.mean(pred_arr, axis=0))\n",
    "  sigmas_gen.append(np.cov(pred_arr, rowvar=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da distribuicao real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
    "# Create data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "trainloader_reduzido = torch.utils.data.DataLoader(trainset_reduzido, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegando imagens sem salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_samples_per_class(dataset, num_samples):\n",
    "    \"\"\"\n",
    "    Selects a specified number of samples per class from the dataset and returns them as tensors.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (torch.utils.data.Dataset): The dataset to select samples from.\n",
    "    num_samples (int): The number of samples to select per class.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where each key corresponds to a class and the value is a tensor of shape [num_samples, 1, 28, 28].\n",
    "    \"\"\"\n",
    "    class_samples = {i: [] for i in range(len(dataset.classes))}\n",
    "    class_counts = {i: 0 for i in range(len(dataset.classes))}\n",
    "\n",
    "    for img, label in dataset:\n",
    "        if class_counts[label] < num_samples:\n",
    "            class_samples[label].append(img)\n",
    "            class_counts[label] += 1\n",
    "        if all(count >= num_samples for count in class_counts.values()):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Warning: Not all classes have the requested number of samples.\")\n",
    "\n",
    "    # Convert lists of tensors to a single tensor per class\n",
    "    for label in class_samples:\n",
    "        if class_samples[label]:  # Check if the list is not empty\n",
    "            class_samples[label] = torch.stack(class_samples[label], dim=0)\n",
    "            class_samples[label] = (class_samples[label] + 1) / 2\n",
    "            class_samples[label] = class_samples[label].repeat(1, 3, 1, 1)\n",
    "        else:\n",
    "            # Handle empty classes if necessary; here we leave an empty tensor\n",
    "            class_samples[label] = torch.Tensor()\n",
    "\n",
    "    return class_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reais = select_samples_per_class(testset, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [torch.utils.data.DataLoader(img_reais[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a random sample of images\n",
    "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    if classes is None:\n",
    "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
    "\n",
    "    if balanced:\n",
    "        # Get the number of classes\n",
    "        num_classes = len(classes)\n",
    "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
    "        indices = []\n",
    "        class_counts = {i: 0 for i in classes}\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        shuffled_indices = list(range(len(dataset)))\n",
    "        random.shuffle(shuffled_indices)\n",
    "\n",
    "        for idx in shuffled_indices:\n",
    "            img = dataset[idx][0]\n",
    "            label = int(dataset[idx][1])\n",
    "            if label in classes and class_counts[label] < samples_per_class:\n",
    "                indices.append(idx)\n",
    "                class_counts[label] += 1\n",
    "            if len(indices) >= num_samples:\n",
    "                break\n",
    "    else:\n",
    "        indices = []\n",
    "        while len(indices) < num_samples:\n",
    "            idx = random.randint(0, len(dataset) - 1)\n",
    "            if int(dataset[idx][1]) in classes:\n",
    "                indices.append(idx)\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx]\n",
    "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
    "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
    "        img = Image.fromarray(img)\n",
    "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  save_random_samples(trainset, num_samples=2050, folder=f'Imagens Testes/mnist_samples_{i}', balanced=True, classes=[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathes = [f\"Imagens Testes/mnist_samples_{i}\" for i in range(10)]\n",
    "pathes = [pathlib.Path(path) for path in pathes]\n",
    "files = [sorted(file for file in path.glob(\"*.png\")) for path in pathes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ImagePathDataset(file, transforms=torchvision.transforms.ToTensor()) for file in files]\n",
    "dataloaders = [torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_real = []\n",
    "sigmas_real = []\n",
    "for c in range(10):\n",
    "  model = InceptionV3([block_idx]).to(device)\n",
    "  model.eval()\n",
    "  pred_arr = np.empty((len(img_reais[0]), dims))\n",
    "  start_idx = 0\n",
    "  for batch in tqdm(dataloaders[c]):\n",
    "          batch = batch.to(device)\n",
    "\n",
    "          with torch.no_grad():\n",
    "              pred = model(batch)[0]\n",
    "\n",
    "          # If model output is not scalar, apply global spatial average pooling.\n",
    "          # This happens if you choose a dimensionality not equal 2048.\n",
    "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "\n",
    "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
    "\n",
    "          start_idx = start_idx + pred.shape[0]\n",
    "  mus_real.append(np.mean(pred_arr, axis=0))\n",
    "  sigmas_real.append(np.cov(pred_arr, rowvar=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_gen = [np.atleast_1d(mu_gen) for mu_gen in mus_gen]\n",
    "mus_real = [np.atleast_1d(mu_real) for mu_real in mus_real]\n",
    "\n",
    "sigmas_gen = [np.atleast_2d(sigma_gen) for sigma_gen in sigmas_gen]\n",
    "sigmas_real = [np.atleast_2d(sigma_real) for sigma_real in sigmas_real]\n",
    "\n",
    "for mu_gen, mu_real, sigma_gen, sigma_real in zip(mus_gen, mus_real, sigmas_gen, sigmas_real):\n",
    "  assert (\n",
    "      mu_gen.shape == mu_real.shape\n",
    "  ), \"Training and test mean vectors have different lengths\"\n",
    "  assert (\n",
    "      sigma_gen.shape == sigma_real.shape\n",
    "  ), \"Training and test covariances have different dimensions\"\n",
    "\n",
    "diffs = [mu_gen - mu_real for mu_gen, mu_real in zip(mus_gen, mus_real)]\n",
    "\n",
    "# Product might be almost singular\n",
    "covmeans = [linalg.sqrtm(sigmas_gen.dot(sigmas_real), disp=False)[0] for sigmas_gen, sigmas_real in zip(sigmas_gen, sigmas_real)]\n",
    "for covmean, sigma_gen, sigma_real in zip(covmeans, sigmas_gen, sigmas_real):\n",
    "  if not np.isfinite(covmean).all():\n",
    "    msg = (\n",
    "        \"fid calculation produces singular product; \"\n",
    "        \"adding %s to diagonal of cov estimates\"\n",
    "    ) % 1e-6\n",
    "    print(msg)\n",
    "    offset = np.eye(sigma_gen.shape[0]) * 1e-6\n",
    "    covmean = linalg.sqrtm((sigma_gen + offset).dot(sigma_real + offset))\n",
    "\n",
    "# Numerical error might give slight imaginary component\n",
    "for i, covmean in enumerate(covmeans):\n",
    "  if np.iscomplexobj(covmean):\n",
    "      if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "          m = np.max(np.abs(covmean.imag))\n",
    "          raise ValueError(\"Imaginary component {}\".format(m))\n",
    "      covmeans[i] = covmean.real\n",
    "\n",
    "tr_covmeans = [np.trace(covmean) for covmean in covmeans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = [diff.dot(diff) + np.trace(sigma_gen) + np.trace(sigma_real) - 2 * tr_covmean for diff, sigma_gen, sigma_real, tr_covmean in zip(diffs, sigmas_gen, sigmas_real, tr_covmeans)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "from flwr_datasets import FederatedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 4\n",
    "alpha_dir = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioner = DirichletPartitioner(\n",
    "    num_partitions=num_partitions,\n",
    "    partition_by=\"label\",\n",
    "    alpha=alpha_dir,\n",
    "    min_partition_size=0,\n",
    "    self_balancing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FederatedDataset(\n",
    "    dataset=\"mnist\",\n",
    "    partitioners={\"train\": partitioner}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_partitions = [fds.load_partition(i, split=\"train\") for i in range(num_partitions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = [int(len(train_partition)/10) for train_partition in train_partitions]\n",
    "train_partition = [train_partition.select(range(n)) for train_partition, n in zip(train_partitions, num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def apply_transforms(batch):\n",
    "    batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_partitions = [train_partition.with_transform(apply_transforms) for train_partition in train_partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "trainloaders = [DataLoader(train_partition, batch_size=batch_size, shuffle=True) for train_partition in train_partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [CGAN() for i in range(num_partitions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma total dos valores dos parâmetros do gerador do modelo 0: 1838.0620361119509\n",
      "Soma total dos valores dos parâmetros do gerador do modelo 1: 1853.2193505465984\n",
      "Soma total dos valores dos parâmetros do gerador do modelo 2: 1794.9006606638432\n",
      "Soma total dos valores dos parâmetros do gerador do modelo 3: 1785.6598414629698\n"
     ]
    }
   ],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    generator_params = [param.data.numpy() for param in model.generator.parameters()]\n",
    "    generator_params_sum = sum([param.sum() for param in generator_params])\n",
    "    print(f\"Soma total dos valores dos parâmetros do gerador do modelo {idx}: {generator_params_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma total dos valores dos parâmetros do gerador do modelo 0: 328.63008058443666\n",
      "Soma total dos valores dos parâmetros do gerador do modelo 1: 237.89485466852784\n",
      "Soma total dos valores dos parâmetros do gerador do modelo 2: 186.06141594797373\n",
      "Soma total dos valores dos parâmetros do gerador do modelo 3: 222.37551382556558\n"
     ]
    }
   ],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    disc_params = [param.data.numpy() for param in model.discriminator.parameters()]\n",
    "    disc_params_sum = sum([param.sum() for param in disc_params])\n",
    "    print(f\"Soma total dos valores dos parâmetros do gerador do modelo {idx}: {disc_params_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [100/134] loss_D_treino: 0.0004\n",
      "Epoch 1 [100/134] loss_D_treino: 0.0001\n",
      "Epoch 0 [100/156] loss_D_treino: 0.0004\n",
      "Epoch 1 [100/156] loss_D_treino: 0.0001\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for net, trainloader in zip(models, trainloaders):\n",
    "    train_gen(net=net,\n",
    "              trainloader=trainloader,\n",
    "              epochs=epochs,\n",
    "              lr=0.0001,\n",
    "              device=\"cpu\",\n",
    "              dataset=\"mnist\",\n",
    "              latent_dim=100,\n",
    "              f2a=True,\n",
    "              cliente=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(net, device, round_number, client_id = None, examples_per_class: int=5, classes: int=10, latent_dim: int=100, server: bool=False):\n",
    "    \"\"\"Gera plot de imagens de cada classe\"\"\"\n",
    "    if server:\n",
    "        import matplotlib\n",
    "        matplotlib.use(\"Agg\")\n",
    "        import matplotlib.pyplot as plt\n",
    "    else:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    net.to(device) \n",
    "    net.eval()\n",
    "    batch_size = examples_per_class * classes\n",
    "\n",
    "    latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
    "    labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_images = net(latent_vectors, labels).cpu()\n",
    "\n",
    "    # Criar uma figura com 10 linhas e 5 colunas de subplots\n",
    "    fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
    "\n",
    "    # Adiciona título no topo da figura\n",
    "    if client_id:\n",
    "        fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
    "    else:\n",
    "        fig.text(0.5, 0.98, f\"Round: {round_number-1}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "    # Exibir as imagens nos subplots\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Ajustar o layout antes de calcular as posições\n",
    "    plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
    "\n",
    "    # Reduzir espaço entre colunas\n",
    "    # plt.subplots_adjust(wspace=0.05)\n",
    "\n",
    "    # Adicionar os rótulos das classes corretamente alinhados\n",
    "    fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
    "    for row in range(classes):\n",
    "        # Obter posição do subplot em coordenadas da figura\n",
    "        bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
    "        pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
    "        center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
    "\n",
    "        # Adicionar o rótulo\n",
    "        fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
    "    \n",
    "    fig.savefig(f\"mnist_CGAN_r{r}_{128}b_{0.0001}lr_f2a.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = CGAN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_losses = []\n",
    "d_losses = []\n",
    "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optim_Ds = [\n",
    "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    for model in models\n",
    "]\n",
    "for r in range(2):\n",
    "    for i, (net, trainloader) in enumerate(zip(models, trainloaders)):\n",
    "        net.to(device)\n",
    "        optim_D = optim_Ds[i]\n",
    "        for e in range(2):\n",
    "            for batch_idx, batch in enumerate(trainloader):\n",
    "                images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                batch_size = images.size(0)\n",
    "                real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "                fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
    "\n",
    "                net.zero_grad()\n",
    "                y_real = net(images, labels)\n",
    "                d_real_loss = net.loss(y_real, real_ident)\n",
    "                z_noise = torch.randn(batch_size, 100, device=device)\n",
    "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "                x_fake = net(z_noise, x_fake_labels)\n",
    "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
    "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optim_D.step()\n",
    "                d_losses.append(d_loss.item())\n",
    "\n",
    "    gen.zero_grad()\n",
    "    z_noise = torch.randn(128, 100, device=device)\n",
    "    x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
    "    x_fake = gen(z_noise, x_fake_labels)\n",
    "    y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
    "    real_ident = torch.full((128, 1), 1., device=device)\n",
    "    y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
    "    Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
    "    y_fake_g = Dmax(x_fake, x_fake_labels)\n",
    "    g_loss = gen.loss(y_fake_g, real_ident)\n",
    "    g_loss.backward()\n",
    "    optim_G.step()\n",
    "    g_losses.append(g_loss.item())\n",
    "    figura = generate_plot(net=gen, device=device, round_number=r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "gen.zero_grad()\n",
    "z_noise = torch.randn(128, 100, device=device)\n",
    "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
    "x_fake = gen(z_noise, x_fake_labels)\n",
    "y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
    "real_ident = torch.full((128, 1), 1., device=device)\n",
    "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
    "Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
    "y_fake_g = Dmax(x_fake, x_fake_labels)\n",
    "g_loss = gen.loss(y_fake_g, real_ident)\n",
    "g_loss.backward()\n",
    "optim_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "gen.zero_grad()\n",
    "z_noise = torch.randn(128, 100, device=device)\n",
    "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
    "x_fake = gen(z_noise, x_fake_labels)\n",
    "y_fake_gs = [model(x_fake, x_fake_labels) for model in models]\n",
    "real_ident = torch.full((128, 1), 1., device=device)\n",
    "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
    "max_idx = y_fake_g_means.index(max(y_fake_g_means))\n",
    "g_loss = gen.loss(y_fake_gs[max_idx], real_ident)\n",
    "g_loss.backward()\n",
    "optim_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import FitRes, Status, Code, ndarrays_to_parameters, parameters_to_ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [[val.cpu().numpy() for _, val in net.state_dict().items()] for net in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_converted = [ndarrays_to_parameters(param) for param in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [(i, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=param, num_examples=len(train_partition), metrics={})) for i, param, train_partition in zip(range(num_partitions), params_converted, train_partitions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.server.strategy.aggregate import aggregate_inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_ndarrays = aggregate_inplace(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_aggregated_gen = ndarrays_to_parameters(aggregated_ndarrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma instância do modelo\n",
    "model = CGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
    "state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_G(net: CGAN, device: str, lr: float, epochs: int, batch_size: int, latent_dim: int):\n",
    "    net.to(device)  # move model to GPU if available\n",
    "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train G\n",
    "        net.zero_grad()\n",
    "        z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
    "        x_fake = net(z_noise, x_fake_labels)\n",
    "        y_fake_g = net(x_fake, x_fake_labels)\n",
    "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
    "        g_loss = net.loss(y_fake_g, real_ident)\n",
    "        g_loss.backward()\n",
    "        optim_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_G(net=model,\n",
    "        device=device,\n",
    "        lr=0.0001,\n",
    "        epochs=2,\n",
    "        batch_size=128,\n",
    "        latent_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flwr.common.typing.Parameters"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parameters_aggregated_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [val.cpu().numpy() for _, val in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = ndarrays_to_parameters(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flwr.common.typing.Parameters"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gerafed_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
