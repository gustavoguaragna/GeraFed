{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bewK05okXn9u"
      },
      "source": [
        "## Treinamento modelo classificador e geradora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vda6HdlXn90"
      },
      "source": [
        "### Centralizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcekFyKmXn90"
      },
      "source": [
        "#### Importacoes, carregamento dos dados, definicao da rede classificadora e treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yQq3sXpTXn92"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2HQ8Sih1Xn_c"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E7ZYEUmkXn_h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3JP3W75Xn_i"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*4*4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVt9PtdGXn_j"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxtofuOWXn_m"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
        "# Create data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainloader_reduzido = DataLoader(trainset_reduzido, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QISJ5_duXn_n"
      },
      "source": [
        "##### Salvando imagens MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON7L-wrUXn_o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpOrcDK2Xn_o"
      },
      "outputs": [],
      "source": [
        "# Function to save a random sample of images\n",
        "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    if classes is None:\n",
        "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
        "\n",
        "    if balanced:\n",
        "        # Get the number of classes\n",
        "        num_classes = len(classes)\n",
        "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
        "        indices = []\n",
        "        class_counts = {i: 0 for i in classes}\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        shuffled_indices = list(range(len(dataset)))\n",
        "        random.shuffle(shuffled_indices)\n",
        "\n",
        "        for idx in shuffled_indices:\n",
        "            img = dataset[idx][0]\n",
        "            label = int(dataset[idx][1])\n",
        "            if label in classes and class_counts[label] < samples_per_class:\n",
        "                indices.append(idx)\n",
        "                class_counts[label] += 1\n",
        "            if len(indices) >= num_samples:\n",
        "                break\n",
        "    else:\n",
        "        indices = []\n",
        "        while len(indices) < num_samples:\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            if int(dataset[idx][1]) in classes:\n",
        "                indices.append(idx)\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = dataset[idx]\n",
        "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
        "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoanL3q-Xn_p"
      },
      "outputs": [],
      "source": [
        "save_random_samples(trainset, num_samples=2048, balanced=True, classes=[i], folder=filter\"Imagens Testes/mnist_samples_{i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTkej1xYXn_p"
      },
      "source": [
        "#### Definicao da GAN e funcoes de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j9zuYSTUXn_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yihGlxjjXn_p"
      },
      "outputs": [],
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(CGAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        if input.dim() == 2:\n",
        "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "            x = self.generator(z)\n",
        "            x = x.view(x.size(0), *self.img_shape) #Em\n",
        "            return x\n",
        "        elif input.dim() == 4:\n",
        "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)\n",
        "\n",
        "\n",
        "def train_gen(net, trainloader, epochs, lr, device, dataset=\"mnist\", latent_dim=100, f2a: bool = False, cliente: bool = False, D=None):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    if dataset == \"mnist\":\n",
        "      imagem = \"image\"\n",
        "    elif dataset == \"cifar10\":\n",
        "      imagem = \"img\"\n",
        "\n",
        "    net.to(device)  # move model to GPU if available\n",
        "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, batch in enumerate(trainloader):\n",
        "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
        "            batch_size = images.size(0)\n",
        "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "            if not f2a:\n",
        "                # Train G\n",
        "                net.zero_grad()\n",
        "                z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                x_fake = net(z_noise, x_fake_labels)\n",
        "                y_fake_g = net(x_fake, x_fake_labels)\n",
        "                g_loss = net.loss(y_fake_g, real_ident)\n",
        "                g_loss.backward()\n",
        "                optim_G.step()\n",
        "\n",
        "                # Train D\n",
        "                net.zero_grad()\n",
        "                y_real = net(images, labels)\n",
        "                d_real_loss = net.loss(y_real, real_ident)\n",
        "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                d_loss.backward()\n",
        "                optim_D.step()\n",
        "\n",
        "                g_losses.append(g_loss.item())\n",
        "                d_losses.append(d_loss.item())\n",
        "\n",
        "                if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                    print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                d_loss.mean().item(),\n",
        "                                g_loss.mean().item()))\n",
        "\n",
        "            else:\n",
        "                if cliente:\n",
        "                    # Train D\n",
        "                    net.zero_grad()\n",
        "                    y_real = net(images, labels)\n",
        "                    d_real_loss = net.loss(y_real, real_ident)\n",
        "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                    x_fake = net(z_noise, x_fake_labels)\n",
        "                    y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "                    d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "                    d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                    d_loss.backward()\n",
        "                    optim_D.step()\n",
        "\n",
        "                    d_losses.append(d_loss.item())\n",
        "\n",
        "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                        print('Epoch {} [{}/{}] loss_D_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                d_loss.mean().item()))\n",
        "\n",
        "                else:\n",
        "                    # Train G\n",
        "                    net.zero_grad()\n",
        "                    z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "                    x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                    x_fake = net(z_noise, x_fake_labels)\n",
        "                    y_fake_g = net(x_fake, x_fake_labels)\n",
        "                    g_loss = net.loss(y_fake_g, real_ident)\n",
        "                    g_loss.backward()\n",
        "                    optim_G.step()\n",
        "\n",
        "                    g_losses.append(g_loss.item())\n",
        "\n",
        "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                        print('Epoch {} [{}/{}] loss_G_treino: {:.4f}'.format(\n",
        "                                epoch, batch_idx, len(trainloader),\n",
        "                                g_loss.mean().item()))\n",
        "\n",
        "\n",
        "\n",
        "def test(net, testloader, device, dataset=\"mnist\", latent_dim=100):\n",
        "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
        "    if dataset == \"mnist\":\n",
        "      imagem = \"image\"\n",
        "    elif dataset == \"cifar10\":\n",
        "      imagem = \"img\"\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(testloader):\n",
        "            images, labels = batch[imagem].to(device), batch[\"label\"].to(device)\n",
        "            batch_size = images.size(0)\n",
        "            real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "            #Gen loss\n",
        "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "            x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "            x_fake = net(z_noise, x_fake_labels)\n",
        "            y_fake_g = net(x_fake, x_fake_labels)\n",
        "            g_loss = net.loss(y_fake_g, real_ident)\n",
        "\n",
        "            #Disc loss\n",
        "            y_real = net(images, labels)\n",
        "            d_real_loss = net.loss(y_real, real_ident)\n",
        "            y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "            d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "            g_losses.append(g_loss.item())\n",
        "            d_losses.append(d_loss.item())\n",
        "\n",
        "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                print('[{}/{}] loss_D_teste: {:.4f} loss_G_teste: {:.4f}'.format(\n",
        "                            batch_idx, len(testloader),\n",
        "                            d_loss.mean().item(),\n",
        "                            g_loss.mean().item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUPN43l4Xn_r"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXzJElIeXn_w"
      },
      "outputs": [],
      "source": [
        "# Configurações\n",
        "LATENT_DIM = 128\n",
        "LEARNING_RATE = 0.0002\n",
        "BETA1 = 0.5\n",
        "BETA2 = 0.9\n",
        "GP_SCALE = 10\n",
        "NUM_CHANNELS = 1\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 50\n",
        "# Camada de Convolução para o Discriminador\n",
        "def conv_block(in_channels, out_channels, kernel_size=5, stride=2, padding=2, use_bn=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
        "    if use_bn:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Discriminador\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            conv_block(NUM_CHANNELS + NUM_CLASSES, 64, use_bn=False),\n",
        "            conv_block(64, 128, use_bn=True),\n",
        "            conv_block(128, 256, use_bn=True),\n",
        "            conv_block(256, 512, use_bn=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 2 * 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Camada de upsample para o Gerador\n",
        "def upsample_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_bn=True):\n",
        "    layers = [\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels) if use_bn else nn.Identity(),\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "    ]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Gerador\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + NUM_CLASSES, 4 * 4 * 256),\n",
        "            nn.BatchNorm1d(4 * 4 * 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Unflatten(1, (256, 4, 4)),\n",
        "            upsample_block(256, 128),\n",
        "            upsample_block(128, 64),\n",
        "            upsample_block(64, 32),\n",
        "            nn.Conv2d(32, NUM_CHANNELS, kernel_size=5, stride=1, padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fPY0beuXoBV"
      },
      "source": [
        "#### Geração de Dados Sintéticos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-l-6raaXoBW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GeneratedDataset(Dataset):\n",
        "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.model = type(self.generator).__name__\n",
        "        self.images, self.labels = self.generate_data()\n",
        "        self.classes = [i for i in range(self.num_classes)]\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        self.generator.eval()\n",
        "        labels = torch.tensor([i for i in range(self.num_classes) for _ in range(self.num_samples // self.num_classes)], device=self.device)\n",
        "        if self.model == 'Generator':\n",
        "            labels_one_hot = F.one_hot(labels, self.num_classes).float().to(self.device) #\n",
        "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "        with torch.no_grad():\n",
        "            if self.model == 'Generator':\n",
        "                gen_imgs = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
        "            elif self.model == 'CGAN':\n",
        "                gen_imgs = self.generator(z, labels)\n",
        "\n",
        "        return gen_imgs.cpu(), labels.cpu()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abkIz6SPXoBW"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cEGGzm2XoBW"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 10000\n",
        "latent_dim = 128\n",
        "\n",
        "G = Generator(latent_dim=128).to(\"cpu\")\n",
        "G.load_state_dict(torch.load(\"wgan_43e_128b_0.0002lr.pth\", map_location=torch.device('cpu'))[\"generator\"])\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=G, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGU4X9cFXoBX"
      },
      "source": [
        "##### CGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR8Apn1XXoBX"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 60000\n",
        "latent_dim = 100\n",
        "\n",
        "gan = CGAN()\n",
        "gan.load_state_dict(torch.load(\"Imagens Testes/FULL_FEDAVG/epochs20/model_round_10_mnist.pt\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=gan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9407SFvSXoBX"
      },
      "source": [
        "Salvando imagens CGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zEozbb5XoBX"
      },
      "outputs": [],
      "source": [
        "save_random_samples(generated_dataset, num_samples=2048, balanced=True, folder='Imagens Testes/cgan_samples_niid_0.7acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjkOWq3mXoBY"
      },
      "outputs": [],
      "source": [
        "net = Net()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "net.train()\n",
        "for epoch in range(5):\n",
        "    for data in generated_dataloader:\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSQ_NYB_XoBY"
      },
      "outputs": [],
      "source": [
        "correct, loss = 0, 0.0\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in testloader:\n",
        "        images = batch[0]\n",
        "        labels = batch[1]\n",
        "        outputs = net(images)\n",
        "        loss += criterion(outputs, labels).item()\n",
        "        correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "accuracy = correct / len(testloader.dataset)\n",
        "loss = loss / len(testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er2sYOlBXoBY"
      },
      "outputs": [],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFt_vSA_XoBY"
      },
      "source": [
        "#### Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jW6ykXwXoBZ"
      },
      "outputs": [],
      "source": [
        "net = CGAN()\n",
        "train(net=net,\n",
        "      trainloader=trainloader,\n",
        "      epochs=50,\n",
        "      learning_rate=0.0001,\n",
        "      device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOjPqx2oXoBZ"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), 'CGAN_50epochs.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCs7kMrjXoBZ"
      },
      "source": [
        "##### WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xfvrkD1XoBa"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihvzrqgrXoBa"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "445r_BCSXoBa"
      },
      "outputs": [],
      "source": [
        "# Inicializar modelos\n",
        "D = Discriminator().to(device)\n",
        "G = Generator(latent_dim=LATENT_DIM).to(device)\n",
        "# Otimizadores\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
        "\n",
        " # Função de perda Wasserstein\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return fake_output.mean() - real_output.mean()\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return -fake_output.mean()\n",
        "\n",
        "# Função para calcular Gradient Penalty\n",
        "def gradient_penalty(D, real_samples, fake_samples):\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
        "    interpolated = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "    d_interpolated = D(interpolated)\n",
        "    gradients = torch.autograd.grad(outputs=d_interpolated, inputs=interpolated,\n",
        "                                    grad_outputs=torch.ones_like(d_interpolated),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0HlgddkXoBb"
      },
      "outputs": [],
      "source": [
        "gan = CGAN(latent_dim=128).to(device)\n",
        "optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUGhPPWsXoBb"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7M7HXKbXoBc"
      },
      "outputs": [],
      "source": [
        "# Treinamento\n",
        "historico_metricas = []\n",
        "wgan = True\n",
        "epoch_bar = tqdm(range(EPOCHS), desc=\"Treinamento\", leave=True, position=0)\n",
        "for epoch in epoch_bar:\n",
        "\n",
        "    print(f\"\\n🔹 Epoch {epoch+1}/{EPOCHS}\")\n",
        "    G_loss = 0\n",
        "    D_loss = 0\n",
        "    batches = 0\n",
        "\n",
        "    batch_bar = tqdm(trainloader_reduzido, desc=\"Batches\", leave=False, position=1)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for real_images, labels in batch_bar:\n",
        "        real_images = real_images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch = real_images.size(0)\n",
        "        fake_labels = torch.randint(0, NUM_CLASSES, (batch,), device=device)\n",
        "        z = torch.randn(batch, LATENT_DIM).to(device)\n",
        "        optimizer_D.zero_grad()\n",
        "        if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "            fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
        "\n",
        "            # Adicionar labels ao real_images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            real_images = torch.cat([real_images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z = torch.cat([z, fake_labels], dim=1)\n",
        "            fake_images = G(z).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            D(real_images)\n",
        "            loss_D = discriminator_loss(D(real_images), D(fake_images)) + GP_SCALE * gradient_penalty(D, real_images, fake_images)\n",
        "\n",
        "        else:\n",
        "            real_ident = torch.full((batch, 1), 1., device=device)\n",
        "            fake_ident = torch.full((batch, 1), 0., device=device)\n",
        "            x_fake = gan(z, fake_labels)\n",
        "\n",
        "            y_real = gan(real_images, labels)\n",
        "            d_real_loss = gan.loss(y_real, real_ident)\n",
        "            y_fake_d = gan(x_fake.detach(), fake_labels)\n",
        "            d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
        "            loss_D = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # z = torch.randn(batch, LATENT_DIM).to(device)\n",
        "        # z = torch.cat([z, fake_labels], dim=1)\n",
        "        if wgan:\n",
        "            fake_images = G(z)\n",
        "            loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
        "        else:\n",
        "            y_fake_g = gan(x_fake, fake_labels)\n",
        "            loss_G = gan.loss(y_fake_g, real_ident)\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        G_loss += loss_G.item()\n",
        "        D_loss += loss_D.item()\n",
        "        batches += BATCH_SIZE\n",
        "\n",
        "    avg_epoch_G_loss = G_loss/batches\n",
        "    avg_epoch_D_loss = D_loss/batches\n",
        "    # Create the dataset and dataloader\n",
        "    if wgan:\n",
        "        generated_dataset = GeneratedDataset(generator=G, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
        "    else:\n",
        "        generated_dataset = GeneratedDataset(generator=gan, num_samples=10000, latent_dim=LATENT_DIM, num_classes=10, device=device)\n",
        "    generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    net = Net()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "    net.train()\n",
        "    for _ in range(5):\n",
        "        for data in generated_dataloader:\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    net.eval()\n",
        "    correct, loss = 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images = batch[0]\n",
        "            labels = batch[1]\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    epoch_bar.set_postfix({\n",
        "        \"D_loss\": f\"{avg_epoch_D_loss:.4f}\",\n",
        "        \"G_loss\": f\"{avg_epoch_G_loss:.4f}\",\n",
        "        \"Acc\": f\"{accuracy:.4f}\"\n",
        "    })\n",
        "\n",
        "    with open(\"Treino_GAN.txt\", \"a\") as f:\n",
        "            f.write(f\"Epoca: {epoch+1}, D_loss: {avg_epoch_D_loss:.4f}, G_loss: {avg_epoch_G_loss:.4f}, Acc: {accuracy:.4f}, Tempo: {total_time:.4f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Atualiza o learning_rate\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "    print(f\"Após Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
        "    if wgan:\n",
        "         # Salvar modelo a cada época\n",
        "        torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, f\"wgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
        "    else:\n",
        "        torch.save(gan.state_dict(), f\"cgan_{epoch+1}e_{BATCH_SIZE}b_{LEARNING_RATE}lr.pth\")\n",
        "\n",
        "print(\"✅ Treinamento Concluído!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8n9eIqhXoCF"
      },
      "outputs": [],
      "source": [
        "torch.save({\"generator\": G.state_dict(), \"discriminator\": D.state_dict()}, \"wgan_29e_64b_0.002lr.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyhBKXfHXoCI"
      },
      "source": [
        "##### Ajuste de hiperparametro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMuI-AbbXoCI"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8lgBKXqXoCI"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import optuna\n",
        "from optuna.importance import get_param_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89xbKVJMXoCJ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=\"cgan\"\n",
        "EPOCHS = 5\n",
        "# Função Objetiva (a ser otimizada pelo Optuna)\n",
        "def objective(trial):\n",
        "    # Escolher os hiperparâmetros dentro de um intervalo\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 16, 1024)\n",
        "    latent_dim = trial.suggest_int(\"latent_dim\", 10, 1000)\n",
        "    lr = trial.suggest_float(\"learning_rate\", 0.0001, 0.05, log=True)\n",
        "    beta1 = trial.suggest_float(\"beta1\", 0.0, 0.9)\n",
        "    beta2 = trial.suggest_float(\"beta2\", 0.8, 0.999)\n",
        "    global model\n",
        "    model = model.lower()\n",
        "    if model==\"wgan\":\n",
        "        gp_scale = trial.suggest_int(\"gp_scale\", 0, 100)\n",
        "\n",
        "    # Criar DataLoader com batch_size otimizado\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Criar novos modelos e otimizadores\n",
        "    if model == \"wgan\":\n",
        "        D = Discriminator().to(device)\n",
        "        G = Generator(latent_dim=latent_dim).to(device)\n",
        "        optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "        optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "        G.train()\n",
        "        D.train()\n",
        "    elif model == \"cgan\":\n",
        "        gan = CGAN(latent_dim=latent_dim).to(device)\n",
        "        optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "        optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "    scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.9)\n",
        "    scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.9)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    # Treinar por algumas épocas\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_batches = 0\n",
        "        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for real_images, labels in progress_bar:\n",
        "            real_images = real_images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            fake_labels = torch.randint(0, NUM_CLASSES, (real_images.size(0),), device=device)\n",
        "            z = torch.randn(real_images.size(0), latent_dim).to(device)\n",
        "            optimizer_D.zero_grad()\n",
        "            if model==\"wgan\":\n",
        "                labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "                fake_labels = torch.nn.functional.one_hot(fake_labels, NUM_CLASSES).float()\n",
        "\n",
        "                # Adicionar labels ao real_images para treinamento do Discriminador\n",
        "                image_labels = labels.view(labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "                image_fake_labels = fake_labels.view(fake_labels.size(0), NUM_CLASSES, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "                real_images = torch.cat([real_images, image_labels], dim=1)\n",
        "\n",
        "                # Treinar Discriminador\n",
        "                z = torch.cat([z, labels], dim=1)\n",
        "                fake_images = G(z).detach()\n",
        "                fake_images = torch.cat([fake_images, image_labels], dim=1)\n",
        "\n",
        "                loss_D = discriminator_loss(D(real_images), D(fake_images)) + gp_scale * gradient_penalty(D, real_images, fake_images)\n",
        "\n",
        "\n",
        "            else:\n",
        "                real_ident = torch.full((real_images.size(0), 1), 1., device=device)\n",
        "                fake_ident = torch.full((real_images.size(0), 1), 0., device=device)\n",
        "                x_fake = gan(z, fake_labels)\n",
        "\n",
        "                y_real = gan(real_images, labels)\n",
        "                d_real_loss = gan.loss(y_real, real_ident)\n",
        "                y_fake_d = gan(x_fake.detach(), fake_labels)\n",
        "                d_fake_loss = gan.loss(y_fake_d, fake_ident)\n",
        "                loss_D = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "\n",
        "            # Treinar Gerador\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            if model==\"wgan\":\n",
        "                fake_images = G(z)\n",
        "                loss_G = generator_loss(D(torch.cat([fake_images, image_fake_labels], dim=1)))\n",
        "            else:\n",
        "                y_fake_g = gan(x_fake, fake_labels)\n",
        "                loss_G = gan.loss(y_fake_g, real_ident)\n",
        "\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            epoch_loss += loss_G.item()\n",
        "            total_loss += loss_G.item()\n",
        "            total_batches += 1\n",
        "            epoch_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix(d_loss=loss_D.item(), g_loss=loss_G.item())\n",
        "\n",
        "        # Calcular a loss média dessa época\n",
        "        epoch_avg_loss = epoch_loss / epoch_batches\n",
        "        # Reporta a loss média da época para pruning\n",
        "        trial.report(epoch_avg_loss, epoch)\n",
        "        if trial.should_prune() and epoch >=3:\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        scheduler_G.step()\n",
        "        scheduler_D.step()\n",
        "        print(f\"Após Epoch {epoch+1}, LR_G: {optimizer_G.param_groups[0]['lr']:.6f}, LR_D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "\n",
        "    return avg_loss  # Optuna tentará minimizar essa métrica\n",
        "\n",
        "# Criar estudo do Optuna e otimizar hiperparâmetros\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Exibir os melhores hiperparâmetros encontrados\n",
        "print(\"\\n🔹 Melhores Hiperparâmetros Encontrados:\")\n",
        "print(study.best_params)\n",
        "\n",
        "importance = get_param_importances(study)\n",
        "print(\"Hyperparameter Importances:\")\n",
        "for param, imp in importance.items():\n",
        "    print(f\"{param}: {imp:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt2EiBMCXoCL"
      },
      "source": [
        "#### Teste para qualidade visual das imagens geradas pelo modelo generativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hdas7twXoCM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByyDvVxcXoCM"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "latent_dim = 128\n",
        "# random.seed(seed)\n",
        "# np.random.seed(seed)\n",
        "#torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "path = \"\"\n",
        "device = \"cpu\"\n",
        "net = CGAN(dataset=\"mnist\", latent_dim=latent_dim).to(device)\n",
        "net.load_state_dict(torch.load(f'{path}/model_round_5_mnist.pt'))\n",
        "# G = Generator(latent_dim=128)\n",
        "# G.load_state_dict(torch.load(\"wgan_5e_512b_0.002lr_0.5B1_0.9B2_10gp_128_ld.pth\")[\"generator\"])\n",
        "#net = CGAN()\n",
        "#net.load_state_dict(torch.load('CGAN_50epochs.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "#net.eval()\n",
        "G.eval()\n",
        "# Assuming netG is your generator model, classes is the total number of classes, and latent_dim is the latent vector size\n",
        "examples_per_class = 5\n",
        "classes = 10\n",
        "batch_size = examples_per_class * classes  # Generate enough images to have `examples_per_class` for each class\n",
        "\n",
        "# Generate latent vectors and corresponding labels\n",
        "latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
        "labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
        "labels = torch.nn.functional.one_hot(labels, NUM_CLASSES).float().to(device)\n",
        "\n",
        "# Generate images\n",
        "with torch.no_grad():\n",
        "    #generated_images = net(latent_vectors, labels)\n",
        "    generated_images = G(torch.cat([latent_vectors, labels], dim=1))\n",
        "\n",
        "# Criar uma figura com 10 linhas e 5 colunas de subplots\n",
        "fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
        "\n",
        "#fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "fig.text(0.5, 0.98, f\"Round: {5}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "# Exibir as imagens nos subplots\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Ajustar o layout antes de calcular as posições\n",
        "plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "# Reduzir espaço entre colunas\n",
        "# plt.subplots_adjust(wspace=0.05)\n",
        "\n",
        "# Adicionar os rótulos das classes corretamente alinhados\n",
        "fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
        "for row in range(classes):\n",
        "    # Obter posição do subplot em coordenadas da figura\n",
        "    bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "    pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "    center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "    # Adicionar o rótulo\n",
        "    fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
        "    plt.savefig(f\"{path}teste.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtY4KCiWXoCO"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import Image as IPImage, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6PlY_TYXoCP"
      },
      "outputs": [],
      "source": [
        "def create_gif(image_files, output_path, duration=200):\n",
        "    \"\"\"\n",
        "    Cria um GIF animado a partir de uma sequência de imagens.\n",
        "\n",
        "    Args:\n",
        "        image_files (list): Lista de caminhos das imagens.\n",
        "        output_path (str): Caminho para salvar o GIF.\n",
        "        duration (int): Tempo de exibição de cada frame (em ms).\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    frames = [Image.open(img) for img in image_files]  # Carregar imagens\n",
        "    frames[0].save(output_path, format=\"GIF\", save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
        "    display(IPImage(filename=output_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txiXYjHEXoCP"
      },
      "outputs": [],
      "source": [
        "# Exemplo de uso\n",
        "image_files = [\"../imagens geradas/mnist_CGAN_r0_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r1_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r2_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r3_100e_64_100z_10c_0.0001lr_niid_01dir.png\",\n",
        "               \"../imagens geradas/mnist_CGAN_r4_100e_64_100z_10c_0.0001lr_niid_01dir.png\"]\n",
        "create_gif(image_files, \"global.gif\", duration=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra5PcELgXoCQ"
      },
      "outputs": [],
      "source": [
        "def create_federated_collage(\n",
        "    agg_image_paths,       # Lista de caminhos para as imagens grandes (1 por round)\n",
        "    clients_image_paths,   # Lista de listas: para cada round, lista de caminhos de imagens de clientes\n",
        "    big_scale=2,           # Escala da imagem grande em relação à imagem pequena\n",
        "    small_size=(500, 900),   # Tamanho desejado para cada imagem pequena (largura, altura)\n",
        "    h_gap=0,               # Espaço horizontal entre bloco de imagens\n",
        "    background_color=(255, 255, 255),\n",
        "    save_path=\"collage.png\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Cria um mosaico onde cada round tem:\n",
        "      - 1 imagem \"agregada\" (maior) à esquerda\n",
        "      - N imagens de cliente empilhadas verticalmente à direita\n",
        "\n",
        "    Parâmetros:\n",
        "      agg_image_paths      : lista de strings (caminhos) para as imagens agregadas (1 por round)\n",
        "      clients_image_paths  : lista de listas de strings. Cada sublista é a lista de caminhos das imagens de cada cliente daquele round\n",
        "      big_scale            : fator de escala da imagem grande em relação às pequenas\n",
        "      small_size           : (largura, altura) desejado para cada imagem pequena\n",
        "      background_color     : cor de fundo do mosaico (RGB)\n",
        "      save_path            : caminho do arquivo final a ser salvo\n",
        "\n",
        "    Retorna:\n",
        "      Um objeto PIL.Image com o mosaico criado.\n",
        "    \"\"\"\n",
        "    # Verifica se temos a mesma quantidade de rounds em agg_image_paths e clients_image_paths\n",
        "    assert len(agg_image_paths) == len(clients_image_paths), \\\n",
        "        \"Número de imagens agregadas deve bater com número de listas de clientes.\"\n",
        "\n",
        "    # Carrega todas as imagens agregadas (rounds)\n",
        "    agg_images = []\n",
        "    for path in agg_image_paths:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        agg_images.append(img)\n",
        "\n",
        "    # Carrega todas as imagens de clientes\n",
        "    # clients_image_paths é lista de listas, cada sublista para um round\n",
        "    client_images = []\n",
        "    for round_paths in clients_image_paths:\n",
        "        imgs = [Image.open(p).convert(\"RGB\") for p in round_paths]\n",
        "        client_images.append(imgs)\n",
        "\n",
        "    # Dimensiona as imagens pequenas para small_size\n",
        "    # e as grandes para (big_scale * small_size)\n",
        "    small_w, small_h = small_size\n",
        "    big_w, big_h = big_scale * small_w, big_scale * small_h\n",
        "\n",
        "    # Faz o resize de todas as imagens\n",
        "    for i, img in enumerate(agg_images):\n",
        "        agg_images[i] = img.resize((big_w, big_h), Image.Resampling.LANCZOS)\n",
        "\n",
        "    for i, imgs in enumerate(client_images):\n",
        "        resized_list = []\n",
        "        for im in imgs:\n",
        "            resized_list.append(im.resize((small_w, small_h), Image.Resampling.LANCZOS))\n",
        "        client_images[i] = resized_list\n",
        "\n",
        "    # Calcula quantos rounds e quantos clientes\n",
        "    num_rounds = len(agg_images)\n",
        "\n",
        "    # Para cada round, vamos colocar:\n",
        "    # - Imagem grande (largura big_w, altura big_h)\n",
        "    # - N clientes empilhados (cada um small_h de altura, total N * small_h)\n",
        "    # A largura de cada \"bloco\" de round = (big_w + small_w)\n",
        "    # A altura do bloco = max(big_h, N * small_h) (para acomodar todas as imagens)\n",
        "\n",
        "    # Descobre o número máximo de clientes em qualquer round (para dimensionar corretamente)\n",
        "    max_clients = max(len(imgs) for imgs in client_images)\n",
        "\n",
        "    # Altura total do bloco para cada round\n",
        "    block_h = max(big_h + small_h, max_clients * small_h)\n",
        "    block_w = big_w + small_w  # Largura do bloco do round\n",
        "\n",
        "    # Largura total = num_rounds * block_w\n",
        "    # Altura total = block_h (vamos colocar rounds lado a lado)\n",
        "    total_w = num_rounds * block_w  + h_gap*2*num_rounds-1\n",
        "    total_h = block_h\n",
        "\n",
        "    # Cria imagem de fundo\n",
        "    collage = Image.new(\"RGB\", (total_w, total_h), color=background_color)\n",
        "\n",
        "    # Posiciona cada round\n",
        "    for r in range(num_rounds):\n",
        "        # Posição x para este round\n",
        "        x_offset = r * block_w + 2*r*h_gap\n",
        "\n",
        "        # Coloca a imagem grande (agg)\n",
        "        collage.paste(agg_images[r], (x_offset, small_h))\n",
        "\n",
        "        # Agora empilha as imagens de cliente ao lado (à direita da imagem grande)\n",
        "        y_offset = 0\n",
        "        for c_img in client_images[r]:\n",
        "            collage.paste(c_img, (x_offset + big_w + h_gap, y_offset))\n",
        "            y_offset += small_h\n",
        "\n",
        "    # Salva o resultado\n",
        "    collage.save(save_path)\n",
        "    print(f\"Mosaico criado e salvo em: {save_path}\")\n",
        "\n",
        "    return collage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoP558u6XoCR"
      },
      "outputs": [],
      "source": [
        "path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p8mhqvHXoCR"
      },
      "outputs": [],
      "source": [
        "agg_image_paths = [f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir.png\" for i in range(10, 20)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wWETQNqXoCS"
      },
      "outputs": [],
      "source": [
        "client_image_paths = [[f\"{path}mnist_CGAN_r{i}_10e_64b_100z_4c_0.0001lr_niid_01dir_cliente{j}.png\" for j in range(4)] for i in range(11, 21)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJztr1mtXoCT"
      },
      "outputs": [],
      "source": [
        "create_federated_collage(\n",
        "    agg_image_paths=agg_image_paths,\n",
        "    clients_image_paths=client_image_paths,\n",
        "    big_scale=2,\n",
        "    small_size=(500, 900),\n",
        "    h_gap=80,\n",
        "    background_color=(255, 255, 255),\n",
        "    save_path=f\"{path}CGAN_evol.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu8-avfIXoCT"
      },
      "source": [
        "### FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI2FOV2rXoCU"
      },
      "source": [
        "### Importacoes, classes e configuracoes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7zeFBY8XoCV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjVnx-3FXoCX"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "num_cpus = os.cpu_count()\n",
        "num_workers = min(8, num_cpus,0)\n",
        "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "dims = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz1K-r-OXoCY"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhtBYnxkXoCY"
      },
      "outputs": [],
      "source": [
        "def _inception_v3(*args, **kwargs):\n",
        "    \"\"\"Wraps `torchvision.models.inception_v3`\"\"\"\n",
        "    try:\n",
        "        version = tuple(map(int, torchvision.__version__.split(\".\")[:2]))\n",
        "    except ValueError:\n",
        "        # Just a caution against weird version strings\n",
        "        version = (0,)\n",
        "\n",
        "    # Skips default weight inititialization if supported by torchvision\n",
        "    # version. See https://github.com/mseitzer/pytorch-fid/issues/28.\n",
        "    if version >= (0, 6):\n",
        "        kwargs[\"init_weights\"] = False\n",
        "\n",
        "    # Backwards compatibility: `weights` argument was handled by `pretrained`\n",
        "    # argument prior to version 0.13.\n",
        "    if version < (0, 13) and \"weights\" in kwargs:\n",
        "        if kwargs[\"weights\"] == \"DEFAULT\":\n",
        "            kwargs[\"pretrained\"] = True\n",
        "        elif kwargs[\"weights\"] is None:\n",
        "            kwargs[\"pretrained\"] = False\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"weights=={} not supported in torchvision {}\".format(\n",
        "                    kwargs[\"weights\"], torchvision.__version__\n",
        "                )\n",
        "            )\n",
        "        del kwargs[\"weights\"]\n",
        "\n",
        "    return torchvision.models.inception_v3(*args, **kwargs)\n",
        "\n",
        "\n",
        "def fid_inception_v3():\n",
        "    \"\"\"Build pretrained Inception model for FID computation\n",
        "\n",
        "    The Inception model for FID computation uses a different set of weights\n",
        "    and has a slightly different structure than torchvision's Inception.\n",
        "\n",
        "    This method first constructs torchvision's Inception and then patches the\n",
        "    necessary parts that are different in the FID Inception model.\n",
        "    \"\"\"\n",
        "    inception = _inception_v3(num_classes=1008, aux_logits=False, weights=None)\n",
        "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
        "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
        "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
        "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
        "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
        "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
        "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
        "\n",
        "    state_dict = load_state_dict_from_url(\"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\", progress=True)\n",
        "    inception.load_state_dict(state_dict)\n",
        "    return inception\n",
        "\n",
        "\n",
        "class FIDInceptionA(torchvision.models.inception.InceptionA):\n",
        "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, pool_features):\n",
        "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionC(torchvision.models.inception.InceptionC):\n",
        "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, channels_7x7):\n",
        "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_1(torchvision.models.inception.InceptionE):\n",
        "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(\n",
        "            x, kernel_size=3, stride=1, padding=1, count_include_pad=False\n",
        "        )\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n",
        "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: The FID Inception model uses max pooling instead of average\n",
        "        # pooling. This is likely an error in this specific Inception\n",
        "        # implementation, as other Inception models use average pooling here\n",
        "        # (which matches the description in the paper).\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgZPF8WdXoCZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC2LlVt4XoCa"
      },
      "outputs": [],
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,  # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3,  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_blocks=(DEFAULT_BLOCK_INDEX,),\n",
        "        resize_input=True,\n",
        "        normalize_input=True,\n",
        "        requires_grad=False,\n",
        "        use_fid_inception=True,\n",
        "    ):\n",
        "        \"\"\"Build pretrained InceptionV3\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_blocks : list of int\n",
        "            Indices of blocks to return features of. Possible values are:\n",
        "                - 0: corresponds to output of first max pooling\n",
        "                - 1: corresponds to output of second max pooling\n",
        "                - 2: corresponds to output which is fed to aux classifier\n",
        "                - 3: corresponds to output of final average pooling\n",
        "        resize_input : bool\n",
        "            If true, bilinearly resizes input to width and height 299 before\n",
        "            feeding input to model. As the network without fully connected\n",
        "            layers is fully convolutional, it should be able to handle inputs\n",
        "            of arbitrary size, so resizing might not be strictly needed\n",
        "        normalize_input : bool\n",
        "            If true, scales the input from range (0, 1) to the range the\n",
        "            pretrained Inception network expects, namely (-1, 1)\n",
        "        requires_grad : bool\n",
        "            If true, parameters of the model require gradients. Possibly useful\n",
        "            for finetuning the network\n",
        "        use_fid_inception : bool\n",
        "            If true, uses the pretrained Inception model used in Tensorflow's\n",
        "            FID implementation. If false, uses the pretrained Inception model\n",
        "            available in torchvision. The FID Inception model has different\n",
        "            weights and a slightly different structure from torchvision's\n",
        "            Inception model. If you want to compute FID scores, you are\n",
        "            strongly advised to set this parameter to true to get comparable\n",
        "            results.\n",
        "        \"\"\"\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \"Last possible output block index is 3\"\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        if use_fid_inception:\n",
        "            inception = fid_inception_v3()\n",
        "        else:\n",
        "            inception = _inception_v3(weights=\"DEFAULT\")\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tOV_I6AXoCc"
      },
      "source": [
        "#### Calculo da distribuicao gerada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snRXJWMYXoCc"
      },
      "outputs": [],
      "source": [
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36YTdjuXXoCc"
      },
      "outputs": [],
      "source": [
        "model = InceptionV3([block_idx]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIXrWCu6XoCd"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDvQF-u3XoCd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2Ia0an8XoCd"
      },
      "outputs": [],
      "source": [
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYgFFYEcXoCd"
      },
      "outputs": [],
      "source": [
        "class ImagePathDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, transforms=None):\n",
        "        self.files = files\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path = self.files[i]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3bQrqaXoCd"
      },
      "source": [
        "Por imagens geradas prontas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOUTdiayXoCe"
      },
      "outputs": [],
      "source": [
        "path = \"../imagens geradas/cgan_samples\"\n",
        "path = pathlib.Path(path)\n",
        "files = sorted(file for file in path.glob(\"*.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCrv8my3XoCe"
      },
      "outputs": [],
      "source": [
        "pred_arr = np.empty((len(files), dims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0LGyDp9XoCe"
      },
      "outputs": [],
      "source": [
        "dataset = ImagePathDataset(files, transforms=torchvision.transforms.ToTensor())\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtFXwOHQXoCe"
      },
      "source": [
        "Por modelo pre-treinado gerando imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiwYtDuvXoCe"
      },
      "outputs": [],
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(CGAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        #self._initialize_weights()\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # Itera sobre todos os módulos da rede geradora\n",
        "        for m in self.generator:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        if input.dim() == 2:\n",
        "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "            x = self.generator(z)\n",
        "            x = x.view(x.size(0), *self.img_shape) #Em\n",
        "            return x\n",
        "        elif input.dim() == 4:\n",
        "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UnF0P-uXoCf"
      },
      "outputs": [],
      "source": [
        "cgan = CGAN()\n",
        "cgan.load_state_dict(torch.load(\"CGAN_50epochs.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU6PYq0JXoCf"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "from datasets import Dataset, Features, ClassLabel\n",
        "from datasets import Image as IMG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YmuBLHNXoCf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GeneratedDataset(Dataset):\n",
        "    def __init__(self, generator, num_samples, latent_dim, num_classes, device):\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.model = type(self.generator).__name__\n",
        "        self.images = self.generate_data()\n",
        "        self.classes = [i for i in range(self.num_classes)]\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        gen_imgs = {}\n",
        "        self.generator.eval()\n",
        "        labels = {c: torch.tensor([c for i in range(self.num_samples)], device=self.device) for c in range(self.num_classes)}\n",
        "        for c, label in labels.items():\n",
        "          if self.model == 'Generator':\n",
        "              labels_one_hot = F.one_hot(label, self.num_classes).float().to(self.device) #\n",
        "          z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "          with torch.no_grad():\n",
        "              if self.model == 'Generator':\n",
        "                  gen_imgs_class = self.generator(torch.cat([z, labels_one_hot], dim=1))\n",
        "              elif self.model == 'CGAN':\n",
        "                  gen_imgs_class = self.generator(z, label)\n",
        "          gen_imgs[c] = gen_imgs_class\n",
        "\n",
        "        return gen_imgs\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples * self.num_classes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Mapear o índice global para (classe, índice interno)\n",
        "        class_idx = idx // self.num_samples\n",
        "        sample_idx = idx % self.num_samples\n",
        "        # Retorna apenas a imagem (sem o rótulo)\n",
        "        return self.images[class_idx][sample_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIh5F0qqXoCf"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 2048\n",
        "latent_dim = 100\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=cgan, num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "gen_dataset = generated_dataset.images\n",
        "\n",
        "for c in gen_dataset.keys():\n",
        "    gen_dataset[c] = (gen_dataset[c] + 1) / 2\n",
        "    gen_dataset[c] = gen_dataset[c].repeat(1, 3, 1, 1)\n",
        "# # Ajustar para o intervalo [0, 1]\n",
        "# gen_dataset = (gen_dataset + 1) / 2\n",
        "# Expandir o canal para RGB (replicando o canal 1 para 3)\n",
        "# gen_dataset = gen_dataset.repeat(1, 3, 1, 1)  # Agora tem shape [2050, 3, 28, 28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceinxLQKXoCf"
      },
      "outputs": [],
      "source": [
        "dataloaders = [torch.utils.data.DataLoader(gen_dataset[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwsjJ5fsXoCh"
      },
      "source": [
        "Calculo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQweQMlEXoCh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2x0DraxXoCi"
      },
      "outputs": [],
      "source": [
        "mus_gen = []\n",
        "sigmas_gen = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgtpBE95XoCl"
      },
      "outputs": [],
      "source": [
        "for c in range(10):\n",
        "  pred_arr = np.empty((len(gen_dataset[c]), dims))\n",
        "  start_idx = 0\n",
        "  for batch in tqdm(dataloaders[c]):\n",
        "          batch = batch.to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              pred = model(batch)[0]\n",
        "\n",
        "          # If model output is not scalar, apply global spatial average pooling.\n",
        "          # This happens if you choose a dimensionality not equal 2048.\n",
        "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "\n",
        "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
        "\n",
        "          start_idx = start_idx + pred.shape[0]\n",
        "  mus_gen.append(np.mean(pred_arr, axis=0))\n",
        "  sigmas_gen.append(np.cov(pred_arr, rowvar=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_w71nZ8XoCl"
      },
      "source": [
        "Calculo da distribuicao real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsx4vJ9BXoCm"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWEAlYaGXoCm"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
        "# Create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "trainloader_reduzido = torch.utils.data.DataLoader(trainset_reduzido, batch_size=128, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ODa5VTrXoCm"
      },
      "outputs": [],
      "source": [
        "testset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8js9LM5qXoCm"
      },
      "source": [
        "Pegando imagens sem salvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgwQDuAyXoCn"
      },
      "outputs": [],
      "source": [
        "def select_samples_per_class(dataset, num_samples):\n",
        "    \"\"\"\n",
        "    Selects a specified number of samples per class from the dataset and returns them as tensors.\n",
        "\n",
        "    Parameters:\n",
        "    dataset (torch.utils.data.Dataset): The dataset to select samples from.\n",
        "    num_samples (int): The number of samples to select per class.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where each key corresponds to a class and the value is a tensor of shape [num_samples, 1, 28, 28].\n",
        "    \"\"\"\n",
        "    class_samples = {i: [] for i in range(len(dataset.classes))}\n",
        "    class_counts = {i: 0 for i in range(len(dataset.classes))}\n",
        "\n",
        "    for img, label in dataset:\n",
        "        if class_counts[label] < num_samples:\n",
        "            class_samples[label].append(img)\n",
        "            class_counts[label] += 1\n",
        "        if all(count >= num_samples for count in class_counts.values()):\n",
        "            break\n",
        "    else:\n",
        "        print(\"Warning: Not all classes have the requested number of samples.\")\n",
        "\n",
        "    # Convert lists of tensors to a single tensor per class\n",
        "    for label in class_samples:\n",
        "        if class_samples[label]:  # Check if the list is not empty\n",
        "            class_samples[label] = torch.stack(class_samples[label], dim=0)\n",
        "            class_samples[label] = (class_samples[label] + 1) / 2\n",
        "            class_samples[label] = class_samples[label].repeat(1, 3, 1, 1)\n",
        "        else:\n",
        "            # Handle empty classes if necessary; here we leave an empty tensor\n",
        "            class_samples[label] = torch.Tensor()\n",
        "\n",
        "    return class_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxtmTmD6XoCn"
      },
      "outputs": [],
      "source": [
        "img_reais = select_samples_per_class(testset, 800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_r4SNe9XoCv"
      },
      "outputs": [],
      "source": [
        "dataloaders = [torch.utils.data.DataLoader(img_reais[c], batch_size=batch_size, num_workers=num_workers, shuffle=False) for c in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXFM_KcOXoCw"
      },
      "source": [
        "Salvando imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4HPBo8iXoCw"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRy47-hWXoCw"
      },
      "outputs": [],
      "source": [
        "# Function to save a random sample of images\n",
        "def save_random_samples(dataset, num_samples=10, folder='Imagens Testes/mnist_samples', balanced=False, classes=None):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    if classes is None:\n",
        "        classes = [int(c.split()[0]) for c in dataset.classes]  # Use all classes if none are specified\n",
        "\n",
        "    if balanced:\n",
        "        # Get the number of classes\n",
        "        num_classes = len(classes)\n",
        "        samples_per_class = -(-num_samples // num_classes)  # Round up division\n",
        "        indices = []\n",
        "        class_counts = {i: 0 for i in classes}\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        shuffled_indices = list(range(len(dataset)))\n",
        "        random.shuffle(shuffled_indices)\n",
        "\n",
        "        for idx in shuffled_indices:\n",
        "            img = dataset[idx][0]\n",
        "            label = int(dataset[idx][1])\n",
        "            if label in classes and class_counts[label] < samples_per_class:\n",
        "                indices.append(idx)\n",
        "                class_counts[label] += 1\n",
        "            if len(indices) >= num_samples:\n",
        "                break\n",
        "    else:\n",
        "        indices = []\n",
        "        while len(indices) < num_samples:\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            if int(dataset[idx][1]) in classes:\n",
        "                indices.append(idx)\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = dataset[idx]\n",
        "        img = (img * 0.5 + 0.5) * 255  # Denormalize the image\n",
        "        img = img.byte().numpy().transpose(1, 2, 0).squeeze()  # Convert to numpy array\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(os.path.join(folder, f'mnist_sample_{i}_label_{label}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhEt3LNsXoCx"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  save_random_samples(trainset, num_samples=2050, folder=f'Imagens Testes/mnist_samples_{i}', balanced=True, classes=[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXpIFPDtXoCx"
      },
      "outputs": [],
      "source": [
        "pathes = [f\"Imagens Testes/mnist_samples_{i}\" for i in range(10)]\n",
        "pathes = [pathlib.Path(path) for path in pathes]\n",
        "files = [sorted(file for file in path.glob(\"*.png\")) for path in pathes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U4U3uYZXoCx"
      },
      "outputs": [],
      "source": [
        "datasets = [ImagePathDataset(file, transforms=torchvision.transforms.ToTensor()) for file in files]\n",
        "dataloaders = [torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False) for dataset in datasets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVRjN1RbXoCx"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OHeQeC6XoCx"
      },
      "source": [
        "Calulo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeivOPmZXoCy"
      },
      "outputs": [],
      "source": [
        "mus_real = []\n",
        "sigmas_real = []\n",
        "for c in range(10):\n",
        "  model = InceptionV3([block_idx]).to(device)\n",
        "  model.eval()\n",
        "  pred_arr = np.empty((len(img_reais[0]), dims))\n",
        "  start_idx = 0\n",
        "  for batch in tqdm(dataloaders[c]):\n",
        "          batch = batch.to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              pred = model(batch)[0]\n",
        "\n",
        "          # If model output is not scalar, apply global spatial average pooling.\n",
        "          # This happens if you choose a dimensionality not equal 2048.\n",
        "          if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "              pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "          pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "\n",
        "          pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
        "\n",
        "          start_idx = start_idx + pred.shape[0]\n",
        "  mus_real.append(np.mean(pred_arr, axis=0))\n",
        "  sigmas_real.append(np.cov(pred_arr, rowvar=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXa0TM2lXoCy"
      },
      "source": [
        "Calculo FID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XtoaLpPXoCy"
      },
      "outputs": [],
      "source": [
        "from scipy import linalg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAGo9hN6XoCz"
      },
      "outputs": [],
      "source": [
        "mus_gen = [np.atleast_1d(mu_gen) for mu_gen in mus_gen]\n",
        "mus_real = [np.atleast_1d(mu_real) for mu_real in mus_real]\n",
        "\n",
        "sigmas_gen = [np.atleast_2d(sigma_gen) for sigma_gen in sigmas_gen]\n",
        "sigmas_real = [np.atleast_2d(sigma_real) for sigma_real in sigmas_real]\n",
        "\n",
        "for mu_gen, mu_real, sigma_gen, sigma_real in zip(mus_gen, mus_real, sigmas_gen, sigmas_real):\n",
        "  assert (\n",
        "      mu_gen.shape == mu_real.shape\n",
        "  ), \"Training and test mean vectors have different lengths\"\n",
        "  assert (\n",
        "      sigma_gen.shape == sigma_real.shape\n",
        "  ), \"Training and test covariances have different dimensions\"\n",
        "\n",
        "diffs = [mu_gen - mu_real for mu_gen, mu_real in zip(mus_gen, mus_real)]\n",
        "\n",
        "# Product might be almost singular\n",
        "covmeans = [linalg.sqrtm(sigmas_gen.dot(sigmas_real), disp=False)[0] for sigmas_gen, sigmas_real in zip(sigmas_gen, sigmas_real)]\n",
        "for covmean, sigma_gen, sigma_real in zip(covmeans, sigmas_gen, sigmas_real):\n",
        "  if not np.isfinite(covmean).all():\n",
        "    msg = (\n",
        "        \"fid calculation produces singular product; \"\n",
        "        \"adding %s to diagonal of cov estimates\"\n",
        "    ) % 1e-6\n",
        "    print(msg)\n",
        "    offset = np.eye(sigma_gen.shape[0]) * 1e-6\n",
        "    covmean = linalg.sqrtm((sigma_gen + offset).dot(sigma_real + offset))\n",
        "\n",
        "# Numerical error might give slight imaginary component\n",
        "for i, covmean in enumerate(covmeans):\n",
        "  if np.iscomplexobj(covmean):\n",
        "      if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "          m = np.max(np.abs(covmean.imag))\n",
        "          raise ValueError(\"Imaginary component {}\".format(m))\n",
        "      covmeans[i] = covmean.real\n",
        "\n",
        "tr_covmeans = [np.trace(covmean) for covmean in covmeans]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlYmUB0RXoCz"
      },
      "outputs": [],
      "source": [
        "fids = [diff.dot(diff) + np.trace(sigma_gen) + np.trace(sigma_real) - 2 * tr_covmean for diff, sigma_gen, sigma_real, tr_covmean in zip(diffs, sigmas_gen, sigmas_real, tr_covmeans)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UAzyULzXoCz"
      },
      "outputs": [],
      "source": [
        "fids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYo0rAs8XoCz"
      },
      "source": [
        "### Federado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flwr_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jgO5dbyOX9IB",
        "outputId": "a3b2b562-b7db-4f9e-b6b9-306668fa6d8c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flwr_datasets\n",
            "  Downloading flwr_datasets-0.5.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting datasets<=3.1.0,>=2.14.6 (from flwr_datasets)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.7.5 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (1.26.4)\n",
            "Requirement already satisfied: seaborn<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (0.13.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.14.6->flwr_datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.1.0,>=2.14.6->flwr_datasets) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.1.0,>=2.14.6->flwr_datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets<=3.1.0,>=2.14.6->flwr_datasets) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.14.6->flwr_datasets) (2025.1.31)\n",
            "Downloading flwr_datasets-0.5.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.0/87.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, flwr_datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 flwr_datasets-0.5.0 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nBvMjRnOXoCz"
      },
      "outputs": [],
      "source": [
        "from flwr_datasets.partitioner import DirichletPartitioner\n",
        "from flwr_datasets import FederatedDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "85htNladXoC0"
      },
      "outputs": [],
      "source": [
        "num_partitions = 1\n",
        "alpha_dir = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R2Xjlca_XoC0"
      },
      "outputs": [],
      "source": [
        "partitioner = DirichletPartitioner(\n",
        "    num_partitions=num_partitions,\n",
        "    partition_by=\"label\",\n",
        "    alpha=alpha_dir,\n",
        "    min_partition_size=0,\n",
        "    self_balancing=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "227p665iXoC0"
      },
      "outputs": [],
      "source": [
        "fds = FederatedDataset(\n",
        "    dataset=\"mnist\",\n",
        "    partitioners={\"train\": partitioner}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jw5CezaqXoC0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "46b96767f5e94b1dab5289f7ca493845",
            "fe0dac5b8f4b4b2db3b12745a0ea41bb",
            "4c559859b32d4c8e9ff5bdc179a3dbc2",
            "9fe53a5854df445dad0487dfc9035021",
            "7f3112f458a1497bafc6dfa40b3873f6",
            "5a673e78c8104c17b88cf65efddf4fcd",
            "2c7ed38f9c094e99b04b89a4121085f0",
            "1964b90585344c1880894924f1d058c5",
            "79be0691cc304fbc974e2463f055b5ad",
            "582503ab5a96473f974e7e68491f5ef2",
            "02ebbb4212b44d70b3c2860c8d98a1e1",
            "ecd6b36466d7420faa39384fbfff990a",
            "d7a1a9f475f9408e991033c95e4c2a7d",
            "928472df19414ba380a724a5d986c5f7",
            "3ede49350efb4a66bb9d9bd9c50d8402",
            "f1d0fd160d944dba80868e3024371e1b",
            "063a8820ba58452d88fdbc48049680cb",
            "456b4c7409c54888acd47a0c085b57de",
            "75abc3232c9a4e71aca3ac1cbc333e65",
            "28fa1055f85e42d584662c332df28cdb",
            "b5fadde8538c40399d0d6175992a2c08",
            "33c90b7d0dff423bb7bb6b6fe1bcd9f5",
            "c65cbcde25be4b67ad0972777fba7164",
            "856b2392293943a0a31ce86e48abf3b4",
            "bf453f0267b841bfbeb8dface06da913",
            "be0c4ae734eb405ab74533eb8ae6d677",
            "bfdd786d2c1640728fa4701f3de65680",
            "07f4993d5ce94b90896882f75058ad4a",
            "0c64263c0cbb4d6a8bb03085f87f806b",
            "5687f3380b0f4371aab7b74c96bf3eae",
            "50ba90dce2634ba7bd282722d41eb7d6",
            "2e4bf9df1c924c8fb2ac5da26778d5f0",
            "a8a6950ad39545de9aaab01e502b1c7e",
            "4f105781904045389b9f80fef1422204",
            "03fe67e2b82e46eda94c5735eb0b3838",
            "0c335ee394e5454688e169253ccc202a",
            "2184aff93e5e4ad4a2bcba4e1b352353",
            "d09cd624408c4b12bca2f98fe5cc9e4d",
            "38f5264ea38e490ab1730095821d5732",
            "b2c2ad47f5714749b1e233d731137a32",
            "71d3ef1e998342b6a909d098edbe5840",
            "eb53eb56d6814e3495324cd1720e403f",
            "bec86a3b96264fa4b742f12438b65c89",
            "9ad15303c12b4fed98d8179f75fa6c6d",
            "748f44fc54004f80b40dc65aa3092679",
            "fac0eb3e353641ce9aab2b90f03a75a1",
            "d8237488423e4f8aab99caecee63f0be",
            "e9655c60cd43415db6a81d0f688b2bf7",
            "2047512a3b8343ea9b84080d1355820b",
            "f1cf09270eb745ea8364b3e44acf920a",
            "518ce70cd0104e37a68d62e9b92c6fc0",
            "c7e04c9861094782b045c8eb510f954f",
            "317cc826243e41c6baea198d26fc8ea7",
            "006c513f6fd54337835cd1073b081b0a",
            "bed1ff516b5e4c33b68312f32fdd7a6f"
          ]
        },
        "outputId": "ce63cdcf-8d4d-48b2-962f-f906f96cda02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/6.97k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46b96767f5e94b1dab5289f7ca493845"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/15.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecd6b36466d7420faa39384fbfff990a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c65cbcde25be4b67ad0972777fba7164"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f105781904045389b9f80fef1422204"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "748f44fc54004f80b40dc65aa3092679"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_partitions = [fds.load_partition(i, split=\"train\") for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Rodar proxima celula somente se quiser testar com dataset reduzido"
      ],
      "metadata": {
        "id": "kJ05wNS_ZSi3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-DfNUiSXoC0"
      },
      "outputs": [],
      "source": [
        "num_samples = [int(len(train_partition)/10) for train_partition in train_partitions]\n",
        "train_partitions = [train_partition.select(range(n)) for train_partition, n in zip(train_partitions, num_samples)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BkqcHp6eXoC1"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LIroIscqXoC1"
      },
      "outputs": [],
      "source": [
        "pytorch_transforms = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nPXZ9XAjXoC1"
      },
      "outputs": [],
      "source": [
        "train_partitions = [train_partition.with_transform(apply_transforms) for train_partition in train_partitions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kF7BB2O-XoC1"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "trainloaders = [DataLoader(train_partition, batch_size=batch_size, shuffle=True) for train_partition in train_partitions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "00vLeJqCXoC2"
      },
      "outputs": [],
      "source": [
        "models = [CGAN() for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrgGRhGmXoC2",
        "outputId": "7b2c21d9-7715-442a-985a-861b784b1e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soma total dos valores dos parâmetros do gerador do modelo 0: 1751.9129284620285\n"
          ]
        }
      ],
      "source": [
        "for idx, model in enumerate(models):\n",
        "    generator_params = [param.data.numpy() for param in model.generator.parameters()]\n",
        "    generator_params_sum = sum([param.sum() for param in generator_params])\n",
        "    print(f\"Soma total dos valores dos parâmetros do gerador do modelo {idx}: {generator_params_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aafji_4jXoC3",
        "outputId": "d95ffb4a-7fb7-47a8-9a87-e863bed21e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Soma total dos valores dos parâmetros do gerador do modelo 0: 328.63008058443666\n",
            "Soma total dos valores dos parâmetros do gerador do modelo 1: 237.89485466852784\n",
            "Soma total dos valores dos parâmetros do gerador do modelo 2: 186.06141594797373\n",
            "Soma total dos valores dos parâmetros do gerador do modelo 3: 222.37551382556558\n"
          ]
        }
      ],
      "source": [
        "for idx, model in enumerate(models):\n",
        "    disc_params = [param.data.numpy() for param in model.discriminator.parameters()]\n",
        "    disc_params_sum = sum([param.sum() for param in disc_params])\n",
        "    print(f\"Soma total dos valores dos parâmetros do gerador do modelo {idx}: {disc_params_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iKBK-xyXoC3",
        "outputId": "5f9fe266-99bd-446f-a567-1d05ea02f2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 [100/134] loss_D_treino: 0.0004\n",
            "Epoch 1 [100/134] loss_D_treino: 0.0001\n",
            "Epoch 0 [100/156] loss_D_treino: 0.0004\n",
            "Epoch 1 [100/156] loss_D_treino: 0.0001\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "for net, trainloader in zip(models, trainloaders):\n",
        "    train_gen(net=net,\n",
        "              trainloader=trainloader,\n",
        "              epochs=epochs,\n",
        "              lr=0.0001,\n",
        "              device=\"cpu\",\n",
        "              dataset=\"mnist\",\n",
        "              latent_dim=100,\n",
        "              f2a=True,\n",
        "              cliente=True\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3NZ6Ak66XoC4"
      },
      "outputs": [],
      "source": [
        "def generate_plot(net, device, round_number, client_id = None, examples_per_class: int=5, classes: int=10, latent_dim: int=100, server: bool=False):\n",
        "    \"\"\"Gera plot de imagens de cada classe\"\"\"\n",
        "    if server:\n",
        "        import matplotlib\n",
        "        matplotlib.use(\"Agg\")\n",
        "        import matplotlib.pyplot as plt\n",
        "    else:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    batch_size = examples_per_class * classes\n",
        "\n",
        "    latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
        "    labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_images = net(latent_vectors, labels).cpu()\n",
        "\n",
        "    # Criar uma figura com 10 linhas e 5 colunas de subplots\n",
        "    fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
        "\n",
        "    # Adiciona título no topo da figura\n",
        "    if client_id:\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
        "    else:\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number-1}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "    # Exibir as imagens nos subplots\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    # Ajustar o layout antes de calcular as posições\n",
        "    plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "    # Reduzir espaço entre colunas\n",
        "    # plt.subplots_adjust(wspace=0.05)\n",
        "\n",
        "    # Adicionar os rótulos das classes corretamente alinhados\n",
        "    fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
        "    for row in range(classes):\n",
        "        # Obter posição do subplot em coordenadas da figura\n",
        "        bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "        pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "        center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "        # Adicionar o rótulo\n",
        "        fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
        "\n",
        "    fig.savefig(f\"mnist_CGAN_r{round_number}_f2a.png\")\n",
        "    plt.close(fig)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VZuTSmgYXoC4"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X7L_24AOXoC5"
      },
      "outputs": [],
      "source": [
        "gen = CGAN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Z8SN_A1oXoC5"
      },
      "outputs": [],
      "source": [
        "net = CGAN()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Treinamento Normal"
      ],
      "metadata": {
        "id": "jrMTGrznZNie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GIbSmG5ZXoC5",
        "outputId": "fe27446d-e8b2-4d16-c8ba-8497e6372bdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 [100/469] loss_D_treino: 0.0000 loss_G_treino: 13.1713\n",
            "Epoch 0 [200/469] loss_D_treino: 0.0004 loss_G_treino: 13.6314\n",
            "Epoch 0 [300/469] loss_D_treino: 0.0003 loss_G_treino: 9.8235\n",
            "Epoch 0 [400/469] loss_D_treino: 0.0001 loss_G_treino: 10.6263\n",
            "Epoch 1 [100/469] loss_D_treino: 0.0001 loss_G_treino: 13.8629\n",
            "Epoch 1 [200/469] loss_D_treino: 0.0001 loss_G_treino: 15.3786\n",
            "Epoch 1 [300/469] loss_D_treino: 0.0000 loss_G_treino: 16.4410\n",
            "Epoch 1 [400/469] loss_D_treino: 0.0000 loss_G_treino: 17.1211\n",
            "Epoch 2 [100/469] loss_D_treino: 0.0000 loss_G_treino: 22.9830\n",
            "Epoch 2 [200/469] loss_D_treino: 0.0001 loss_G_treino: 25.0739\n",
            "Epoch 2 [300/469] loss_D_treino: 0.0000 loss_G_treino: 25.2764\n",
            "Epoch 2 [400/469] loss_D_treino: 0.0000 loss_G_treino: 25.9688\n",
            "Epoch 3 [100/469] loss_D_treino: 0.0000 loss_G_treino: 25.1124\n",
            "Epoch 3 [200/469] loss_D_treino: 0.0000 loss_G_treino: 23.1310\n",
            "Epoch 3 [300/469] loss_D_treino: 0.0000 loss_G_treino: 23.0600\n",
            "Epoch 3 [400/469] loss_D_treino: 0.0000 loss_G_treino: 20.4331\n",
            "Epoch 4 [100/469] loss_D_treino: 0.0000 loss_G_treino: 20.0439\n",
            "Epoch 4 [200/469] loss_D_treino: 0.0000 loss_G_treino: 19.5436\n",
            "Epoch 4 [300/469] loss_D_treino: 0.0000 loss_G_treino: 18.5789\n",
            "Epoch 4 [400/469] loss_D_treino: 0.0000 loss_G_treino: 18.5903\n",
            "Epoch 5 [100/469] loss_D_treino: 0.0000 loss_G_treino: 17.6145\n",
            "Epoch 5 [200/469] loss_D_treino: 0.0000 loss_G_treino: 16.6716\n",
            "Epoch 5 [300/469] loss_D_treino: 0.0000 loss_G_treino: 16.1160\n",
            "Epoch 5 [400/469] loss_D_treino: 0.0000 loss_G_treino: 16.7640\n",
            "Epoch 6 [100/469] loss_D_treino: 0.0000 loss_G_treino: 17.0087\n",
            "Epoch 6 [200/469] loss_D_treino: 0.0000 loss_G_treino: 16.7377\n",
            "Epoch 6 [300/469] loss_D_treino: 0.0000 loss_G_treino: 16.7486\n",
            "Epoch 6 [400/469] loss_D_treino: 0.0000 loss_G_treino: 17.1509\n",
            "Epoch 7 [100/469] loss_D_treino: 0.0000 loss_G_treino: 17.9734\n",
            "Epoch 7 [200/469] loss_D_treino: 0.0000 loss_G_treino: 17.4602\n",
            "Epoch 7 [300/469] loss_D_treino: 0.0000 loss_G_treino: 17.6704\n",
            "Epoch 7 [400/469] loss_D_treino: 0.0000 loss_G_treino: 17.7758\n",
            "Epoch 8 [100/469] loss_D_treino: 0.0000 loss_G_treino: 17.8958\n",
            "Epoch 8 [200/469] loss_D_treino: 0.0000 loss_G_treino: 18.0254\n",
            "Epoch 8 [300/469] loss_D_treino: 0.0000 loss_G_treino: 18.0384\n",
            "Epoch 8 [400/469] loss_D_treino: 0.0000 loss_G_treino: 18.6689\n",
            "Epoch 9 [100/469] loss_D_treino: 0.0000 loss_G_treino: 19.3255\n",
            "Epoch 9 [200/469] loss_D_treino: 0.0000 loss_G_treino: 18.6742\n",
            "Epoch 9 [300/469] loss_D_treino: 0.0000 loss_G_treino: 18.9646\n",
            "Epoch 9 [400/469] loss_D_treino: 0.0000 loss_G_treino: 19.0366\n",
            "Epoch 10 [100/469] loss_D_treino: 0.0000 loss_G_treino: 19.6412\n",
            "Epoch 10 [200/469] loss_D_treino: 0.0000 loss_G_treino: 19.7935\n",
            "Epoch 10 [300/469] loss_D_treino: 0.0000 loss_G_treino: 19.7259\n",
            "Epoch 10 [400/469] loss_D_treino: 0.0000 loss_G_treino: 20.3234\n",
            "Epoch 11 [100/469] loss_D_treino: 0.0000 loss_G_treino: 20.7797\n",
            "Epoch 11 [200/469] loss_D_treino: 0.0000 loss_G_treino: 21.4888\n",
            "Epoch 11 [300/469] loss_D_treino: 0.0000 loss_G_treino: 21.1526\n",
            "Epoch 11 [400/469] loss_D_treino: 0.0000 loss_G_treino: 20.9488\n",
            "Epoch 12 [100/469] loss_D_treino: 0.0000 loss_G_treino: 20.0572\n",
            "Epoch 12 [200/469] loss_D_treino: 0.0000 loss_G_treino: 18.1556\n",
            "Epoch 12 [300/469] loss_D_treino: 0.0000 loss_G_treino: 19.3763\n",
            "Epoch 12 [400/469] loss_D_treino: 0.0000 loss_G_treino: 16.4229\n",
            "Epoch 13 [100/469] loss_D_treino: 0.0000 loss_G_treino: 16.6320\n",
            "Epoch 13 [200/469] loss_D_treino: 0.0000 loss_G_treino: 16.9211\n",
            "Epoch 13 [300/469] loss_D_treino: 0.0000 loss_G_treino: 17.4741\n",
            "Epoch 13 [400/469] loss_D_treino: 0.0000 loss_G_treino: 17.0683\n",
            "Epoch 14 [100/469] loss_D_treino: 0.0000 loss_G_treino: 16.9701\n",
            "Epoch 14 [200/469] loss_D_treino: 0.0000 loss_G_treino: 16.6790\n",
            "Epoch 14 [300/469] loss_D_treino: 0.0000 loss_G_treino: 17.6570\n",
            "Epoch 14 [400/469] loss_D_treino: 0.0000 loss_G_treino: 17.5587\n",
            "Epoch 15 [100/469] loss_D_treino: 0.0000 loss_G_treino: 18.3055\n",
            "Epoch 15 [200/469] loss_D_treino: 0.0000 loss_G_treino: 18.1632\n",
            "Epoch 15 [300/469] loss_D_treino: 0.0000 loss_G_treino: 18.1444\n",
            "Epoch 15 [400/469] loss_D_treino: 0.0000 loss_G_treino: 18.2032\n",
            "Epoch 16 [100/469] loss_D_treino: 0.0000 loss_G_treino: 17.9168\n",
            "Epoch 16 [200/469] loss_D_treino: 0.0000 loss_G_treino: 18.6085\n",
            "Epoch 16 [300/469] loss_D_treino: 0.0000 loss_G_treino: 18.4536\n",
            "Epoch 16 [400/469] loss_D_treino: 0.0000 loss_G_treino: 18.0406\n",
            "Epoch 17 [100/469] loss_D_treino: 0.0000 loss_G_treino: 18.1854\n",
            "Epoch 17 [200/469] loss_D_treino: 0.0000 loss_G_treino: 18.1755\n",
            "Epoch 17 [300/469] loss_D_treino: 0.0000 loss_G_treino: 18.2766\n",
            "Epoch 17 [400/469] loss_D_treino: 0.0000 loss_G_treino: 18.2469\n",
            "Epoch 18 [100/469] loss_D_treino: 0.0000 loss_G_treino: 18.6474\n",
            "Epoch 18 [200/469] loss_D_treino: 0.0000 loss_G_treino: 18.7458\n",
            "Epoch 18 [300/469] loss_D_treino: 0.0000 loss_G_treino: 18.5611\n",
            "Epoch 18 [400/469] loss_D_treino: 0.0000 loss_G_treino: 18.7944\n",
            "Epoch 19 [100/469] loss_D_treino: 0.0000 loss_G_treino: 19.6598\n",
            "Epoch 19 [200/469] loss_D_treino: 0.0000 loss_G_treino: 18.6447\n",
            "Epoch 19 [300/469] loss_D_treino: 0.0000 loss_G_treino: 18.9339\n",
            "Epoch 19 [400/469] loss_D_treino: 0.0000 loss_G_treino: 18.9927\n",
            "Epoch 20 [100/469] loss_D_treino: 0.0000 loss_G_treino: 19.4633\n",
            "Epoch 20 [200/469] loss_D_treino: 0.0000 loss_G_treino: 20.0759\n",
            "Epoch 20 [300/469] loss_D_treino: 0.0000 loss_G_treino: 19.5702\n",
            "Epoch 20 [400/469] loss_D_treino: 0.0000 loss_G_treino: 20.0350\n",
            "Epoch 21 [100/469] loss_D_treino: 0.0000 loss_G_treino: 21.0679\n",
            "Epoch 21 [200/469] loss_D_treino: 0.0000 loss_G_treino: 21.4847\n",
            "Epoch 21 [300/469] loss_D_treino: 0.0000 loss_G_treino: 21.7699\n",
            "Epoch 21 [400/469] loss_D_treino: 0.0000 loss_G_treino: 22.1077\n",
            "Epoch 22 [100/469] loss_D_treino: 0.0000 loss_G_treino: 21.9391\n",
            "Epoch 22 [200/469] loss_D_treino: 0.0000 loss_G_treino: 22.7868\n",
            "Epoch 22 [300/469] loss_D_treino: 0.0000 loss_G_treino: 22.7737\n",
            "Epoch 22 [400/469] loss_D_treino: 0.0000 loss_G_treino: 23.1223\n",
            "Epoch 23 [100/469] loss_D_treino: 0.0000 loss_G_treino: 24.4093\n",
            "Epoch 23 [200/469] loss_D_treino: 0.0000 loss_G_treino: 24.7748\n",
            "Epoch 23 [300/469] loss_D_treino: 0.0000 loss_G_treino: 25.4106\n",
            "Epoch 23 [400/469] loss_D_treino: 0.0000 loss_G_treino: 25.2950\n",
            "Epoch 24 [100/469] loss_D_treino: 0.0000 loss_G_treino: 27.3160\n",
            "Epoch 24 [200/469] loss_D_treino: 0.0000 loss_G_treino: 27.6602\n",
            "Epoch 24 [300/469] loss_D_treino: 0.0000 loss_G_treino: 27.7550\n",
            "Epoch 24 [400/469] loss_D_treino: 0.0000 loss_G_treino: 27.9567\n",
            "Epoch 25 [100/469] loss_D_treino: 0.0000 loss_G_treino: 27.3070\n",
            "Epoch 25 [200/469] loss_D_treino: 0.0000 loss_G_treino: 27.5870\n",
            "Epoch 25 [300/469] loss_D_treino: 0.0000 loss_G_treino: 28.0228\n",
            "Epoch 25 [400/469] loss_D_treino: 0.0000 loss_G_treino: 27.8565\n",
            "Epoch 26 [100/469] loss_D_treino: 0.0000 loss_G_treino: 26.8113\n",
            "Epoch 26 [200/469] loss_D_treino: 0.0000 loss_G_treino: 27.1192\n",
            "Epoch 26 [300/469] loss_D_treino: 0.0000 loss_G_treino: 26.1697\n",
            "Epoch 26 [400/469] loss_D_treino: 0.0000 loss_G_treino: 25.8844\n",
            "Epoch 27 [100/469] loss_D_treino: 0.0000 loss_G_treino: 26.2843\n",
            "Epoch 27 [200/469] loss_D_treino: 0.0000 loss_G_treino: 26.7666\n",
            "Epoch 27 [300/469] loss_D_treino: 0.0000 loss_G_treino: 25.7399\n",
            "Epoch 27 [400/469] loss_D_treino: 0.0000 loss_G_treino: 26.1028\n",
            "Epoch 28 [100/469] loss_D_treino: 0.0000 loss_G_treino: 26.4640\n",
            "Epoch 28 [200/469] loss_D_treino: 0.0000 loss_G_treino: 26.1052\n",
            "Epoch 28 [300/469] loss_D_treino: 0.0000 loss_G_treino: 26.6324\n",
            "Epoch 28 [400/469] loss_D_treino: 0.0000 loss_G_treino: 26.8030\n",
            "Epoch 29 [100/469] loss_D_treino: 0.0000 loss_G_treino: 26.8408\n",
            "Epoch 29 [200/469] loss_D_treino: 0.0000 loss_G_treino: 27.3069\n",
            "Epoch 29 [300/469] loss_D_treino: 0.0000 loss_G_treino: 27.7452\n",
            "Epoch 29 [400/469] loss_D_treino: 0.0000 loss_G_treino: 27.6432\n",
            "Epoch 30 [100/469] loss_D_treino: 0.0000 loss_G_treino: 27.1720\n",
            "Epoch 30 [200/469] loss_D_treino: 0.0000 loss_G_treino: 27.1997\n",
            "Epoch 30 [300/469] loss_D_treino: 0.0000 loss_G_treino: 27.2778\n",
            "Epoch 30 [400/469] loss_D_treino: 0.0000 loss_G_treino: 26.8734\n",
            "Epoch 31 [100/469] loss_D_treino: 0.0000 loss_G_treino: 27.2276\n",
            "Epoch 31 [200/469] loss_D_treino: 0.0000 loss_G_treino: 27.0792\n",
            "Epoch 31 [300/469] loss_D_treino: 0.0000 loss_G_treino: 27.0617\n",
            "Epoch 31 [400/469] loss_D_treino: 0.0000 loss_G_treino: 27.8467\n",
            "Epoch 32 [100/469] loss_D_treino: 0.0000 loss_G_treino: 27.1313\n",
            "Epoch 32 [200/469] loss_D_treino: 0.0000 loss_G_treino: 26.6847\n",
            "Epoch 32 [300/469] loss_D_treino: 0.0000 loss_G_treino: 27.0131\n",
            "Epoch 32 [400/469] loss_D_treino: 0.0000 loss_G_treino: 27.3465\n",
            "Epoch 33 [100/469] loss_D_treino: 0.0000 loss_G_treino: 26.7584\n",
            "Epoch 33 [200/469] loss_D_treino: 0.0000 loss_G_treino: 26.6412\n",
            "Epoch 33 [300/469] loss_D_treino: 0.0000 loss_G_treino: 26.4425\n",
            "Epoch 33 [400/469] loss_D_treino: 0.0000 loss_G_treino: 25.9598\n",
            "Epoch 34 [100/469] loss_D_treino: 0.0000 loss_G_treino: 25.9244\n",
            "Epoch 34 [200/469] loss_D_treino: 0.0000 loss_G_treino: 25.9650\n",
            "Epoch 34 [300/469] loss_D_treino: 0.0000 loss_G_treino: 25.3782\n",
            "Epoch 34 [400/469] loss_D_treino: 0.0000 loss_G_treino: 25.2163\n",
            "Epoch 35 [100/469] loss_D_treino: 0.0000 loss_G_treino: 25.5090\n",
            "Epoch 35 [200/469] loss_D_treino: 0.0000 loss_G_treino: 24.8997\n",
            "Epoch 35 [300/469] loss_D_treino: 0.0000 loss_G_treino: 24.4701\n",
            "Epoch 35 [400/469] loss_D_treino: 0.0000 loss_G_treino: 24.8093\n",
            "Epoch 36 [100/469] loss_D_treino: 0.0000 loss_G_treino: 24.8100\n",
            "Epoch 36 [200/469] loss_D_treino: 0.0000 loss_G_treino: 24.7037\n",
            "Epoch 36 [300/469] loss_D_treino: 0.0000 loss_G_treino: 24.5686\n",
            "Epoch 36 [400/469] loss_D_treino: 0.0000 loss_G_treino: 24.3706\n",
            "Epoch 37 [100/469] loss_D_treino: 0.0000 loss_G_treino: 24.3007\n",
            "Epoch 37 [200/469] loss_D_treino: 0.0000 loss_G_treino: 24.5111\n",
            "Epoch 37 [300/469] loss_D_treino: 0.0000 loss_G_treino: 24.4912\n",
            "Epoch 37 [400/469] loss_D_treino: 0.0000 loss_G_treino: 24.4900\n",
            "Epoch 38 [100/469] loss_D_treino: 0.0000 loss_G_treino: 23.9845\n",
            "Epoch 38 [200/469] loss_D_treino: 0.0000 loss_G_treino: 23.9948\n",
            "Epoch 38 [300/469] loss_D_treino: 0.0000 loss_G_treino: 23.9840\n",
            "Epoch 38 [400/469] loss_D_treino: 0.0000 loss_G_treino: 23.8274\n",
            "Epoch 39 [100/469] loss_D_treino: 0.0000 loss_G_treino: 23.6519\n",
            "Epoch 39 [200/469] loss_D_treino: 0.0000 loss_G_treino: 23.6882\n",
            "Epoch 39 [300/469] loss_D_treino: 0.0000 loss_G_treino: 24.3531\n",
            "Epoch 39 [400/469] loss_D_treino: 0.0000 loss_G_treino: 24.1260\n",
            "Epoch 40 [100/469] loss_D_treino: 0.0000 loss_G_treino: 24.5867\n",
            "Epoch 40 [200/469] loss_D_treino: 0.0000 loss_G_treino: 24.7488\n",
            "Epoch 40 [300/469] loss_D_treino: 0.0000 loss_G_treino: 24.6003\n",
            "Epoch 40 [400/469] loss_D_treino: 0.0000 loss_G_treino: 24.5400\n",
            "Epoch 41 [100/469] loss_D_treino: 0.0000 loss_G_treino: 25.1178\n",
            "Epoch 41 [200/469] loss_D_treino: 0.0000 loss_G_treino: 24.9509\n",
            "Epoch 41 [300/469] loss_D_treino: 0.0000 loss_G_treino: 24.7660\n",
            "Epoch 41 [400/469] loss_D_treino: 0.0000 loss_G_treino: 24.9975\n",
            "Epoch 42 [100/469] loss_D_treino: 0.0000 loss_G_treino: 25.0347\n",
            "Epoch 42 [200/469] loss_D_treino: 0.0000 loss_G_treino: 25.3500\n",
            "Epoch 42 [300/469] loss_D_treino: 0.0000 loss_G_treino: 25.1000\n",
            "Epoch 42 [400/469] loss_D_treino: 0.0000 loss_G_treino: 25.3018\n",
            "Epoch 43 [100/469] loss_D_treino: 0.0000 loss_G_treino: 25.4233\n",
            "Epoch 43 [200/469] loss_D_treino: 0.0000 loss_G_treino: 25.7333\n",
            "Epoch 43 [300/469] loss_D_treino: 0.0000 loss_G_treino: 25.5301\n",
            "Epoch 43 [400/469] loss_D_treino: 0.0000 loss_G_treino: 26.0048\n",
            "Epoch 44 [100/469] loss_D_treino: 0.0000 loss_G_treino: 25.9144\n",
            "Epoch 44 [200/469] loss_D_treino: 0.0000 loss_G_treino: 26.4585\n",
            "Epoch 44 [300/469] loss_D_treino: 0.0000 loss_G_treino: 26.6391\n",
            "Epoch 44 [400/469] loss_D_treino: 0.0000 loss_G_treino: 26.6804\n",
            "Epoch 45 [100/469] loss_D_treino: 0.0000 loss_G_treino: 27.1142\n",
            "Epoch 45 [200/469] loss_D_treino: 0.0000 loss_G_treino: 27.3880\n",
            "Epoch 45 [300/469] loss_D_treino: 0.0000 loss_G_treino: 27.8127\n",
            "Epoch 45 [400/469] loss_D_treino: 0.0000 loss_G_treino: 28.0256\n",
            "Epoch 46 [100/469] loss_D_treino: 0.0000 loss_G_treino: 27.5764\n",
            "Epoch 46 [200/469] loss_D_treino: 0.0000 loss_G_treino: 28.0094\n",
            "Epoch 46 [300/469] loss_D_treino: 0.0000 loss_G_treino: 28.6472\n",
            "Epoch 46 [400/469] loss_D_treino: 0.0000 loss_G_treino: 28.8190\n",
            "Epoch 47 [100/469] loss_D_treino: 0.0000 loss_G_treino: 29.1855\n",
            "Epoch 47 [200/469] loss_D_treino: 0.0000 loss_G_treino: 28.5761\n",
            "Epoch 47 [300/469] loss_D_treino: 0.0000 loss_G_treino: 29.1283\n",
            "Epoch 47 [400/469] loss_D_treino: 0.0000 loss_G_treino: 28.9871\n",
            "Epoch 48 [100/469] loss_D_treino: 0.0000 loss_G_treino: 29.2649\n",
            "Epoch 48 [200/469] loss_D_treino: 0.0000 loss_G_treino: 29.3167\n",
            "Epoch 48 [300/469] loss_D_treino: 0.0000 loss_G_treino: 29.3969\n",
            "Epoch 48 [400/469] loss_D_treino: 0.0000 loss_G_treino: 29.6265\n",
            "Epoch 49 [100/469] loss_D_treino: 0.0000 loss_G_treino: 29.8498\n",
            "Epoch 49 [200/469] loss_D_treino: 0.0000 loss_G_treino: 29.3632\n",
            "Epoch 49 [300/469] loss_D_treino: 0.0000 loss_G_treino: 29.6379\n",
            "Epoch 49 [400/469] loss_D_treino: 0.0000 loss_G_treino: 29.7063\n",
            "Epoch 50 [100/469] loss_D_treino: 0.0000 loss_G_treino: 29.6206\n",
            "Epoch 50 [200/469] loss_D_treino: 0.0000 loss_G_treino: 29.6573\n",
            "Epoch 50 [300/469] loss_D_treino: 0.0000 loss_G_treino: 30.0581\n",
            "Epoch 50 [400/469] loss_D_treino: 0.0000 loss_G_treino: 30.3253\n",
            "Epoch 51 [100/469] loss_D_treino: 0.0000 loss_G_treino: 30.5105\n",
            "Epoch 51 [200/469] loss_D_treino: 0.0000 loss_G_treino: 30.5466\n",
            "Epoch 51 [300/469] loss_D_treino: 0.0000 loss_G_treino: 30.5504\n",
            "Epoch 51 [400/469] loss_D_treino: 0.0000 loss_G_treino: 30.8115\n",
            "Epoch 52 [100/469] loss_D_treino: 0.0000 loss_G_treino: 30.5840\n",
            "Epoch 52 [200/469] loss_D_treino: 0.0000 loss_G_treino: 30.7053\n",
            "Epoch 52 [300/469] loss_D_treino: 0.0000 loss_G_treino: 30.6806\n",
            "Epoch 52 [400/469] loss_D_treino: 0.0000 loss_G_treino: 30.6762\n",
            "Epoch 53 [100/469] loss_D_treino: 0.0000 loss_G_treino: 30.3600\n",
            "Epoch 53 [200/469] loss_D_treino: 0.0000 loss_G_treino: 30.8500\n",
            "Epoch 53 [300/469] loss_D_treino: 0.0000 loss_G_treino: 30.8227\n",
            "Epoch 53 [400/469] loss_D_treino: 0.0000 loss_G_treino: 30.4256\n",
            "Epoch 54 [100/469] loss_D_treino: 0.0000 loss_G_treino: 30.8894\n",
            "Epoch 54 [200/469] loss_D_treino: 0.0000 loss_G_treino: 30.7704\n",
            "Epoch 54 [300/469] loss_D_treino: 0.0000 loss_G_treino: 30.8388\n",
            "Epoch 54 [400/469] loss_D_treino: 0.0000 loss_G_treino: 31.1483\n",
            "Epoch 55 [100/469] loss_D_treino: 0.0000 loss_G_treino: 30.9614\n",
            "Epoch 55 [200/469] loss_D_treino: 0.0000 loss_G_treino: 31.0273\n",
            "Epoch 55 [300/469] loss_D_treino: 0.0000 loss_G_treino: 31.1154\n",
            "Epoch 55 [400/469] loss_D_treino: 0.0000 loss_G_treino: 31.1178\n",
            "Epoch 56 [100/469] loss_D_treino: 0.0000 loss_G_treino: 31.1539\n",
            "Epoch 56 [200/469] loss_D_treino: 0.0000 loss_G_treino: 31.1810\n",
            "Epoch 56 [300/469] loss_D_treino: 0.0000 loss_G_treino: 31.3472\n",
            "Epoch 56 [400/469] loss_D_treino: 0.0000 loss_G_treino: 31.3369\n",
            "Epoch 57 [100/469] loss_D_treino: 0.0000 loss_G_treino: 31.9083\n",
            "Epoch 57 [200/469] loss_D_treino: 0.0000 loss_G_treino: 32.2677\n",
            "Epoch 57 [300/469] loss_D_treino: 0.0000 loss_G_treino: 31.8974\n",
            "Epoch 57 [400/469] loss_D_treino: 0.0000 loss_G_treino: 32.2954\n",
            "Epoch 58 [100/469] loss_D_treino: 0.0000 loss_G_treino: 31.7218\n",
            "Epoch 58 [200/469] loss_D_treino: 0.0000 loss_G_treino: 32.0885\n",
            "Epoch 58 [300/469] loss_D_treino: 0.0000 loss_G_treino: 32.3697\n",
            "Epoch 58 [400/469] loss_D_treino: 0.0000 loss_G_treino: 32.2209\n",
            "Epoch 59 [100/469] loss_D_treino: 0.0000 loss_G_treino: 32.3699\n",
            "Epoch 59 [200/469] loss_D_treino: 0.0000 loss_G_treino: 32.6728\n",
            "Epoch 59 [300/469] loss_D_treino: 0.0000 loss_G_treino: 33.0453\n",
            "Epoch 59 [400/469] loss_D_treino: 0.0000 loss_G_treino: 32.6501\n",
            "Epoch 60 [100/469] loss_D_treino: 0.0000 loss_G_treino: 31.9855\n",
            "Epoch 60 [200/469] loss_D_treino: 0.0000 loss_G_treino: 33.0815\n",
            "Epoch 60 [300/469] loss_D_treino: 0.0000 loss_G_treino: 32.5928\n",
            "Epoch 60 [400/469] loss_D_treino: 0.0000 loss_G_treino: 32.9265\n"
          ]
        }
      ],
      "source": [
        "net.to(device)  # move model to GPU if available\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optim_D = torch.optim.Adam(net.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "g_losses_batch = []\n",
        "d_losses_batch = []\n",
        "g_losses_epoch = []\n",
        "d_losses_epoch = []\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    epoch_g_loss = 0.0\n",
        "    epoch_d_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for batch_idx, batch in enumerate(trainloaders[0]):\n",
        "        images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "        fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "        # Train D\n",
        "        net.zero_grad()\n",
        "        z_noise = torch.randn(batch_size, 100, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "        y_real = net(images, labels)\n",
        "        d_real_loss = net.loss(y_real, real_ident)\n",
        "        y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "        d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optim_D.step()\n",
        "\n",
        "        # Train G\n",
        "        net.zero_grad()\n",
        "        y_fake_g = net(x_fake, x_fake_labels)\n",
        "        g_loss = gen.loss(y_fake_g, real_ident)\n",
        "        g_loss.backward()\n",
        "        optim_G.step()\n",
        "\n",
        "        g_losses_batch.append(g_loss.item())\n",
        "        d_losses_batch.append(d_loss.item())\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            print('Epoch {} [{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
        "                        epoch, batch_idx, len(trainloaders[0]),\n",
        "                        d_loss.mean().item(),\n",
        "                        g_loss.mean().item()))\n",
        "\n",
        "    # Calcula médias da época\n",
        "    avg_g_loss = epoch_g_loss / num_batches\n",
        "    avg_d_loss = epoch_d_loss / num_batches\n",
        "\n",
        "    # Armazena as médias\n",
        "    g_losses_epoch.append(avg_g_loss)\n",
        "    d_losses_epoch.append(avg_d_loss)\n",
        "\n",
        "\n",
        "    figura = generate_plot(net=net, device=device, round_number=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOWx2wAkXoC6",
        "outputId": "327b9e5e-c1ca-443c-ddee-03d088990efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10/17] loss_D_treino: 0.0000 loss_G_treino: 0.0004\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, batch in enumerate(trainloaders[0]):\n",
        "    if batch_idx % 10 == 0 and batch_idx > 0:\n",
        "            print('[{}/{}] loss_D_treino: {:.4f} loss_G_treino: {:.4f}'.format(\n",
        "                        batch_idx, len(trainloaders[0]),\n",
        "                        d_loss.mean().item(),\n",
        "                        g_loss.mean().item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Treinamento de Geradora única, após clientes treinarem Discriminadoras por todos os dados."
      ],
      "metadata": {
        "id": "wqnZhxxUZaHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdhiGs-HXoC8"
      },
      "outputs": [],
      "source": [
        "g_losses = []  # Perda média do gerador por rodada\n",
        "d_losses = []  # Perda média do discriminador por rodada\n",
        "num_discriminator_epochs = 1  # Épocas de treino do discriminador por rodada\n",
        "num_generator_epochs = 50       # Épocas de treino do gerador por rodada\n",
        "\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "for r in range(50):  # 100 rodadas federadas\n",
        "    # ========================================================================\n",
        "    # Treino dos Discriminadores (clientes)\n",
        "    # ========================================================================\n",
        "    round_d_losses = []  # Armazena as perdas dos discriminadores nesta rodada\n",
        "\n",
        "    for i, (net, trainloader) in enumerate(zip(models, trainloaders)):\n",
        "        net.to(device)\n",
        "        optim_D = optim_Ds[i]\n",
        "\n",
        "        for e in range(num_discriminator_epochs):  # Épocas locais\n",
        "            epoch_d_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            for batch in trainloader:\n",
        "                images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "                batch_size = images.size(0)\n",
        "                real_ident = torch.full((batch_size, 1), 1.0, device=device)\n",
        "                fake_ident = torch.full((batch_size, 1), 0.0, device=device)\n",
        "\n",
        "                # Treino do Discriminador\n",
        "                net.zero_grad()\n",
        "\n",
        "                # Dados reais\n",
        "                y_real = net(images, labels)\n",
        "                d_real_loss = net.loss(y_real, real_ident)\n",
        "\n",
        "                # Dados falsos (gerados)\n",
        "                z_noise = torch.randn(batch_size, 100, device=device)\n",
        "                x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "                x_fake = gen(z_noise, x_fake_labels)  # Usa o gerador global\n",
        "                y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "                d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "\n",
        "                # Loss total e backprop\n",
        "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "                d_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(net.discriminator.parameters(), max_norm=1.0)\n",
        "                optim_D.step()\n",
        "\n",
        "                # Acumula a perda\n",
        "                epoch_d_loss += d_loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            # Média da perda do discriminador nesta época\n",
        "            epoch_d_loss /= num_batches\n",
        "            round_d_losses.append(epoch_d_loss)\n",
        "\n",
        "    # Média da perda dos discriminadores nesta rodada\n",
        "    avg_d_loss = sum(round_d_losses) / len(round_d_losses)\n",
        "    d_losses.append(avg_d_loss)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Treino do Gerador (global)\n",
        "    # ========================================================================\n",
        "    round_g_losses = []  # Armazena as perdas do gerador nesta rodada\n",
        "\n",
        "    for e in range(num_generator_epochs):  # Épocas do gerador\n",
        "        gen.zero_grad()\n",
        "\n",
        "        # Gera dados falsos\n",
        "        z_noise = torch.randn(32, 100, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (32,), device=device)\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "        real_ident = torch.full((32, 1), 1.0, device=device)\n",
        "        y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "        Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "\n",
        "        # Calcula a perda do gerador\n",
        "        y_fake_g = Dmax(x_fake, x_fake_labels)\n",
        "        g_loss = gen.loss(y_fake_g, real_ident)\n",
        "        g_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "        optim_G.step()\n",
        "\n",
        "        # Acumula a perda\n",
        "        round_g_losses.append(g_loss.item())\n",
        "\n",
        "    # Média da perda do gerador nesta rodada\n",
        "    avg_g_loss = sum(round_g_losses) / len(round_g_losses)\n",
        "    g_losses.append(avg_g_loss)\n",
        "\n",
        "    # Gera a figura (opcional)\n",
        "    figura = generate_plot(net=gen, device=device, round_number=r)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Treinamento de Geradora única, após cada batch em discriminadoras"
      ],
      "metadata": {
        "id": "hgwSieOWZxdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "n4jz6hpA2dKW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_chunks = 5\n",
        "client_chunks = []\n",
        "for train_partition in train_partitions:\n",
        "  chunk_size = math.ceil(len(train_partition)/num_chunks)\n",
        "\n",
        "  chunks = []\n",
        "  for i in range(num_chunks):\n",
        "      start = i * chunk_size\n",
        "      end = min((i + 1) * chunk_size, len(train_partition))\n",
        "      chunks.append(Subset(train_partition, range(start, end)))\n",
        "\n",
        "  client_chunks.append(chunks)"
      ],
      "metadata": {
        "id": "zJPR2SUg12H2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_losses_chunk = []\n",
        "d_losses_chunk = []\n",
        "g_losses_round = []\n",
        "d_losses_round = []\n",
        "\n",
        "\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "batch_size_gen = 128\n",
        "batch_tam = 128\n",
        "extra_g_e = 5\n",
        "\n",
        "for epoch in range(50):\n",
        "  g_loss_c = 0.0\n",
        "  d_loss_c = 0.0\n",
        "  total_d_samples = 0  # Amostras totais processadas pelos discriminadores\n",
        "  total_g_samples = 0  # Amostras totais processadas pelo gerador\n",
        "\n",
        "  for chunk_idx in range(num_chunks):\n",
        "    # ====================================================================\n",
        "    # Treino dos Discriminadores (clientes) no bloco atual\n",
        "    # ====================================================================\n",
        "    d_loss_b = 0\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "    for i, (net, chunks) in enumerate(zip(models, client_chunks)):\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size = batch_tam, shuffle=True)\n",
        "\n",
        "      # Treinar o discriminador no bloco\n",
        "      net.to(device)\n",
        "      optim_D = optim_Ds[i]\n",
        "\n",
        "      for batch in chunk_loader:\n",
        "          images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "          batch_size = images.size(0)\n",
        "          real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "          fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "          # Train D\n",
        "          net.zero_grad()\n",
        "\n",
        "          # Dados Reais\n",
        "          y_real = net(images, labels)\n",
        "          d_real_loss = net.loss(y_real, real_ident)\n",
        "\n",
        "          # Dados Falsos\n",
        "          z_noise = torch.randn(batch_size, 100, device=device)\n",
        "          x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "          x_fake = gen(z_noise, x_fake_labels)\n",
        "          y_fake_d = net(x_fake.detach(), x_fake_labels)\n",
        "          d_fake_loss = net.loss(y_fake_d, fake_ident)\n",
        "\n",
        "          # Loss total e backprop\n",
        "          d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "          d_loss.backward()\n",
        "          # torch.nn.utils.clip_grad_norm_(net.discriminator.parameters(), max_norm=1.0)\n",
        "          optim_D.step()\n",
        "          d_loss_b += d_loss.item() * batch_size\n",
        "          total_chunk_samples += 1\n",
        "    # Média da perda dos discriminadores neste chunk\n",
        "    avg_d_loss_chunk = d_loss_b / total_chunk_samples if total_chunk_samples > 0 else 0.0\n",
        "    d_losses_chunk.append(avg_d_loss_chunk)\n",
        "    d_loss_c += avg_d_loss_chunk * total_chunk_samples\n",
        "    total_d_samples += total_chunk_samples\n",
        "\n",
        "    chunk_g_loss = 0.0\n",
        "    for g_epoch in range(extra_g_e):\n",
        "      # Train G\n",
        "      gen.zero_grad()\n",
        "\n",
        "      # Gera dados falsos\n",
        "      z_noise = torch.randn(batch_size_gen, 100, device=device)\n",
        "      x_fake_labels = torch.randint(0, 10, (batch_size_gen,), device=device)\n",
        "      x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "      # Seleciona o melhor discriminador (Dmax)\n",
        "      y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "      y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "      Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "\n",
        "      # Calcula a perda do gerador\n",
        "      real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "      y_fake_g = Dmax(x_fake, x_fake_labels)  # Detach explícito\n",
        "      g_loss = gen.loss(y_fake_g, real_ident)\n",
        "      g_loss.backward()\n",
        "      #torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "      optim_G.step()\n",
        "\n",
        "      chunk_g_loss += g_loss.item()\n",
        "    g_losses_chunk.append(chunk_g_loss /extra_g_e)\n",
        "    g_loss_c += chunk_g_loss /extra_g_e\n",
        "\n",
        "  g_loss_e = g_loss_c/num_chunks\n",
        "  d_loss_e = d_loss_c / total_d_samples if total_d_samples > 0 else 0.0\n",
        "\n",
        "  g_losses_round.append(g_loss_e)\n",
        "  d_losses_round.append(d_loss_e)\n",
        "\n",
        "  figura = generate_plot(net=net, device=device, round_number=epoch)\n",
        "  print(f\"Época {epoch} completa\")"
      ],
      "metadata": {
        "id": "b5QYQs60Zwwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8afa083-4d5e-4a3d-b7d6-03ff46159d6d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 0 completa\n",
            "Época 1 completa\n",
            "Época 2 completa\n",
            "Época 3 completa\n",
            "Época 4 completa\n",
            "Época 5 completa\n",
            "Época 6 completa\n",
            "Época 7 completa\n",
            "Época 8 completa\n",
            "Época 9 completa\n",
            "Época 10 completa\n",
            "Época 11 completa\n",
            "Época 12 completa\n",
            "Época 13 completa\n",
            "Época 14 completa\n",
            "Época 15 completa\n",
            "Época 16 completa\n",
            "Época 17 completa\n",
            "Época 18 completa\n",
            "Época 19 completa\n",
            "Época 20 completa\n",
            "Época 21 completa\n",
            "Época 22 completa\n",
            "Época 23 completa\n",
            "Época 24 completa\n",
            "Época 25 completa\n",
            "Época 26 completa\n",
            "Época 27 completa\n",
            "Época 28 completa\n",
            "Época 29 completa\n",
            "Época 30 completa\n",
            "Época 31 completa\n",
            "Época 32 completa\n",
            "Época 33 completa\n",
            "Época 34 completa\n",
            "Época 35 completa\n",
            "Época 36 completa\n",
            "Época 37 completa\n",
            "Época 38 completa\n",
            "Época 39 completa\n",
            "Época 40 completa\n",
            "Época 41 completa\n",
            "Época 42 completa\n",
            "Época 43 completa\n",
            "Época 44 completa\n",
            "Época 45 completa\n",
            "Época 46 completa\n",
            "Época 47 completa\n",
            "Época 48 completa\n",
            "Época 49 completa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot losses"
      ],
      "metadata": {
        "id": "vBWFA3seZ_-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "38ycqSVjaT4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0_nwm1SXoC9"
      },
      "outputs": [],
      "source": [
        "def loss_graph(g_losses: int, d_losses: int) -> None:\n",
        "    \"\"\"Funcao para gerar grafico de evolucao das perdas da geradora e discriminadora\"\"\"\n",
        "\n",
        "    # Número de iterações/épocas para cada lista\n",
        "    epochs_g = range(len(g_losses))  # Eixo x para o gerador\n",
        "    epochs_d = range(len(d_losses))  # Eixo x para o discriminador\n",
        "\n",
        "    # Criar o gráfico\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs_g, g_losses, label='Generator Loss')\n",
        "    plt.plot(epochs_d, d_losses, label='Discriminator Loss')\n",
        "\n",
        "    # Adicionar título e rótulos aos eixos\n",
        "    plt.title('Generator and Discriminator Losses Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Mostrar o gráfico\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_graph(g_losses=g_losses_epoch, d_losses=d_losses_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "leK6Jt3Manph",
        "outputId": "dbf9d258-2d79-407f-f1e7-6416fec950df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd2fQWWzXoC_"
      },
      "outputs": [],
      "source": [
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "gen.zero_grad()\n",
        "z_noise = torch.randn(128, 100, device=device)\n",
        "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
        "x_fake = gen(z_noise, x_fake_labels)\n",
        "y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "real_ident = torch.full((128, 1), 1., device=device)\n",
        "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "Dmax = models[y_fake_g_means.index(max(y_fake_g_means))]\n",
        "y_fake_g = Dmax(x_fake, x_fake_labels)\n",
        "g_loss = gen.loss(y_fake_g, real_ident)\n",
        "g_loss.backward()\n",
        "optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjrUZkoAXoDD"
      },
      "outputs": [],
      "source": [
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "gen.zero_grad()\n",
        "z_noise = torch.randn(128, 100, device=device)\n",
        "x_fake_labels = torch.randint(0, 10, (128,), device=device)\n",
        "x_fake = gen(z_noise, x_fake_labels)\n",
        "y_fake_gs = [model(x_fake, x_fake_labels) for model in models]\n",
        "real_ident = torch.full((128, 1), 1., device=device)\n",
        "y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "max_idx = y_fake_g_means.index(max(y_fake_g_means))\n",
        "g_loss = gen.loss(y_fake_gs[max_idx], real_ident)\n",
        "g_loss.backward()\n",
        "optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_mj5MokXoDM"
      },
      "outputs": [],
      "source": [
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.common import FitRes, Status, Code, ndarrays_to_parameters, parameters_to_ndarrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDEMtyGKXoDM"
      },
      "outputs": [],
      "source": [
        "params = [[val.cpu().numpy() for _, val in net.state_dict().items()] for net in models]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72r92dCPXoDM"
      },
      "outputs": [],
      "source": [
        "params_converted = [ndarrays_to_parameters(param) for param in params]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is3LfQQLXoDM"
      },
      "outputs": [],
      "source": [
        "results = [(i, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=param, num_examples=len(train_partition), metrics={})) for i, param, train_partition in zip(range(num_partitions), params_converted, train_partitions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxj1CVhoXoDN"
      },
      "outputs": [],
      "source": [
        "from flwr.server.strategy.aggregate import aggregate_inplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvmLuOmPXoDN"
      },
      "outputs": [],
      "source": [
        "aggregated_ndarrays = aggregate_inplace(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-e-StinXoDN"
      },
      "outputs": [],
      "source": [
        "parameters_aggregated_gen = ndarrays_to_parameters(aggregated_ndarrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTzi5zSvXoDP"
      },
      "outputs": [],
      "source": [
        "# Cria uma instância do modelo\n",
        "model = CGAN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wj3iAxHXoDQ"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7ia1SieXoFS",
        "outputId": "bee573da-6d62-4eb0-9314-00c09c3d92f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = next(model.parameters()).device\n",
        "params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
        "state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
        "model.load_state_dict(state_dict, strict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDTkH5S6XoFT"
      },
      "outputs": [],
      "source": [
        "def train_G(net: CGAN, device: str, lr: float, epochs: int, batch_size: int, latent_dim: int):\n",
        "    net.to(device)  # move model to GPU if available\n",
        "    optim_G = torch.optim.Adam(net.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train G\n",
        "        net.zero_grad()\n",
        "        z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "        x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "        x_fake = net(z_noise, x_fake_labels)\n",
        "        y_fake_g = net(x_fake, x_fake_labels)\n",
        "        real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "        g_loss = net.loss(y_fake_g, real_ident)\n",
        "        g_loss.backward()\n",
        "        optim_G.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iwNpLftXoFT"
      },
      "outputs": [],
      "source": [
        "train_G(net=model,\n",
        "        device=device,\n",
        "        lr=0.0001,\n",
        "        epochs=2,\n",
        "        batch_size=128,\n",
        "        latent_dim=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoZsab-tXoFU",
        "outputId": "d9cc408c-5563-4d41-bd07-718c97ff8f99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "flwr.common.typing.Parameters"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(parameters_aggregated_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4knrQHgXoFV"
      },
      "outputs": [],
      "source": [
        "params = [val.cpu().numpy() for _, val in model.state_dict().items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZdf9fw0XoFV"
      },
      "outputs": [],
      "source": [
        "param = ndarrays_to_parameters(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnpXx2j_XoFV",
        "outputId": "dcffe647-d5a4-40ea-90b0-390885be8d28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "flwr.common.typing.Parameters"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(param)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46b96767f5e94b1dab5289f7ca493845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe0dac5b8f4b4b2db3b12745a0ea41bb",
              "IPY_MODEL_4c559859b32d4c8e9ff5bdc179a3dbc2",
              "IPY_MODEL_9fe53a5854df445dad0487dfc9035021"
            ],
            "layout": "IPY_MODEL_7f3112f458a1497bafc6dfa40b3873f6"
          }
        },
        "fe0dac5b8f4b4b2db3b12745a0ea41bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a673e78c8104c17b88cf65efddf4fcd",
            "placeholder": "​",
            "style": "IPY_MODEL_2c7ed38f9c094e99b04b89a4121085f0",
            "value": "README.md: 100%"
          }
        },
        "4c559859b32d4c8e9ff5bdc179a3dbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1964b90585344c1880894924f1d058c5",
            "max": 6971,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79be0691cc304fbc974e2463f055b5ad",
            "value": 6971
          }
        },
        "9fe53a5854df445dad0487dfc9035021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_582503ab5a96473f974e7e68491f5ef2",
            "placeholder": "​",
            "style": "IPY_MODEL_02ebbb4212b44d70b3c2860c8d98a1e1",
            "value": " 6.97k/6.97k [00:00&lt;00:00, 154kB/s]"
          }
        },
        "7f3112f458a1497bafc6dfa40b3873f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a673e78c8104c17b88cf65efddf4fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c7ed38f9c094e99b04b89a4121085f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1964b90585344c1880894924f1d058c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79be0691cc304fbc974e2463f055b5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "582503ab5a96473f974e7e68491f5ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ebbb4212b44d70b3c2860c8d98a1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecd6b36466d7420faa39384fbfff990a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7a1a9f475f9408e991033c95e4c2a7d",
              "IPY_MODEL_928472df19414ba380a724a5d986c5f7",
              "IPY_MODEL_3ede49350efb4a66bb9d9bd9c50d8402"
            ],
            "layout": "IPY_MODEL_f1d0fd160d944dba80868e3024371e1b"
          }
        },
        "d7a1a9f475f9408e991033c95e4c2a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063a8820ba58452d88fdbc48049680cb",
            "placeholder": "​",
            "style": "IPY_MODEL_456b4c7409c54888acd47a0c085b57de",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "928472df19414ba380a724a5d986c5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75abc3232c9a4e71aca3ac1cbc333e65",
            "max": 15561616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28fa1055f85e42d584662c332df28cdb",
            "value": 15561616
          }
        },
        "3ede49350efb4a66bb9d9bd9c50d8402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5fadde8538c40399d0d6175992a2c08",
            "placeholder": "​",
            "style": "IPY_MODEL_33c90b7d0dff423bb7bb6b6fe1bcd9f5",
            "value": " 15.6M/15.6M [00:00&lt;00:00, 22.7MB/s]"
          }
        },
        "f1d0fd160d944dba80868e3024371e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063a8820ba58452d88fdbc48049680cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "456b4c7409c54888acd47a0c085b57de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75abc3232c9a4e71aca3ac1cbc333e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28fa1055f85e42d584662c332df28cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5fadde8538c40399d0d6175992a2c08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33c90b7d0dff423bb7bb6b6fe1bcd9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65cbcde25be4b67ad0972777fba7164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_856b2392293943a0a31ce86e48abf3b4",
              "IPY_MODEL_bf453f0267b841bfbeb8dface06da913",
              "IPY_MODEL_be0c4ae734eb405ab74533eb8ae6d677"
            ],
            "layout": "IPY_MODEL_bfdd786d2c1640728fa4701f3de65680"
          }
        },
        "856b2392293943a0a31ce86e48abf3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07f4993d5ce94b90896882f75058ad4a",
            "placeholder": "​",
            "style": "IPY_MODEL_0c64263c0cbb4d6a8bb03085f87f806b",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "bf453f0267b841bfbeb8dface06da913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5687f3380b0f4371aab7b74c96bf3eae",
            "max": 2595890,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50ba90dce2634ba7bd282722d41eb7d6",
            "value": 2595890
          }
        },
        "be0c4ae734eb405ab74533eb8ae6d677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e4bf9df1c924c8fb2ac5da26778d5f0",
            "placeholder": "​",
            "style": "IPY_MODEL_a8a6950ad39545de9aaab01e502b1c7e",
            "value": " 2.60M/2.60M [00:00&lt;00:00, 38.1MB/s]"
          }
        },
        "bfdd786d2c1640728fa4701f3de65680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f4993d5ce94b90896882f75058ad4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c64263c0cbb4d6a8bb03085f87f806b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5687f3380b0f4371aab7b74c96bf3eae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ba90dce2634ba7bd282722d41eb7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e4bf9df1c924c8fb2ac5da26778d5f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8a6950ad39545de9aaab01e502b1c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f105781904045389b9f80fef1422204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03fe67e2b82e46eda94c5735eb0b3838",
              "IPY_MODEL_0c335ee394e5454688e169253ccc202a",
              "IPY_MODEL_2184aff93e5e4ad4a2bcba4e1b352353"
            ],
            "layout": "IPY_MODEL_d09cd624408c4b12bca2f98fe5cc9e4d"
          }
        },
        "03fe67e2b82e46eda94c5735eb0b3838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38f5264ea38e490ab1730095821d5732",
            "placeholder": "​",
            "style": "IPY_MODEL_b2c2ad47f5714749b1e233d731137a32",
            "value": "Generating train split: 100%"
          }
        },
        "0c335ee394e5454688e169253ccc202a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71d3ef1e998342b6a909d098edbe5840",
            "max": 60000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb53eb56d6814e3495324cd1720e403f",
            "value": 60000
          }
        },
        "2184aff93e5e4ad4a2bcba4e1b352353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bec86a3b96264fa4b742f12438b65c89",
            "placeholder": "​",
            "style": "IPY_MODEL_9ad15303c12b4fed98d8179f75fa6c6d",
            "value": " 60000/60000 [00:01&lt;00:00, 53659.41 examples/s]"
          }
        },
        "d09cd624408c4b12bca2f98fe5cc9e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38f5264ea38e490ab1730095821d5732": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c2ad47f5714749b1e233d731137a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71d3ef1e998342b6a909d098edbe5840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb53eb56d6814e3495324cd1720e403f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bec86a3b96264fa4b742f12438b65c89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad15303c12b4fed98d8179f75fa6c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "748f44fc54004f80b40dc65aa3092679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fac0eb3e353641ce9aab2b90f03a75a1",
              "IPY_MODEL_d8237488423e4f8aab99caecee63f0be",
              "IPY_MODEL_e9655c60cd43415db6a81d0f688b2bf7"
            ],
            "layout": "IPY_MODEL_2047512a3b8343ea9b84080d1355820b"
          }
        },
        "fac0eb3e353641ce9aab2b90f03a75a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1cf09270eb745ea8364b3e44acf920a",
            "placeholder": "​",
            "style": "IPY_MODEL_518ce70cd0104e37a68d62e9b92c6fc0",
            "value": "Generating test split: 100%"
          }
        },
        "d8237488423e4f8aab99caecee63f0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e04c9861094782b045c8eb510f954f",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_317cc826243e41c6baea198d26fc8ea7",
            "value": 10000
          }
        },
        "e9655c60cd43415db6a81d0f688b2bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_006c513f6fd54337835cd1073b081b0a",
            "placeholder": "​",
            "style": "IPY_MODEL_bed1ff516b5e4c33b68312f32fdd7a6f",
            "value": " 10000/10000 [00:00&lt;00:00, 19212.23 examples/s]"
          }
        },
        "2047512a3b8343ea9b84080d1355820b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1cf09270eb745ea8364b3e44acf920a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "518ce70cd0104e37a68d62e9b92c6fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7e04c9861094782b045c8eb510f954f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "317cc826243e41c6baea198d26fc8ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "006c513f6fd54337835cd1073b081b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed1ff516b5e4c33b68312f32fdd7a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}