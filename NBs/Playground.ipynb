{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, seed=None):\n",
    "        if seed is not None:\n",
    "          torch.manual_seed(seed)\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F2U_GAN(nn.Module):\n",
    "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=128, condition=True, seed=None):\n",
    "        if seed is not None:\n",
    "          torch.manual_seed(seed)\n",
    "        super(F2U_GAN, self).__init__()\n",
    "        if dataset == \"mnist\":\n",
    "            self.classes = 10\n",
    "            self.channels = 1\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only MNIST is supported\")\n",
    "\n",
    "        self.condition = condition\n",
    "        self.label_embedding = nn.Embedding(self.classes, self.classes) if condition else None\n",
    "        #self.label_embedding_disc = nn.Embedding(self.classes, self.img_size*self.img_size) if condition else None\n",
    "        self.img_size = img_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
    "        self.input_shape_gen = self.latent_dim + self.label_embedding.embedding_dim if condition else self.latent_dim\n",
    "        self.input_shape_disc = self.channels + self.classes if condition else self.channels\n",
    "\n",
    "        self.adv_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Generator (unchanged) To calculate output shape of convtranspose layers, we can use the formula:\n",
    "        # output_shape = (input_shape - 1) * stride - 2 * padding + kernel_size + output_padding (or dilation * (kernel_size - 1) + 1 inplace of kernel_size if using dilation)\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(self.input_shape_gen, 256 * 7 * 7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Unflatten(1, (256, 7, 7)),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (256,7,7) -> (128,14,14)\n",
    "            nn.BatchNorm2d(128, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # (128,14,14) -> (64,28,28)\n",
    "            nn.BatchNorm2d(64, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, self.channels, kernel_size=3, stride=1, padding=1), # (64,28,28) -> (1,28,28)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Discriminator (corrected) To calculate output shape of conv layers, we can use the formula:\n",
    "        # output_shape = ⌊(input_shape - kernel_size + 2 * padding) / stride + 1⌋ (or (dilation * (kernel_size - 1) - 1) inplace of kernel_size if using dilation)\n",
    "        self.discriminator = nn.Sequential(\n",
    "        # Camada 1: (1,28,28) -> (32,13,13)\n",
    "        nn.utils.spectral_norm(nn.Conv2d(self.input_shape_disc, 32, kernel_size=3, stride=2, padding=0)),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        # Camada 2: (32,14,14) -> (64,7,7)\n",
    "        nn.utils.spectral_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        # Camada 3: (64,7,7) -> (128,3,3)\n",
    "        nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0)),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        # Camada 4: (128,3,3) -> (256,1,1)\n",
    "        nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0)),  # Padding 0 aqui!\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "        # Achata e concatena com as labels\n",
    "        nn.Flatten(), # (256,1,1) -> (256*1*1,)\n",
    "        nn.utils.spectral_norm(nn.Linear(256 * 1 * 1, 1))  # 256 (features)\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels=None):\n",
    "        if input.dim() == 2:\n",
    "            # Generator forward pass (unchanged)\n",
    "            if self.condition:\n",
    "                embedded_labels = self.label_embedding(labels)\n",
    "                gen_input = torch.cat((input, embedded_labels), dim=1)\n",
    "                x = self.generator(gen_input)\n",
    "            else:\n",
    "                x = self.generator(input)\n",
    "            return x.view(-1, *self.img_shape)\n",
    "\n",
    "        elif input.dim() == 4:\n",
    "            # Discriminator forward pass\n",
    "            if self.condition:\n",
    "                embedded_labels = self.label_embedding(labels)\n",
    "                image_labels = embedded_labels.view(embedded_labels.size(0), self.label_embedding.embedding_dim, 1, 1).expand(-1, -1, self.img_size, self.img_size)\n",
    "                x = torch.cat((input, image_labels), dim=1)\n",
    "            else:\n",
    "                x = input\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return self.adv_loss(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = Net()\n",
    "optims = [torch.optim.Adam(global_net.parameters())]\n",
    "gen = F2U_GAN()\n",
    "optim_G = torch.optim.Adam(gen.generator.parameters())\n",
    "models = [F2U_GAN() for _ in range(4)]\n",
    "optim_Ds = [torch.optim.Adam(disc.discriminator.parameters()) for disc in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "            'epoch': 0,  # número da última época concluída\n",
    "            'alvo_state_dict': global_net.state_dict(),\n",
    "            'optimizer_alvo_state_dict': [optim.state_dict() for optim in optims],\n",
    "            'gen_state_dict': gen.state_dict(),\n",
    "            'optim_G_state_dict': optim_G.state_dict(),\n",
    "            'discs_state_dict': [model.state_dict() for model in models],\n",
    "            'optim_Ds_state_dict:': [optim_d.state_dict() for optim_d in optim_Ds]\n",
    "          }\n",
    "checkpoint_file = f\"checkpoint_epoch{000}.pth\"\n",
    "torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = torch.load(\"../Experimentos/Flwr_run/GeraFed_F2U_4c_NIID_Class/checkpoint_epoch1.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net.load_state_dict(check['classifier_state_dict'])\n",
    "\n",
    "gen.load_state_dict(check[\"gen_state_dict\"])\n",
    "optim_G.load_state_dict(check[\"optim_G_state_dict\"])\n",
    "\n",
    "for model, optim_d, state_model, state_optim in zip(models, optim_Ds, check[\"discs_state_dict\"].values(), check[\"optimDs_state_dict\"].values()):\n",
    "    model.load_state_dict(state_model)\n",
    "    optim_d.load_state_dict(state_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_net.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': tensor(29.),\n",
       " 'exp_avg': tensor([-0.0003, -0.0047, -0.0022, -0.0022, -0.0015, -0.0020, -0.0012, -0.0028,\n",
       "          0.0185, -0.0029,  0.0089, -0.0002,  0.0081, -0.0070,  0.0069,  0.0002,\n",
       "         -0.0055, -0.0011, -0.0154,  0.0005,  0.0144,  0.0142,  0.0084,  0.0154,\n",
       "         -0.0352,  0.0064,  0.0239,  0.0103, -0.0247, -0.0012, -0.0229, -0.0190]),\n",
       " 'exp_avg_sq': tensor([5.8063e-08, 7.0552e-07, 2.1241e-06, 1.0153e-06, 1.8042e-07, 9.2494e-08,\n",
       "         5.3453e-07, 1.7549e-06, 4.7928e-06, 3.6051e-07, 1.5378e-06, 1.5724e-07,\n",
       "         1.4774e-06, 3.7966e-06, 1.4055e-06, 1.0009e-07, 1.9653e-06, 1.4337e-06,\n",
       "         5.3053e-06, 1.9597e-07, 2.4811e-06, 3.1176e-06, 2.0234e-06, 3.1287e-06,\n",
       "         1.3793e-05, 6.2629e-07, 5.7887e-06, 4.2533e-06, 5.7951e-06, 5.5780e-07,\n",
       "         7.9965e-06, 3.5455e-06])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_d.state_dict()['state'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0086, -0.0516])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['discriminator.2.bias'][4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1%1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gerafed_env",
   "language": "python",
   "name": "gerafed_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
