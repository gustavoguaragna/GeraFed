{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07f8307e",
      "metadata": {
        "id": "07f8307e"
      },
      "source": [
        "# Inicialização"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f47d1996",
      "metadata": {
        "id": "f47d1996"
      },
      "source": [
        "## Prepara o ambiente local ou colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a7ea1c",
      "metadata": {
        "id": "88a7ea1c"
      },
      "outputs": [],
      "source": [
        "# --- Detectar Ambiente (Colab ou Local) ---\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    # Tenta importar um módulo específico do Colab\n",
        "    from google.colab import drive\n",
        "    import shutil # Usaremos para copiar, se necessário, mas salvar direto é melhor\n",
        "    import os\n",
        "\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        # Crie um diretório específico para salvar os resultados desta execução\n",
        "        save_base_dir = \"/content/drive/MyDrive/GAN_Training_Results\" # Ajuste o caminho como desejar\n",
        "        os.makedirs(save_base_dir, exist_ok=True)\n",
        "        # Opcional: Crie um subdiretório único para esta execução específica (ex: baseado em timestamp)\n",
        "        # import datetime\n",
        "        # timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        # save_dir = os.path.join(save_base_dir, f\"run_{timestamp}\")\n",
        "        # os.makedirs(save_dir, exist_ok=True)\n",
        "        # Por simplicidade, vamos usar o diretório base diretamente por enquanto\n",
        "        save_dir = save_base_dir\n",
        "        print(f\"✅ Google Drive montado. Arquivos serão salvos em: {save_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao montar o Google Drive: {e}\")\n",
        "        print(\"   Downloads diretos serão tentados, mas podem atrasar.\")\n",
        "        save_dir = \".\" # Salvar localmente se o Drive falhar\n",
        "    IN_COLAB = True\n",
        "    print(\"✅ Ambiente Google Colab detectado. Downloads automáticos (a cada 2 épocas) ativados.\")\n",
        "except ImportError:\n",
        "    print(\"✅ Ambiente local detectado. Downloads automáticos desativados.\")\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e68593",
      "metadata": {
        "id": "08e68593"
      },
      "source": [
        "## Importa Pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1e43ad3e",
      "metadata": {
        "id": "1e43ad3e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5df872b",
      "metadata": {
        "id": "b5df872b"
      },
      "source": [
        "## Modelo Classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4395f020",
      "metadata": {
        "id": "4395f020"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*4*4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a98df505",
      "metadata": {
        "id": "a98df505"
      },
      "outputs": [],
      "source": [
        "class Net_Cifar(nn.Module):\n",
        "    def __init__(self,seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8c276e",
      "metadata": {
        "id": "6d8c276e"
      },
      "source": [
        "## Carrega Dados MNIST centralizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a82cbfd1",
      "metadata": {
        "id": "a82cbfd1"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ad4aaf",
      "metadata": {
        "id": "13ad4aaf"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
        "# Create data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainloader_reduzido = DataLoader(trainset_reduzido, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
        "\n",
        "dataset = \"mnist\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64be3f56",
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# Define transform com ToTensor e Normalize para 3 canais\n",
        "transform_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),  # média por canal R,G,B\n",
        "                         (0.5, 0.5, 0.5))  # desvio padrão por canal\n",
        "])\n",
        "\n",
        "# Carrega os datasets de treino e teste\n",
        "trainset_cifar = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_cifar\n",
        ")\n",
        "testset_cifar = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_cifar\n",
        ")\n",
        "\n",
        "# Cria um subset reduzido de treino (por exemplo, 1000 amostras)\n",
        "#trainset_cifar_reduzido = random_split(trainset_cifar, [1000, len(trainset_cifar) - 1000])[0]\n",
        "\n",
        "# DataLoaders\n",
        "trainloader_cifar = DataLoader(\n",
        "    trainset_cifar,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "# trainloader_cifar_reduzido = DataLoader(\n",
        "#     trainset_cifar_reduzido,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=True,\n",
        "#     num_workers=2,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "testloader_cifar = DataLoader(\n",
        "    testset_cifar,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "dataset = \"mnist\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c27cfeac",
      "metadata": {
        "id": "c27cfeac"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# parameters\n",
        "num_classes = 10\n",
        "samples_per_class = 5\n",
        "\n",
        "if dataset == \"cifar\":\n",
        "    class_names = [\n",
        "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "     'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "    ]\n",
        "\n",
        "# containers\n",
        "class_counts = {i: 0 for i in range(num_classes)}\n",
        "class_images = {i: [] for i in range(num_classes)}\n",
        "\n",
        "# gather up to 5 images per class\n",
        "for img, label in trainset:\n",
        "    if class_counts[label] < samples_per_class:\n",
        "        class_images[label].append(img)\n",
        "        class_counts[label] += 1\n",
        "    # stop early once we have enough of every class\n",
        "    if all(count >= samples_per_class for count in class_counts.values()):\n",
        "        break\n",
        "\n",
        "# plot\n",
        "fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(5, 9))\n",
        "for cls in range(num_classes):\n",
        "    for i in range(samples_per_class):\n",
        "        ax = axes[cls, i]\n",
        "        img = class_images[cls][i]\n",
        "        if dataset == \"mnist\":\n",
        "            ax.imshow(img.squeeze(), cmap='gray')\n",
        "        else:\n",
        "            img_denorm = (img * 0.5 + 0.5)  # denormalize for visualization\n",
        "            ax.imshow(img_denorm.permute(1, 2, 0).numpy())\n",
        "        ax.axis('off')\n",
        "    # label the rows on the leftmost subplot\n",
        "   # axes[cls, 0].set_ylabel(str(cls), rotation=0, labelpad=12, va='center', fontsize=12)\n",
        "\n",
        "# Ajustar o layout antes de calcular as posições\n",
        "plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "# Adicionar os rótulos das classes corretamente alinhados\n",
        "fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
        "for row in range(num_classes):\n",
        "    # Obter posição do subplot em coordenadas da figura\n",
        "    bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "    pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "    center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "    # Adicionar o rótulo\n",
        "    fig.text(0.03, center_y, str(row), va='center', fontsize=22, color='black')\n",
        "\n",
        "plt.suptitle(\"Real\", fontsize=30, y=0.99)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f6fbda",
      "metadata": {
        "id": "54f6fbda"
      },
      "source": [
        "## Modelo Generativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ea03ea69",
      "metadata": {
        "id": "ea03ea69"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "314c3604",
      "metadata": {
        "id": "314c3604"
      },
      "source": [
        "### CGAN (simples, mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23cfec37",
      "metadata": {
        "id": "23cfec37"
      },
      "outputs": [],
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(CGAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        if input.dim() == 2:\n",
        "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "            x = self.generator(z)\n",
        "            x = x.view(x.size(0), *self.img_shape) #Em\n",
        "            return x\n",
        "        elif input.dim() == 4:\n",
        "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1a5b73",
      "metadata": {
        "id": "4f1a5b73"
      },
      "source": [
        "### Arquitetura do paper F2U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cbb8292a",
      "metadata": {
        "id": "cbb8292a"
      },
      "outputs": [],
      "source": [
        "class F2U_GAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=128, condition=True, seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super(F2U_GAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only MNIST is supported\")\n",
        "\n",
        "        self.condition = condition\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes) if condition else None\n",
        "        #self.label_embedding_disc = nn.Embedding(self.classes, self.img_size*self.img_size) if condition else None\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.input_shape_gen = self.latent_dim + self.label_embedding.embedding_dim if condition else self.latent_dim\n",
        "        self.input_shape_disc = self.channels + self.classes if condition else self.channels\n",
        "\n",
        "        self.adv_loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Generator (unchanged) To calculate output shape of convtranspose layers, we can use the formula:\n",
        "        # output_shape = (input_shape - 1) * stride - 2 * padding + kernel_size + output_padding (or dilation * (kernel_size - 1) + 1 inplace of kernel_size if using dilation)\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.Linear(self.input_shape_gen, 256 * 7 * 7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (256, 7, 7)),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (256,7,7) -> (128,14,14)\n",
        "            nn.BatchNorm2d(128, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # (128,14,14) -> (64,28,28)\n",
        "            nn.BatchNorm2d(64, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, self.channels, kernel_size=3, stride=1, padding=1), # (64,28,28) -> (1,28,28)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Discriminator (corrected) To calculate output shape of conv layers, we can use the formula:\n",
        "        # output_shape = ⌊(input_shape - kernel_size + 2 * padding) / stride + 1⌋ (or (dilation * (kernel_size - 1) - 1) inplace of kernel_size if using dilation)\n",
        "        self.discriminator = nn.Sequential(\n",
        "        # Camada 1: (1,28,28) -> (32,13,13)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(self.input_shape_disc, 32, kernel_size=3, stride=2, padding=0)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Camada 2: (32,14,14) -> (64,7,7)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Camada 3: (64,7,7) -> (128,3,3)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Camada 4: (128,3,3) -> (256,1,1)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0)),  # Padding 0 aqui!\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Achata e concatena com as labels\n",
        "        nn.Flatten(), # (256,1,1) -> (256*1*1,)\n",
        "        nn.utils.spectral_norm(nn.Linear(256 * 1 * 1, 1))  # 256 (features)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, labels=None):\n",
        "        if input.dim() == 2:\n",
        "            # Generator forward pass (unchanged)\n",
        "            if self.condition:\n",
        "                embedded_labels = self.label_embedding(labels)\n",
        "                gen_input = torch.cat((input, embedded_labels), dim=1)\n",
        "                x = self.generator(gen_input)\n",
        "            else:\n",
        "                x = self.generator(input)\n",
        "            return x.view(-1, *self.img_shape)\n",
        "\n",
        "        elif input.dim() == 4:\n",
        "            # Discriminator forward pass\n",
        "            if self.condition:\n",
        "                embedded_labels = self.label_embedding(labels)\n",
        "                image_labels = embedded_labels.view(embedded_labels.size(0), self.label_embedding.embedding_dim, 1, 1).expand(-1, -1, self.img_size, self.img_size)\n",
        "                x = torch.cat((input, image_labels), dim=1)\n",
        "            else:\n",
        "                x = input\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "decb2888",
      "metadata": {},
      "outputs": [],
      "source": [
        "class F2U_GAN_SlowDisc(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=128, condition=True, seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super(F2U_GAN_SlowDisc, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only MNIST is supported\")\n",
        "\n",
        "        self.condition = condition\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes) if condition else None\n",
        "        #self.label_embedding_disc = nn.Embedding(self.classes, self.img_size*self.img_size) if condition else None\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.input_shape_gen = self.latent_dim + self.label_embedding.embedding_dim if condition else self.latent_dim\n",
        "        self.input_shape_disc = self.channels + self.classes if condition else self.channels\n",
        "\n",
        "        self.adv_loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Generator (unchanged) To calculate output shape of convtranspose layers, we can use the formula:\n",
        "        # output_shape = (input_shape - 1) * stride - 2 * padding + kernel_size + output_padding (or dilation * (kernel_size - 1) + 1 inplace of kernel_size if using dilation)\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.Linear(self.input_shape_gen, 256 * 7 * 7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (256, 7, 7)),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (256,7,7) -> (128,14,14)\n",
        "            nn.BatchNorm2d(128, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # (128,14,14) -> (64,28,28)\n",
        "            nn.BatchNorm2d(64, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, self.channels, kernel_size=3, stride=1, padding=1), # (64,28,28) -> (1,28,28)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Discriminator (corrected) To calculate output shape of conv layers, we can use the formula:\n",
        "        # output_shape = ⌊(input_shape - kernel_size + 2 * padding) / stride + 1⌋ (or (dilation * (kernel_size - 1) - 1) inplace of kernel_size if using dilation)\n",
        "        self.discriminator = nn.Sequential(\n",
        "        # Camada 1: (1,28,28) -> (32,13,13)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(self.input_shape_disc, 32, kernel_size=3, stride=2, padding=0)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout2d(0.3),\n",
        "\n",
        "        # Camada 2: (32,14,14) -> (64,7,7)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout2d(0.3),\n",
        "\n",
        "        # Camada 3: (64,7,7) -> (128,3,3)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout2d(0.3),\n",
        "\n",
        "        # Camada 4: (128,3,3) -> (256,1,1)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0)),  # Padding 0 aqui!\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Achata e concatena com as labels\n",
        "        nn.Flatten(), # (256,1,1) -> (256*1*1,)\n",
        "        nn.utils.spectral_norm(nn.Linear(256 * 1 * 1, 1))  # 256 (features)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, labels=None):\n",
        "        if input.dim() == 2:\n",
        "            # Generator forward pass (unchanged)\n",
        "            if self.condition:\n",
        "                embedded_labels = self.label_embedding(labels)\n",
        "                gen_input = torch.cat((input, embedded_labels), dim=1)\n",
        "                x = self.generator(gen_input)\n",
        "            else:\n",
        "                x = self.generator(input)\n",
        "            return x.view(-1, *self.img_shape)\n",
        "\n",
        "        elif input.dim() == 4:\n",
        "            # Discriminator forward pass\n",
        "            if self.condition:\n",
        "                embedded_labels = self.label_embedding(labels)\n",
        "                image_labels = embedded_labels.view(embedded_labels.size(0), self.label_embedding.embedding_dim, 1, 1).expand(-1, -1, self.img_size, self.img_size)\n",
        "                input = input + torch.randn_like(input) * 0.1\n",
        "                x = torch.cat((input, image_labels), dim=1)\n",
        "            else:\n",
        "                x = input\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c52363e9",
      "metadata": {
        "id": "c52363e9"
      },
      "outputs": [],
      "source": [
        "class F2U_GAN_CIFAR(nn.Module):\n",
        "    def __init__(self, img_size=32, latent_dim=128, condition=True, seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super(F2U_GAN_CIFAR, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.classes = 10\n",
        "        self.channels = 3\n",
        "        self.condition = condition\n",
        "\n",
        "        # Embedding para condicionamento\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes) if self.condition else None\n",
        "\n",
        "        # Shapes de entrada\n",
        "        self.input_shape_gen = self.latent_dim + (self.classes if self.condition else 0)\n",
        "        self.input_shape_disc = self.channels + (self.classes if self.condition else 0)\n",
        "\n",
        "        # -----------------\n",
        "        #  Generator\n",
        "        # -----------------\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.Linear(self.input_shape_gen, 512 * 4 * 4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (512, 4, 4)),                  # → (512,4,4)\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # → (256,8,8)\n",
        "            nn.BatchNorm2d(256, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # → (128,16,16)\n",
        "            nn.BatchNorm2d(128, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128,  64, kernel_size=4, stride=2, padding=1),  # → ( 64,32,32)\n",
        "            nn.BatchNorm2d(64,  momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d( 64,   self.channels, kernel_size=3, stride=1, padding=1),  # → (3,32,32)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # -----------------\n",
        "        #  Discriminator\n",
        "        # -----------------\n",
        "        layers = []\n",
        "        in_ch = self.input_shape_disc\n",
        "        cfg = [\n",
        "            ( 64, 3, 1),  # → spatial stays 32\n",
        "            ( 64, 4, 2),  # → 16\n",
        "            (128, 3, 1),  # → 16\n",
        "            (128, 4, 2),  # → 8\n",
        "            (256, 4, 2),  # → 4\n",
        "        ]\n",
        "        for out_ch, k, s in cfg:\n",
        "            layers += [\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=1)\n",
        "                ),\n",
        "                nn.LeakyReLU(0.1, inplace=True)\n",
        "            ]\n",
        "            in_ch = out_ch\n",
        "\n",
        "        layers += [\n",
        "            nn.Flatten(),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Linear(256 * 4 * 4, 1)\n",
        "            )\n",
        "        ]\n",
        "        self.discriminator = nn.Sequential(*layers)\n",
        "\n",
        "        # adversarial loss\n",
        "        self.adv_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, input, labels=None):\n",
        "        # Generator pass\n",
        "        if input.dim() == 2 and input.size(1) == self.latent_dim:\n",
        "            if self.condition:\n",
        "                if labels is None:\n",
        "                    raise ValueError(\"Labels must be provided for conditional generation\")\n",
        "                embedded = self.label_embedding(labels)\n",
        "                gen_input = torch.cat((input, embedded), dim=1)\n",
        "            else:\n",
        "                gen_input = input\n",
        "            img = self.generator(gen_input)\n",
        "            return img\n",
        "\n",
        "        # Discriminator pass\n",
        "        elif input.dim() == 4 and input.size(1) == self.channels:\n",
        "            x = input\n",
        "            if self.condition:\n",
        "                if labels is None:\n",
        "                    raise ValueError(\"Labels must be provided for conditional discrimination\")\n",
        "                embedded = self.label_embedding(labels)\n",
        "                # criar mapa de labels e concatenar\n",
        "                lbl_map = embedded.view(-1, self.classes, 1, 1).expand(-1, self.classes, self.img_size, self.img_size)\n",
        "                x = torch.cat((x, lbl_map), dim=1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Input shape not recognized\")\n",
        "\n",
        "    def loss(self, logits, targets):\n",
        "        return self.adv_loss(logits.view(-1), targets.float().view(-1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144a24f4",
      "metadata": {
        "id": "144a24f4"
      },
      "source": [
        "## Funções para geração de dataset e imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3487d31",
      "metadata": {
        "id": "b3487d31"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import random # Needed for handling remainders if samples aren't perfectly divisible\n",
        "\n",
        "class GeneratedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 generator,\n",
        "                 num_samples,\n",
        "                 latent_dim=100,\n",
        "                 num_classes=10, # Total classes the generator model knows\n",
        "                 desired_classes=None, # Optional: List of specific class indices to generate\n",
        "                 device=\"cpu\",\n",
        "                 image_col_name=\"image\",\n",
        "                 label_col_name=\"label\"):\n",
        "        \"\"\"\n",
        "        Generates a dataset using a conditional generative model, potentially\n",
        "        focusing on a subset of classes.\n",
        "\n",
        "        Args:\n",
        "            generator: The pre-trained generative model.\n",
        "            num_samples (int): Total number of images to generate across the desired classes.\n",
        "            latent_dim (int): Dimension of the latent space vector (z).\n",
        "            num_classes (int): The total number of classes the generator was trained on.\n",
        "                               This is crucial for correct label conditioning (e.g., one-hot dim).\n",
        "            desired_classes (list[int], optional): A list of integer class indices to generate.\n",
        "                                                  If None or empty, images for all classes\n",
        "                                                  (from 0 to num_classes-1) will be generated,\n",
        "                                                  distributed as evenly as possible.\n",
        "                                                  Defaults to None.\n",
        "            device (str): Device to run generation on ('cpu' or 'cuda').\n",
        "            image_col_name (str): Name for the image column in the output dictionary.\n",
        "            label_col_name (str): Name for the label column in the output dictionary.\n",
        "        \"\"\"\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        # Store the total number of classes the generator understands\n",
        "        self.total_num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.model_type = type(self.generator).__name__ # Get generator class name\n",
        "        self.image_col_name = image_col_name\n",
        "        self.label_col_name = label_col_name\n",
        "\n",
        "        # Determine the actual classes to generate based on desired_classes\n",
        "        if desired_classes is not None and len(desired_classes) > 0:\n",
        "            # Validate that desired classes are within the generator's known range\n",
        "            if not all(0 <= c < self.total_num_classes for c in desired_classes):\n",
        "                raise ValueError(f\"All desired classes must be integers between 0 and {self.total_num_classes - 1}\")\n",
        "            # Use only the unique desired classes, sorted for consistency\n",
        "            self._actual_classes_to_generate = sorted(list(set(desired_classes)))\n",
        "        else:\n",
        "            # If no specific classes desired, generate all classes\n",
        "            self._actual_classes_to_generate = list(range(self.total_num_classes))\n",
        "\n",
        "        # The 'classes' attribute of the dataset reflects only those generated\n",
        "        self.classes = self._actual_classes_to_generate\n",
        "        self.num_generated_classes = len(self.classes) # Number of classes being generated\n",
        "\n",
        "        if self.num_generated_classes == 0 and self.num_samples > 0:\n",
        "             raise ValueError(\"Cannot generate samples with an empty list of desired classes.\")\n",
        "        elif self.num_samples == 0:\n",
        "             print(\"Warning: num_samples is 0. Dataset will be empty.\")\n",
        "             self.images = torch.empty(0) # Adjust shape if known\n",
        "             self.labels = torch.empty(0, dtype=torch.long)\n",
        "        else:\n",
        "             # Generate the data only if needed\n",
        "             self.images, self.labels = self.generate_data()\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        \"\"\"Generates images and corresponding labels for the specified classes.\"\"\"\n",
        "        self.generator.eval()\n",
        "        self.generator.to(self.device)\n",
        "\n",
        "        # --- Create Labels ---\n",
        "        generated_labels_list = []\n",
        "        if self.num_generated_classes > 0:\n",
        "            # Distribute samples as evenly as possible among the desired classes\n",
        "            samples_per_class = self.num_samples // self.num_generated_classes\n",
        "            for cls in self._actual_classes_to_generate:\n",
        "                generated_labels_list.extend([cls] * samples_per_class)\n",
        "\n",
        "            # Handle remaining samples if num_samples is not perfectly divisible\n",
        "            num_remaining = self.num_samples - len(generated_labels_list)\n",
        "            if num_remaining > 0:\n",
        "                # Add remaining samples by randomly choosing from the desired classes\n",
        "                remainder_labels = random.choices(self._actual_classes_to_generate, k=num_remaining)\n",
        "                generated_labels_list.extend(remainder_labels)\n",
        "\n",
        "            # Shuffle labels for better distribution in batches later\n",
        "            random.shuffle(generated_labels_list)\n",
        "\n",
        "        # Convert labels list to tensor\n",
        "        labels = torch.tensor(generated_labels_list, dtype=torch.long, device=self.device)\n",
        "\n",
        "        # Double check label count (should match num_samples due to logic above)\n",
        "        if len(labels) != self.num_samples:\n",
        "             # This indicates an unexpected issue, potentially if num_generated_classes was 0 initially\n",
        "             # but num_samples > 0. Raise error or adjust. Let's adjust defensively.\n",
        "             print(f\"Warning: Label count mismatch. Expected {self.num_samples}, got {len(labels)}. Adjusting size.\")\n",
        "             if len(labels) > self.num_samples:\n",
        "                 labels = labels[:self.num_samples]\n",
        "             else:\n",
        "                 # Pad if too few (less likely with current logic unless num_generated_classes=0)\n",
        "                 num_needed = self.num_samples - len(labels)\n",
        "                 if self.num_generated_classes > 0:\n",
        "                      padding = torch.tensor(random.choices(self._actual_classes_to_generate, k=num_needed), dtype=torch.long, device=self.device)\n",
        "                      labels = torch.cat((labels, padding))\n",
        "                 # If no classes to generate from, labels tensor might remain smaller\n",
        "\n",
        "        # --- Create Latent Noise ---\n",
        "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "\n",
        "        # --- Generate Images in Batches ---\n",
        "        generated_images_list = []\n",
        "        # Consider making batch_size configurable\n",
        "        batch_size = min(1024, self.num_samples) if self.num_samples > 0 else 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, self.num_samples, batch_size):\n",
        "                z_batch = z[i : min(i + batch_size, self.num_samples)]\n",
        "                labels_batch = labels[i : min(i + batch_size, self.num_samples)]\n",
        "\n",
        "                # Skip if batch is empty (can happen if num_samples = 0)\n",
        "                if z_batch.shape[0] == 0:\n",
        "                    continue\n",
        "\n",
        "                # --- Condition the generator based on its type ---\n",
        "                if self.model_type == 'Generator': # Assumes input: concat(z, one_hot_label)\n",
        "                    # One-hot encode labels using the TOTAL number of classes the generator knows\n",
        "                    labels_one_hot_batch = F.one_hot(labels_batch, num_classes=self.total_num_classes).float()\n",
        "                    generator_input = torch.cat([z_batch, labels_one_hot_batch], dim=1)\n",
        "                    gen_imgs = self.generator(generator_input)\n",
        "                elif self.model_type in ('CGAN', 'F2U_GAN', 'F2U_GAN_CIFAR'): # Assumes input: z, label_index\n",
        "                    gen_imgs = self.generator(z_batch, labels_batch)\n",
        "                else:\n",
        "                    # Handle other potential generator architectures or raise an error\n",
        "                    raise NotImplementedError(f\"Generation logic not defined for model type: {self.model_type}\")\n",
        "\n",
        "                generated_images_list.append(gen_imgs.cpu()) # Move generated images to CPU\n",
        "\n",
        "        self.generator.cpu() # Move generator back to CPU after generation\n",
        "\n",
        "        # Concatenate all generated image batches\n",
        "        if generated_images_list:\n",
        "            all_gen_imgs = torch.cat(generated_images_list, dim=0)\n",
        "        else:\n",
        "            # If no images were generated (e.g., num_samples = 0)\n",
        "            # Create an empty tensor. Shape needs care - determine from generator or use placeholder.\n",
        "            # Let's attempt a placeholder [0, C, H, W] - requires knowing C, H, W.\n",
        "            # For now, a simple empty tensor. User might need to handle this downstream.\n",
        "            print(\"Warning: No images generated. Returning empty tensor for images.\")\n",
        "            all_gen_imgs = torch.empty(0)\n",
        "\n",
        "        return all_gen_imgs, labels.cpu() # Return images and labels (on CPU)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the actual number of samples generated\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(\"Dataset index out of range\")\n",
        "        return {\n",
        "            self.image_col_name: self.images[idx],\n",
        "            self.label_col_name: int(self.labels[idx]) # Return label as standard Python int\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c3d341",
      "metadata": {
        "id": "b1c3d341"
      },
      "outputs": [],
      "source": [
        "class UnconditionalGeneratedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 generator,\n",
        "                 num_samples,\n",
        "                 latent_dim=128,\n",
        "                 device=\"cpu\",\n",
        "                 image_col_name=\"image\"):\n",
        "        \"\"\"\n",
        "        Generates a dataset using an unconditional generative model.\n",
        "\n",
        "        Args:\n",
        "            generator: The pre-trained unconditional generative model.\n",
        "            num_samples (int): Total number of images to generate.\n",
        "            latent_dim (int): Dimension of the latent space vector (z).\n",
        "            device (str): Device to run generation on ('cpu' or 'cuda').\n",
        "            image_col_name (str): Name for the image column in the output dictionary.\n",
        "        \"\"\"\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        self.device = device\n",
        "        self.image_col_name = image_col_name\n",
        "\n",
        "        if self.num_samples < 0:\n",
        "            raise ValueError(\"num_samples must be non-negative\")\n",
        "        elif self.num_samples == 0:\n",
        "            print(\"Warning: num_samples is 0. Dataset will be empty.\")\n",
        "            self.images = torch.empty(0)\n",
        "        else:\n",
        "            self.images = self._generate_images()\n",
        "\n",
        "    def _generate_images(self):\n",
        "        self.generator.eval()\n",
        "        self.generator.to(self.device)\n",
        "\n",
        "        # Create latent noise\n",
        "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "\n",
        "        # Generate images in batches\n",
        "        generated_images = []\n",
        "        batch_size = min(1024, self.num_samples)\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, self.num_samples, batch_size):\n",
        "                z_batch = z[i : min(i + batch_size, self.num_samples)]\n",
        "                gen_imgs = self.generator(z_batch)\n",
        "                generated_images.append(gen_imgs.cpu())\n",
        "\n",
        "        self.generator.cpu()\n",
        "        return torch.cat(generated_images, dim=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(\"Dataset index out of range\")\n",
        "        return { self.image_col_name: self.images[idx] }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9452cf54",
      "metadata": {
        "id": "9452cf54"
      },
      "outputs": [],
      "source": [
        "def generate_plot(net, device, round_number, client_id = None, examples_per_class: int=5, classes: int=10, latent_dim: int=100):\n",
        "    \"\"\"Gera plot de imagens de cada classe\"\"\"\n",
        "\n",
        "    net_type = type(net).__name__\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    batch_size = examples_per_class * classes\n",
        "    dataset = \"mnist\" if  not net_type == \"F2U_GAN_CIFAR\" else \"cifar10\"\n",
        "\n",
        "    latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
        "    labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if net_type == \"Generator\":\n",
        "            labels_one_hot = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            generated_images = net(torch.cat([latent_vectors, labels_one_hot], dim=1))\n",
        "        else:\n",
        "            generated_images = net(latent_vectors, labels)\n",
        "\n",
        "    # Criar uma figura com 10 linhas e 5 colunas de subplots\n",
        "    fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
        "\n",
        "    # Adiciona título no topo da figura\n",
        "    if isinstance(client_id, int):\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
        "    else:\n",
        "        fig.text(0.5, 0.96, f\"Epoch: {round_number}\", ha=\"center\", fontsize=30)\n",
        "\n",
        "    # Exibir as imagens nos subplots\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if dataset == \"mnist\":\n",
        "            ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
        "        else:\n",
        "            images = (generated_images[i] + 1)/2\n",
        "            ax.imshow(images.permute(1, 2, 0).clamp(0,1))\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    # Ajustar o layout antes de calcular as posições\n",
        "    plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "    # Reduzir espaço entre colunas\n",
        "    # plt.subplots_adjust(wspace=0.05)\n",
        "\n",
        "    # Adicionar os rótulos das classes corretamente alinhados\n",
        "    fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
        "    for row in range(classes):\n",
        "        # Obter posição do subplot em coordenadas da figura\n",
        "        bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "        pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "        center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "        # Adicionar o rótulo\n",
        "        fig.text(0.03, center_y, str(row), va='center', fontsize=22, color='black')\n",
        "\n",
        "    IN_COLAB = False\n",
        "    try:\n",
        "        # Tenta importar um módulo específico do Colab\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if IN_COLAB:\n",
        "        if isinstance(client_id, int):\n",
        "            fig.savefig(os.path.join(save_dir, f\"{dataset}_{net_type}_r{round_number}_c{client_id}.png\"))\n",
        "            print(\"Imagem do cliente salva no drive\")\n",
        "        else:\n",
        "            fig.savefig(os.path.join(save_dir, f\"{dataset}{net_type}_r{round_number}.png\"))\n",
        "            print(\"Imagem do servidor salva no drive\")\n",
        "    else:\n",
        "        if isinstance(client_id, int):\n",
        "            fig.savefig(f\"{dataset}{net_type}_r{round_number}_c{client_id}.png\")\n",
        "            print(\"Imagem do cliente salva\")\n",
        "        else:\n",
        "            fig.savefig(f\"{dataset}{net_type}_r{round_number}.png\")\n",
        "            print(\"Imagem do servidor salva\")\n",
        "    plt.close(fig)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a88e7c6",
      "metadata": {
        "id": "3a88e7c6"
      },
      "outputs": [],
      "source": [
        "def plot_unconditional_generated(\n",
        "        generator,\n",
        "        device,\n",
        "        total_samples,\n",
        "        samples_per_row=5,\n",
        "        latent_dim=100,\n",
        "        save_path=None,\n",
        "        round_number=None):\n",
        "    \"\"\"\n",
        "    Generates and plots images from an unconditional generator in a grid.\n",
        "\n",
        "    Args:\n",
        "        generator: The unconditional torch generator model (z -> image).\n",
        "        device: Device to run generation on ('cpu' or 'cuda').\n",
        "        total_samples (int): Number of images to generate.\n",
        "        samples_per_row (int): Number of images per row in the grid.\n",
        "        latent_dim (int): Dimension of latent vector.\n",
        "        save_path (str, optional): Filepath to save the figure. If None, just shows plot.\n",
        "    \"\"\"\n",
        "\n",
        "    generator.eval()\n",
        "    generator.to(device)\n",
        "\n",
        "    # Sample latent vectors\n",
        "    z = torch.randn(total_samples, latent_dim, device=device)\n",
        "    with torch.no_grad():\n",
        "        imgs = generator(z)\n",
        "\n",
        "    # Determine grid size\n",
        "    cols = samples_per_row\n",
        "    rows = math.ceil(total_samples / cols)\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols-2*cols/(rows+cols), rows-1*rows/(rows+cols)))\n",
        "    axes = axes.flatten() if total_samples > 1 else [axes]\n",
        "\n",
        "    fig.text(0.5, 0.99, f\"Round: {round_number}\", ha=\"center\", fontsize=11)\n",
        "\n",
        "    for idx in range(rows * cols):\n",
        "        ax = axes[idx]\n",
        "        ax.axis('off')\n",
        "        if idx < total_samples:\n",
        "            img = imgs[idx]\n",
        "            # Assume (C, H, W) and single-channel\n",
        "            ax.imshow(img[0], cmap='gray')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        fig.savefig(save_path)\n",
        "        print(f\"Figure saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd1a6ff",
      "metadata": {
        "id": "5bd1a6ff"
      },
      "source": [
        "## Importa Pacotes Federado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7c02c2",
      "metadata": {
        "id": "3e7c02c2"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !pip install flwr_datasets\n",
        "    !pip install flwr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9793cd9a",
      "metadata": {
        "id": "9793cd9a"
      },
      "outputs": [],
      "source": [
        "from flwr_datasets.partitioner import DirichletPartitioner, IidPartitioner\n",
        "from flwr_datasets.visualization import plot_label_distributions\n",
        "from flwr_datasets import FederatedDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ac2e55",
      "metadata": {
        "id": "a5ac2e55"
      },
      "source": [
        "## Particionador por classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acd60f5",
      "metadata": {
        "id": "3acd60f5"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Flower Labs GmbH. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Class-based partitioner for Hugging Face Datasets.\"\"\"\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from typing import Optional, List\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from flwr_datasets.partitioner.partitioner import Partitioner  # Assuming this is in the package structure\n",
        "\n",
        "\n",
        "class ClassPartitioner(Partitioner):\n",
        "    \"\"\"Partitions a dataset by class, ensuring each class appears in exactly one partition.\n",
        "\n",
        "    Attributes:\n",
        "        num_partitions (int): Total number of partitions to create\n",
        "        seed (int, optional): Random seed for reproducibility\n",
        "        label_column (str): Name of the column containing class labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_partitions: int,\n",
        "        seed: Optional[int] = None,\n",
        "        label_column: str = \"label\"\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self._num_partitions = num_partitions\n",
        "        self._seed = seed\n",
        "        self._label_column = label_column\n",
        "        self._partition_indices: Optional[List[List[int]]] = None\n",
        "\n",
        "    def _create_partitions(self) -> None:\n",
        "        \"\"\"Create class-based partitions and store indices.\"\"\"\n",
        "        # Extract labels from dataset\n",
        "        labels = self.dataset[self._label_column]\n",
        "\n",
        "        # Group indices by class\n",
        "        class_indices = defaultdict(list)\n",
        "        for idx, label in enumerate(labels):\n",
        "            class_indices[label].append(idx)\n",
        "\n",
        "        classes = list(class_indices.keys())\n",
        "        num_classes = len(classes)\n",
        "\n",
        "        # Validate number of partitions\n",
        "        if self._num_partitions > num_classes:\n",
        "            raise ValueError(\n",
        "                f\"Cannot create {self._num_partitions} partitions with only {num_classes} classes. \"\n",
        "                f\"Reduce partitions to ≤ {num_classes}.\"\n",
        "            )\n",
        "\n",
        "        # Shuffle classes for random distribution\n",
        "        rng = random.Random(self._seed)\n",
        "        rng.shuffle(classes)\n",
        "\n",
        "        # Split classes into partitions\n",
        "        partition_classes = np.array_split(classes, self._num_partitions)\n",
        "\n",
        "        # Create index lists for each partition\n",
        "        self._partition_indices = []\n",
        "        for class_group in partition_classes:\n",
        "            indices = []\n",
        "            for cls in class_group:\n",
        "                indices.extend(class_indices[cls])\n",
        "            self._partition_indices.append(indices)\n",
        "\n",
        "    @property\n",
        "    def dataset(self) -> Dataset:\n",
        "        return super().dataset\n",
        "\n",
        "    @dataset.setter\n",
        "    def dataset(self, value: Dataset) -> None:\n",
        "        # Use parent setter for basic validation\n",
        "        super(ClassPartitioner, ClassPartitioner).dataset.fset(self, value)\n",
        "\n",
        "        # Create partitions once dataset is set\n",
        "        self._create_partitions()\n",
        "\n",
        "    def load_partition(self, partition_id: int) -> Dataset:\n",
        "        \"\"\"Load a partition containing exclusive classes.\n",
        "\n",
        "        Args:\n",
        "            partition_id: The ID of the partition to load (0-based index)\n",
        "\n",
        "        Returns:\n",
        "            Dataset: Subset of the dataset containing only the specified partition's data\n",
        "        \"\"\"\n",
        "        if not self.is_dataset_assigned():\n",
        "            raise RuntimeError(\"Dataset must be assigned before loading partitions\")\n",
        "        if partition_id < 0 or partition_id >= self.num_partitions:\n",
        "            raise ValueError(f\"Invalid partition ID: {partition_id}\")\n",
        "\n",
        "        return self.dataset.select(self._partition_indices[partition_id])\n",
        "\n",
        "    @property\n",
        "    def num_partitions(self) -> int:\n",
        "        return self._num_partitions\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f\"ClassPartitioner(num_partitions={self._num_partitions}, \"\n",
        "                f\"seed={self._seed}, label_column='{self._label_column}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ee55f5",
      "metadata": {
        "id": "a1ee55f5"
      },
      "source": [
        "## Carrega e divide dados entre clientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fdba10d",
      "metadata": {
        "id": "6fdba10d"
      },
      "outputs": [],
      "source": [
        "num_partitions = 4\n",
        "alpha_dir = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd9c472",
      "metadata": {
        "id": "6cd9c472"
      },
      "source": [
        "Rodar somente o particionador desejado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42644935",
      "metadata": {
        "id": "42644935"
      },
      "outputs": [],
      "source": [
        "partitioner = ClassPartitioner(num_partitions=num_partitions, seed=42, label_column=\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312626b5",
      "metadata": {
        "id": "312626b5"
      },
      "outputs": [],
      "source": [
        "partitioner = DirichletPartitioner(\n",
        "    num_partitions=num_partitions,\n",
        "    partition_by=\"label\",\n",
        "    alpha=alpha_dir,\n",
        "    min_partition_size=0,\n",
        "    self_balancing=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8582a179",
      "metadata": {
        "id": "8582a179"
      },
      "outputs": [],
      "source": [
        "partitioner = IidPartitioner(\n",
        "    num_partitions=num_partitions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f9a435",
      "metadata": {
        "id": "a3f9a435"
      },
      "outputs": [],
      "source": [
        "fds = FederatedDataset(\n",
        "    dataset=\"mnist\",\n",
        "    partitioners={\"train\": partitioner}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d21553c",
      "metadata": {
        "id": "7d21553c"
      },
      "outputs": [],
      "source": [
        "fds = FederatedDataset(\n",
        "    dataset=\"cifar10\",\n",
        "    partitioners={\"train\": partitioner}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8abf0ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.ticker import FuncFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026da6c7",
      "metadata": {
        "id": "026da6c7"
      },
      "outputs": [],
      "source": [
        "partitioner = fds.partitioners[\"train\"]\n",
        "figure, axis, dataframe = plot_label_distributions(\n",
        "    partitioner=partitioner,\n",
        "    label_name=\"label\",\n",
        "    title=\"Dir01\",\n",
        "    legend=False,\n",
        "    verbose_labels=True,\n",
        "    size_unit=\"absolute\",\n",
        "    partition_id_axis=\"x\",\n",
        "    legend_kwargs={'fontsize': 10, 'title_fontsize': 10},\n",
        "    figsize=(6, 5)\n",
        ")\n",
        "\n",
        "axis.title.set_fontsize(18)\n",
        "\n",
        "# 2. Modify the returned 'axis' object for labels and ticks\n",
        "# Set font size for the axis titles (e.g., \"Partition ID\", \"Count\")\n",
        "axis.xaxis.label.set_fontsize(18)\n",
        "axis.yaxis.label.set_fontsize(18)\n",
        "\n",
        "axis.yaxis.set_major_formatter(FuncFormatter(lambda y, _: int(y/1000)))\n",
        "#axis.set_ylabel(\"Count (x$10^3$)\", fontsize=16)\n",
        "\n",
        "axis.set_yticks([0, 5000, 10000, 15000, 20000])\n",
        "\n",
        "axis.set_ylabel(\"Count (x$10^3$)\", fontsize=18)\n",
        "\n",
        "# Set font size for the tick numbers on both axes\n",
        "axis.tick_params(axis='both', labelsize=20)\n",
        "\n",
        "# # 3. Adjust layout and show the final plot\n",
        "# figure.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ff7171",
      "metadata": {
        "id": "93ff7171"
      },
      "outputs": [],
      "source": [
        "train_partitions = [fds.load_partition(i, split=\"train\") for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j-L_He1Qvz0e",
      "metadata": {
        "id": "j-L_He1Qvz0e"
      },
      "source": [
        "Rodar proxima celula somente se quiser testar com dataset reduzido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w2n6dMxcvx2k",
      "metadata": {
        "id": "w2n6dMxcvx2k"
      },
      "outputs": [],
      "source": [
        "# num_samples = [int(len(train_partition)/10) for train_partition in train_partitions]\n",
        "# train_partitions = [train_partition.select(range(n)) for train_partition, n in zip(train_partitions, num_samples)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b7cf8f",
      "metadata": {
        "id": "d1b7cf8f"
      },
      "source": [
        "Cria dicionario de label para cliente para controle do dmax_mismatch. Tive que colocar aqui antes do apply_transform para não dar erro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07fdcb9",
      "metadata": {
        "id": "e07fdcb9"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b106654b",
      "metadata": {
        "id": "b106654b"
      },
      "outputs": [],
      "source": [
        "min_lbl_count = 0.05\n",
        "class_labels = train_partitions[0].info.features[\"label\"]\n",
        "labels_str = class_labels.names\n",
        "label_to_client = {lbl: [] for lbl in labels_str}\n",
        "for idx, ds in enumerate(train_partitions):\n",
        "    counts = Counter(ds['label'])\n",
        "    for label, cnt in counts.items():\n",
        "        if cnt / len(ds) >= min_lbl_count:\n",
        "            label_to_client[class_labels.int2str(label)].append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56dcbb9b",
      "metadata": {
        "id": "56dcbb9b"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df48b029",
      "metadata": {
        "id": "df48b029"
      },
      "outputs": [],
      "source": [
        "pytorch_transforms = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbbc667a",
      "metadata": {
        "id": "cbbc667a"
      },
      "outputs": [],
      "source": [
        "# Para CIFAR-10: 3 canais, normalização média=0.5 e std=0.5\n",
        "pytorch_transforms = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    # batch[\"image\"] é uma lista de PIL.Image ou tensores em H×W×C\n",
        "    # aplicamos o mesmo transform a cada imagem e depois empilhamos\n",
        "    batch[\"img\"] = torch.stack([pytorch_transforms(img) for img in batch[\"img\"]])\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34de09aa",
      "metadata": {
        "id": "34de09aa"
      },
      "outputs": [],
      "source": [
        "train_partitions = [train_partition.with_transform(apply_transforms) for train_partition in train_partitions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MwJAQ213fi-w",
      "metadata": {
        "id": "MwJAQ213fi-w"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CuBEfRZ6fX8i",
      "metadata": {
        "id": "CuBEfRZ6fX8i"
      },
      "outputs": [],
      "source": [
        "test_frac = 0.2\n",
        "client_datasets = []\n",
        "\n",
        "for train_part in train_partitions:\n",
        "    total     = len(train_part)\n",
        "    test_size = int(total * test_frac)\n",
        "    train_size = total - test_size\n",
        "\n",
        "    client_train, client_test = random_split(\n",
        "        train_part,\n",
        "        [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42),\n",
        "    )\n",
        "\n",
        "    client_datasets.append({\n",
        "        \"train\": client_train,\n",
        "        \"test\":  client_test,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ae0635",
      "metadata": {
        "id": "b0ae0635"
      },
      "source": [
        "## Inicializa modelos e otimizadores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b818e92",
      "metadata": {
        "id": "1b818e92"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5be9636",
      "metadata": {
        "id": "b5be9636"
      },
      "source": [
        "Rodar somente o modelo desejado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e76e482",
      "metadata": {
        "id": "8e76e482"
      },
      "outputs": [],
      "source": [
        "models = [CGAN() for i in range(num_partitions)]\n",
        "gen = CGAN().to(device)\n",
        "optim_G = torch.optim.Adam(list(gen.generator.parameters())+list(gen.label_embedding.parameters()), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70debb0f",
      "metadata": {
        "id": "70debb0f"
      },
      "outputs": [],
      "source": [
        "models = [F2U_GAN(condition=True, seed=42) for i in range(num_partitions)]\n",
        "gen = F2U_GAN(condition=True, seed=42).to(device)\n",
        "optim_G = torch.optim.Adam(list(gen.generator.parameters())+list(gen.label_embedding.parameters()), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52f5343",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [F2U_GAN_SlowDisc(condition=True, seed=42) for i in range(num_partitions)]\n",
        "gen = F2U_GAN(condition=True, seed=42).to(device)\n",
        "optim_G = torch.optim.Adam(list(gen.generator.parameters())+list(gen.label_embedding.parameters()), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a813dfe",
      "metadata": {
        "id": "9a813dfe"
      },
      "outputs": [],
      "source": [
        "models = [F2U_GAN_CIFAR(condition=True, seed=42) for i in range(num_partitions)]\n",
        "gen = F2U_GAN_CIFAR(condition=True, seed=42).to(device)\n",
        "optim_G = torch.optim.Adam(list(gen.generator.parameters())+list(gen.label_embedding.parameters()), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f37272",
      "metadata": {
        "id": "70f37272"
      },
      "outputs": [],
      "source": [
        "optim_Ds = [\n",
        "    torch.optim.Adam(list(model.discriminator.parameters())+list(model.label_embedding.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "# scheduler_D = torch.optim.lr_scheduler.StepLR(optim_D, step_size=5, gamma=0.9)\n",
        "# scheduler_G = torch.optim.lr_scheduler.StepLR(optim_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e08c26a0",
      "metadata": {
        "id": "e08c26a0"
      },
      "source": [
        "Inicializa lambda para F2A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893b45d4",
      "metadata": {
        "id": "893b45d4"
      },
      "outputs": [],
      "source": [
        "# initial λ* (unconstrained), wrap with ReLU to keep λ ≥ 0\n",
        "lambda_star = nn.Parameter(torch.tensor(0.1, device=device))\n",
        "relu = nn.ReLU()\n",
        "\n",
        "beta = 0.1  # same β as in the paper\n",
        "\n",
        "# now make your generator optimizer also update lambda_star\n",
        "# (so its gradient from the βλ² term can flow)\n",
        "optim_G = torch.optim.Adam(\n",
        "    list(gen.parameters()) + [lambda_star],\n",
        "    lr=2e-4, betas=(0.5, 0.999)\n",
        ") #ACHO QUE TA ERRADO AQUI, OPTIM_G PEGANDO TODOS PARAMETROS, NAO QUE VAI MUDAR ALGO POIS FAÇO INSTANCIACOES DIFERENTES PARA GE E DISC\n",
        "# optim_G = torch.optim.Adam(\n",
        "#     list(gen.generator.parameters())+list(gen.label_embedding.parameters()) + [lambda_star],\n",
        "#     lr=2e-4, betas=(0.5, 0.999)\n",
        "# ) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3a7e9e",
      "metadata": {
        "id": "7a3a7e9e"
      },
      "source": [
        "# Treinamento dos modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b38a6962",
      "metadata": {
        "id": "b38a6962"
      },
      "source": [
        "## Cria chunks para o treinamento alternado entre discriminadora e geradora ser mais constante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b5d8adc",
      "metadata": {
        "id": "9b5d8adc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2505ab00",
      "metadata": {
        "id": "2505ab00"
      },
      "source": [
        "Quanto menos chunks, mais dados em cada chunk e mais dados são treinados na discriminadora antes de treinar a geradora. No paper do F2U, não está claro como os treinamentos são alternados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FTvOVoSLVpta",
      "metadata": {
        "id": "FTvOVoSLVpta"
      },
      "outputs": [],
      "source": [
        "# prompt: set each train partition as the only first minimum lenght of the partitions samples, the partitions have same lenght\n",
        "\n",
        "min_len = min(len(p) for p in train_partitions)\n",
        "train_partitions = [p.select(range(min_len)) for p in train_partitions]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CVz-ThoCVfoh",
      "metadata": {
        "id": "CVz-ThoCVfoh"
      },
      "outputs": [],
      "source": [
        "for train_partition in train_partitions:\n",
        "  print(len(train_partition))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff747284",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_chunks = 100\n",
        "seed = 42  # escolha qualquer inteiro para reprodutibilidade\n",
        "client_chunks = []\n",
        "\n",
        "for train_partition in client_datasets:\n",
        "    dataset = train_partition[\"train\"]\n",
        "    n = len(dataset)\n",
        "\n",
        "    # 1) embaralha os índices com seed fixa\n",
        "    indices = list(range(n))\n",
        "    random.seed(seed)\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    # 2) calcula tamanho aproximado de cada chunk\n",
        "    chunk_size = math.ceil(n / num_chunks)\n",
        "\n",
        "    # 3) divide em chunks usando fatias dos índices embaralhados\n",
        "    chunks = []\n",
        "    for i in range(num_chunks):\n",
        "        start = i * chunk_size\n",
        "        end = min((i + 1) * chunk_size, n)\n",
        "        chunk_indices = indices[start:end]\n",
        "        chunks.append(Subset(dataset, chunk_indices))\n",
        "\n",
        "    client_chunks.append(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o6WJTKp6B5vD",
      "metadata": {
        "id": "o6WJTKp6B5vD"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "client_test_loaders = [DataLoader(dataset=ds[\"test\"], batch_size=batch_size, shuffle=True) for ds in client_datasets]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ab8d65e",
      "metadata": {
        "id": "4ab8d65e"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e70bd3c8",
      "metadata": {
        "id": "e70bd3c8"
      },
      "outputs": [],
      "source": [
        "nets = [Net(42).to(device) for _ in range(num_partitions)]\n",
        "optims = [torch.optim.Adam(net.parameters(), lr=0.01) for net in nets]\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc668cf",
      "metadata": {
        "id": "bbc668cf"
      },
      "outputs": [],
      "source": [
        "testpartition = fds.load_split(\"test\")\n",
        "testpartition = testpartition.with_transform(apply_transforms)\n",
        "testloader = DataLoader(testpartition, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e86ba4bc",
      "metadata": {
        "id": "e86ba4bc"
      },
      "source": [
        "Carregar modelo pré-treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3c145b",
      "metadata": {
        "id": "3d3c145b"
      },
      "outputs": [],
      "source": [
        "global_net = Net(42).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b66f9a29",
      "metadata": {
        "id": "b66f9a29"
      },
      "outputs": [],
      "source": [
        "checkpoint_loaded = torch.load(\"../Experimentos/NB_F2U/GeraFed_4c_NIIDClass/MNIST/checkpoint_epoch100.pth\")\n",
        "\n",
        "global_net.load_state_dict(checkpoint_loaded['alvo_state_dict'])\n",
        "global_net.to(device)\n",
        "for optim, state in zip(optims, checkpoint_loaded['optimizer_alvo_state_dict']):\n",
        "    optim.load_state_dict(state)\n",
        "\n",
        "gen.load_state_dict(checkpoint_loaded[\"gen_state_dict\"])\n",
        "gen.to(device)\n",
        "optim_G.load_state_dict(checkpoint_loaded[\"optim_G_state_dict\"])\n",
        "\n",
        "for model, optim_d, state_model, state_optim in zip(models, optim_Ds, checkpoint_loaded[\"discs_state_dict\"], checkpoint_loaded[\"optim_Ds_state_dict:\"]):\n",
        "    model.load_state_dict(state_model)\n",
        "    model.to(device)\n",
        "    optim_d.load_state_dict(state_optim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1265b7",
      "metadata": {
        "id": "aa1265b7"
      },
      "source": [
        "Não esquecer de reinicializar os modelos e otimizadores se for reinicializar o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d116bedf",
      "metadata": {
        "id": "d116bedf"
      },
      "outputs": [],
      "source": [
        "from flwr.server.strategy.aggregate import aggregate_inplace\n",
        "from flwr.common import FitRes, Status, Code, ndarrays_to_parameters\n",
        "from collections import OrderedDict, defaultdict\n",
        "from torch.utils.data import ConcatDataset\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac00db4",
      "metadata": {},
      "source": [
        "### GeraFed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1447e9f",
      "metadata": {
        "collapsed": true,
        "id": "c1447e9f"
      },
      "outputs": [],
      "source": [
        "wgan = False\n",
        "f2a = False\n",
        "epochs = 1\n",
        "losses_dict = {\"g_losses_chunk\": [],\n",
        "               \"d_losses_chunk\": [],\n",
        "               \"g_losses_round\": [],\n",
        "               \"d_losses_round\": [],\n",
        "               \"net_loss_chunk\": [],\n",
        "               \"net_acc_chunk\": [],\n",
        "               \"net_loss_round\": [],\n",
        "               \"net_acc_round\": [],\n",
        "               \"time_chunk\": [],\n",
        "               \"time_round\": [],\n",
        "               \"net_time\": [],\n",
        "               \"disc_time\": [],\n",
        "               \"gen_time\": [],\n",
        "               \"img_syn_time\": [],\n",
        "               \"track_mismatch_time\": []\n",
        "               }\n",
        "\n",
        "epoch_bar = tqdm(range(0, epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "\n",
        "batch_size_gen = 1\n",
        "batch_tam = 32\n",
        "extra_g_e = 20\n",
        "latent_dim = 128\n",
        "num_classes = 10\n",
        "if type(nets[0]).__name__ == \"Net\":\n",
        "  image = \"image\"\n",
        "else:\n",
        "  image = \"img\"\n",
        "\n",
        "if IN_COLAB:\n",
        "  acc_filename = os.path.join(save_dir,\"accuracy_report.txt\")\n",
        "  loss_filename = os.path.join(save_dir, \"losses.json\")\n",
        "  dmax_mismatch_log = os.path.join(save_dir, \"dmax_mismatch.txt\")\n",
        "  lambda_log = os.path.join(save_dir, \"lambda_log.txt\")\n",
        "\n",
        "else:\n",
        "  acc_filename = \"accuracy_report.txt\"\n",
        "  loss_filename = \"losses.json\"\n",
        "  dmax_mismatch_log = \"dmax_mismatch.txt\"\n",
        "  lambda_log = \"lambda_log.txt\"\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "  epoch_start_time = time.time()\n",
        "  mismatch_count = 0\n",
        "  total_checked = 0\n",
        "  g_loss_c = 0.0\n",
        "  d_loss_c = 0.0\n",
        "  total_d_samples = 0  # Amostras totais processadas pelos discriminadores\n",
        "  total_g_samples = 0  # Amostras totais processadas pelo gerador\n",
        "\n",
        "  chunk_bar = tqdm(range(num_chunks), desc=\"Chunks\", leave=True, position=1)\n",
        "\n",
        "  for chunk_idx in chunk_bar:\n",
        "    chunk_start_time = time.time()\n",
        "    params = []\n",
        "    results = []\n",
        "    # ====================================================================\n",
        "    # Treino dos Discriminadores (clientes) no bloco atual\n",
        "    # ====================================================================\n",
        "    d_loss_b = 0\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "\n",
        "    client_bar = tqdm(enumerate(zip(nets, models, client_chunks)), desc=\"Clients\", leave=True, position=2)\n",
        "\n",
        "    for cliente, (net, disc, chunks) in client_bar:\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      if len(chunk_dataset) == 0:\n",
        "        print(f\"Chunk {chunk_idx} for client {cliente} is empty, skipping.\")\n",
        "        continue\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size=batch_tam, shuffle=True)\n",
        "      if chunk_idx == 0:\n",
        "        client_eval_time = time.time()\n",
        "        # Evaluation in client test\n",
        "        # Initialize counters\n",
        "        class_correct = defaultdict(int)\n",
        "        class_total = defaultdict(int)\n",
        "        predictions_counter = defaultdict(int)\n",
        "\n",
        "        global_net.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in client_test_loaders[cliente]:\n",
        "                images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "                outputs = global_net(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Update counts for each sample in batch\n",
        "                for true_label, pred_label in zip(labels, predicted):\n",
        "                    true_idx = true_label.item()\n",
        "                    pred_idx = pred_label.item()\n",
        "\n",
        "                    class_total[true_idx] += 1\n",
        "                    predictions_counter[pred_idx] += 1\n",
        "\n",
        "                    if true_idx == pred_idx:\n",
        "                        class_correct[true_idx] += 1\n",
        "\n",
        "            # Create results dictionary\n",
        "            results_metrics = {\n",
        "                \"class_metrics\": {},\n",
        "                \"overall_accuracy\": None,\n",
        "                \"prediction_distribution\": dict(predictions_counter)\n",
        "            }\n",
        "\n",
        "            # Calculate class-wise metrics\n",
        "            for i in range(num_classes):\n",
        "                metrics = {\n",
        "                    \"samples\": class_total[i],\n",
        "                    \"predictions\": predictions_counter[i],\n",
        "                    \"accuracy\": class_correct[i] / class_total[i] if class_total[i] > 0 else \"N/A\"\n",
        "                }\n",
        "                results_metrics[\"class_metrics\"][f\"class_{i}\"] = metrics\n",
        "\n",
        "            # Calculate overall accuracy\n",
        "            total_samples = sum(class_total.values())\n",
        "            results_metrics[\"overall_accuracy\"] = sum(class_correct.values()) / total_samples\n",
        "\n",
        "            # Save to txt file\n",
        "            with open(acc_filename, \"a\") as f:\n",
        "                f.write(f\"Epoch {epoch + 1} - Client {cliente}\\n\")\n",
        "                # Header with fixed widths\n",
        "                f.write(\"{:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
        "                    \"Class\", \"Accuracy\", \"Samples\", \"Predictions\"))\n",
        "                f.write(\"-\"*45 + \"\\n\")\n",
        "\n",
        "                # Class rows with consistent formatting\n",
        "                for cls in range(num_classes):\n",
        "                    metrics = results_metrics[\"class_metrics\"][f\"class_{cls}\"]\n",
        "\n",
        "                    # Format accuracy (handle \"N/A\" case)\n",
        "                    accuracy = (f\"{metrics['accuracy']:.4f}\"\n",
        "                              if isinstance(metrics['accuracy'], float)\n",
        "                              else \"  N/A  \")\n",
        "\n",
        "                    f.write(\"{:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
        "                        f\"Class {cls}\",\n",
        "                        accuracy,\n",
        "                        metrics['samples'],\n",
        "                        metrics['predictions']\n",
        "                    ))\n",
        "\n",
        "                # Footer with alignment\n",
        "                f.write(\"\\n{:<20} {:.4f}\".format(\"Overall Accuracy:\", results_metrics[\"overall_accuracy\"]))\n",
        "                f.write(\"\\n{:<20} {}\".format(\"Total Samples:\", total_samples))\n",
        "                f.write(\"\\n{:<20} {}\".format(\"Total Predictions:\", sum(predictions_counter.values())))\n",
        "                f.write(\"\\n{:<20} {:.4f}\".format(\"Client Evaluation Time:\", time.time() - client_eval_time))\n",
        "                f.write(\"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        print(\"Results saved to accuracy_report.txt\")\n",
        "\n",
        "      # Treinar o discriminador no bloco\n",
        "      net.load_state_dict(global_net.state_dict(), strict=True)\n",
        "      net.to(device)\n",
        "      net.train()\n",
        "      disc.to(device)\n",
        "      optim = optims[cliente]\n",
        "      optim_D = optim_Ds[cliente]\n",
        "\n",
        "      start_img_syn_time = time.time()\n",
        "      num_samples = int(13 * (math.exp(0.01*epoch) - 1) / (math.exp(0.01*50) - 1)) * 10\n",
        "      generated_dataset = GeneratedDataset(generator=gen.to(\"cpu\"), num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\", image_col_name=image)\n",
        "      gen.to(device)\n",
        "      cmb_ds = ConcatDataset([chunk_dataset, generated_dataset])\n",
        "      combined_dataloader= DataLoader(cmb_ds, batch_size=batch_tam, shuffle=True)\n",
        "\n",
        "      img_syn_time = time.time() - start_img_syn_time\n",
        "\n",
        "      batch_bar_net = tqdm(combined_dataloader, desc=\"Batches\", leave=True, position=3)\n",
        "      start_net_time = time.time()\n",
        "      for batch in batch_bar_net:\n",
        "        images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        if batch_size == 1:\n",
        "          print(\"Batch size is 1, skipping batch\")\n",
        "          continue\n",
        "        optim.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "      net_time = time.time() - start_net_time\n",
        "\n",
        "      batch_bar = tqdm(chunk_loader, desc=\"Batches\", leave=True, position=4)\n",
        "\n",
        "      start_disc_time = time.time()\n",
        "      for batch in batch_bar:\n",
        "          images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "          batch_size = images.size(0)\n",
        "          if batch_size == 1:\n",
        "            print(\"Batch size is 1, skipping batch\")\n",
        "            continue\n",
        "\n",
        "          real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "          fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "          z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "          x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "\n",
        "          # Train D\n",
        "          optim_D.zero_grad()\n",
        "\n",
        "          if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            x_fake_l = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "\n",
        "            # Adicionar labels ao images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = x_fake_l.view(x_fake_l.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            images = torch.cat([images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z = torch.cat([z_noise, x_fake_l], dim=1)\n",
        "            fake_images = gen(z).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            d_loss = discriminator_loss(disc(images), disc(fake_images)) + 10 * gradient_penalty(disc, images, fake_images)\n",
        "\n",
        "          else:\n",
        "            # Dados Reais\n",
        "            y_real = disc(images, labels)\n",
        "            d_real_loss = disc.loss(y_real, real_ident)\n",
        "\n",
        "            # Dados Falsos\n",
        "            x_fake = gen(z_noise, x_fake_labels).detach()\n",
        "            y_fake_d = disc(x_fake, x_fake_labels)\n",
        "            d_fake_loss = disc.loss(y_fake_d, fake_ident)\n",
        "\n",
        "            # Loss total e backprop\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "          d_loss.backward()\n",
        "          #torch.nn.utils.clip_grad_norm_(disc.discriminator.parameters(), max_norm=1.0)\n",
        "          optim_D.step()\n",
        "          d_loss_b += d_loss.item()\n",
        "          total_chunk_samples += 1\n",
        "      disc_time = time.time() - start_disc_time  \n",
        "\n",
        "      params.append(ndarrays_to_parameters([val.cpu().numpy() for _, val in net.state_dict().items()]))\n",
        "      results.append((cliente, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=params[cliente], num_examples=len(chunk_loader.dataset), metrics={})))\n",
        "\n",
        "    aggregated_ndarrays = aggregate_inplace(results)\n",
        "\n",
        "    params_dict = zip(global_net.state_dict().keys(), aggregated_ndarrays)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
        "    global_net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    # Evaluation\n",
        "    if chunk_idx % 10 == 0:\n",
        "        global_net.eval()\n",
        "        correct, loss = 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                images = batch[image].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                outputs = global_net(images)\n",
        "                loss += criterion(outputs, labels).item()\n",
        "                correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        accuracy = correct / len(testloader.dataset)\n",
        "        losses_dict[\"net_loss_chunk\"].append(loss / len(testloader))\n",
        "        losses_dict[\"net_acc_chunk\"].append(accuracy)\n",
        "\n",
        "\n",
        "    # Média da perda dos discriminadores neste chunk\n",
        "    avg_d_loss_chunk = d_loss_b / total_chunk_samples if total_chunk_samples > 0 else 0.0\n",
        "    losses_dict[\"d_losses_chunk\"].append(avg_d_loss_chunk)\n",
        "    d_loss_c += avg_d_loss_chunk * total_chunk_samples\n",
        "    total_d_samples += total_chunk_samples\n",
        "\n",
        "    chunk_g_loss = 0.0\n",
        "\n",
        "    epoch_gen_bar = tqdm(range(extra_g_e), desc=\"Gerador\", leave=True, position=2)\n",
        "\n",
        "    start_gen_time = time.time()\n",
        "    for g_epoch in epoch_gen_bar:\n",
        "      # Train G\n",
        "      optim_G.zero_grad()\n",
        "\n",
        "      # Gera dados falsos\n",
        "      z_noise = torch.randn(batch_size_gen, latent_dim, device=device)\n",
        "      x_fake_labels = torch.randint(0, 10, (batch_size_gen,), device=device)\n",
        "      label = int(x_fake_labels.item())\n",
        "\n",
        "      if wgan:\n",
        "        x_fake_labels = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "        z_noise = torch.cat([z_noise, x_fake_labels], dim=1)\n",
        "        fake_images = gen(z_noise)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        image_fake_labels = x_fake_labels.view(x_fake_labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "        fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "        y_fake_gs = [model(fake_images.detach()) for model in models]\n",
        "\n",
        "      else:\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "        if f2a:\n",
        "          y_fakes = []\n",
        "          for D in models:\n",
        "              D = D.to(device)\n",
        "              y_fakes.append(D(x_fake, x_fake_labels))  # each is [B,1]\n",
        "          # stack into [N_discriminators, B, 1]\n",
        "          y_stack = torch.stack(y_fakes, dim=0)\n",
        "\n",
        "          # 4) Compute λ = ReLU(lambda_star) to enforce λ ≥ 0\n",
        "          lam = relu(lambda_star)\n",
        "\n",
        "          # 5) Soft‐max weights across the 0th dim (discriminators)\n",
        "          #    we want S_i = exp(λ D_i) / sum_j exp(λ D_j)\n",
        "          #    shape remains [N, B, 1]\n",
        "          S = torch.softmax(lam * y_stack, dim=0)\n",
        "\n",
        "          # 6) Weighted sum: D_agg shape [B,1]\n",
        "          D_agg = (S * y_stack).sum(dim=0)\n",
        "\n",
        "          # 7) Compute your generator loss + β λ² regularizer\n",
        "          real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "          adv_loss   = gen.loss(D_agg, real_ident)       # BCEWithLogitsLoss or whatever\n",
        "          reg_loss   = beta * lam.pow(2)                 # β λ²\n",
        "          g_loss     = adv_loss + reg_loss\n",
        "\n",
        "        else:\n",
        "          # Seleciona o melhor discriminador (Dmax)\n",
        "          y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "          y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "          dmax_index = y_fake_g_means.index(max(y_fake_g_means))\n",
        "          Dmax = models[dmax_index]\n",
        "\n",
        "          start_track_mismatch_time = time.time()\n",
        "          #Track mismatches\n",
        "          expected_indexes = label_to_client[class_labels.int2str(x_fake_labels.item())] ##PEGA SOMENTE A PRIMEIRA LABEL, SE BATCH_SIZE_GEN FOR DIFERENTE DE 1 VAI DAR ERRO\n",
        "          if dmax_index not in expected_indexes:\n",
        "              mismatch_count += 1\n",
        "              total_checked +=1\n",
        "              percent_mismatch =  mismatch_count / total_checked\n",
        "              with open(dmax_mismatch_log, \"a\") as mismatch_file:\n",
        "                  mismatch_file.write(f\"{epoch+1} {x_fake_labels.item()} {expected_indexes} {dmax_index} {percent_mismatch:.2f}\\n\")\n",
        "          else:\n",
        "              total_checked += 1\n",
        "              if g_epoch == extra_g_e - 1 and chunk_idx == num_chunks - 1:\n",
        "                percent_mismatch =  mismatch_count / total_checked\n",
        "                with open(dmax_mismatch_log, \"a\") as mismatch_file:\n",
        "                  mismatch_file.write(f\"{epoch+1} {x_fake_labels.item()} {expected_indexes} {dmax_index} {percent_mismatch:.2f}\\n\")\n",
        "          track_mismatch_time = time.time() - start_track_mismatch_time\n",
        "\n",
        "          # Calcula a perda do gerador\n",
        "          real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "          if wgan:\n",
        "            y_fake_g = Dmax(fake_images)\n",
        "            g_loss = generator_loss(y_fake_g)\n",
        "\n",
        "          else:\n",
        "            y_fake_g = Dmax(x_fake, x_fake_labels)  # Detach explícito\n",
        "            g_loss = gen.loss(y_fake_g, real_ident)\n",
        "\n",
        "      g_loss.backward()\n",
        "      #torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "      optim_G.step()\n",
        "      gen.to(device)\n",
        "      chunk_g_loss += g_loss.item()\n",
        "    gen_time = time.time() - start_gen_time\n",
        "\n",
        "    losses_dict[\"g_losses_chunk\"].append(chunk_g_loss / extra_g_e)\n",
        "    g_loss_c += chunk_g_loss /extra_g_e\n",
        "\n",
        "    losses_dict[\"time_chunk\"].append(time.time() - chunk_start_time)\n",
        "    losses_dict[\"net_time\"].append(net_time)\n",
        "    losses_dict[\"disc_time\"].append(disc_time)\n",
        "    losses_dict[\"gen_time\"].append(gen_time)\n",
        "    losses_dict[\"img_syn_time\"].append(img_syn_time)\n",
        "    losses_dict[\"track_mismatch_time\"].append(track_mismatch_time)\n",
        "\n",
        "\n",
        "  g_loss_e = g_loss_c/num_chunks\n",
        "  d_loss_e = d_loss_c / total_d_samples if total_d_samples > 0 else 0.0\n",
        "\n",
        "  losses_dict[\"g_losses_round\"].append(g_loss_e)\n",
        "  losses_dict[\"d_losses_round\"].append(d_loss_e)\n",
        "\n",
        "  if (epoch+1)%2==0:\n",
        "      checkpoint = {\n",
        "            'epoch': epoch+1,  # número da última época concluída\n",
        "            'alvo_state_dict': global_net.state_dict(),\n",
        "            'optimizer_alvo_state_dict': [optim.state_dict() for optim in optims],\n",
        "            'gen_state_dict': gen.state_dict(),\n",
        "            'optim_G_state_dict': optim_G.state_dict(),\n",
        "            'discs_state_dict': [model.state_dict() for model in models],\n",
        "            'optim_Ds_state_dict:': [optim_d.state_dict() for optim_d in optim_Ds]\n",
        "          }\n",
        "      checkpoint_file = f\"checkpoint_epoch{epoch+1}.pth\"\n",
        "      if IN_COLAB:\n",
        "          checkpoint_file = os.path.join(save_dir, checkpoint_file)\n",
        "      torch.save(checkpoint, checkpoint_file)\n",
        "      print(f\"Global net saved to {checkpoint_file}\")\n",
        "\n",
        "      if f2a:\n",
        "        current_lambda_star = lambda_star.item()\n",
        "        current_lam         = F.relu(lambda_star).item()\n",
        "\n",
        "        with open(lambda_log, \"a\") as f:\n",
        "          f.write(f\"{current_lambda_star},{current_lam}\\n\")\n",
        "\n",
        "  correct, loss = 0, 0.0\n",
        "  global_net.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch in testloader:\n",
        "          images = batch[image].to(device)\n",
        "          labels = batch[\"label\"].to(device)\n",
        "          outputs = global_net(images)\n",
        "          loss += criterion(outputs, labels).item()\n",
        "          correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "  accuracy = correct / len(testloader.dataset)\n",
        "  losses_dict[\"net_loss_round\"].append(loss / len(testloader))\n",
        "  losses_dict[\"net_acc_round\"].append(accuracy)\n",
        "\n",
        "  print(f\"Época {epoch+1} completa\")\n",
        "  generate_plot(gen, \"cpu\", epoch+1, latent_dim=128)\n",
        "  gen.to(device)\n",
        "\n",
        "  losses_dict[\"time_round\"].append(time.time() - epoch_start_time)\n",
        "\n",
        "  try:\n",
        "      with open(loss_filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(losses_dict, f, ensure_ascii=False, indent=4) # indent makes it readable\n",
        "      print(f\"Losses dict successfully saved to {loss_filename}\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving losses dict to JSON: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4677e695",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26024cd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(100):\n",
        "print(\"Epoch\", epoch, int(13 * (math.exp(0.01*epoch) - 1) / (math.exp(0.01*50) - 1)) * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9aa99aa",
      "metadata": {},
      "source": [
        "### Somente Classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_OdHAvK0LQp3",
      "metadata": {
        "collapsed": true,
        "id": "_OdHAvK0LQp3"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "losses_dict = {\"net_loss_chunk\": [],\n",
        "               \"net_acc_chunk\": [],\n",
        "               \"net_loss_round\": [],\n",
        "               \"net_acc_round\": [],\n",
        "               \"time_chunk\": [],\n",
        "               \"time_round\": []}\n",
        "\n",
        "epoch_bar = tqdm(range(0, epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "\n",
        "batch_tam = 32\n",
        "latent_dim = 128\n",
        "num_classes = 10\n",
        "if type(nets[0]).__name__ == \"Net\":\n",
        "  image = \"image\"\n",
        "else:\n",
        "  image = \"img\"\n",
        "\n",
        "if IN_COLAB:\n",
        "  acc_filename = os.path.join(save_dir,\"accuracy_report.txt\")\n",
        "  loss_filename = os.path.join(save_dir, \"losses.json\")\n",
        "else:\n",
        "  acc_filename = \"accuracy_report.txt\"\n",
        "  loss_filename = \"losses.json\"\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "  epoch_start_time = time.time()\n",
        "\n",
        "  chunk_bar = tqdm(range(num_chunks), desc=\"Chunks\", leave=True, position=1)\n",
        "\n",
        "  for chunk_idx in chunk_bar:\n",
        "    params = []\n",
        "    results = []\n",
        "    chunk_start_time = time.time()\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "    client_bar = tqdm(enumerate(zip(nets, client_chunks)), desc=\"Clients\", leave=True, position=2)\n",
        "\n",
        "    for cliente, (net, chunks) in client_bar:\n",
        "\n",
        "      if chunk_idx == 0:\n",
        "        client_eval_time = time.time()\n",
        "        # Evaluation in client test\n",
        "        # Initialize counters\n",
        "        class_correct = defaultdict(int)\n",
        "        class_total = defaultdict(int)\n",
        "        predictions_counter = defaultdict(int)\n",
        "\n",
        "        global_net.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in client_test_loaders[cliente]:\n",
        "                images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "                outputs = global_net(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Update counts for each sample in batch\n",
        "                for true_label, pred_label in zip(labels, predicted):\n",
        "                    true_idx = true_label.item()\n",
        "                    pred_idx = pred_label.item()\n",
        "\n",
        "                    class_total[true_idx] += 1\n",
        "                    predictions_counter[pred_idx] += 1\n",
        "\n",
        "                    if true_idx == pred_idx:\n",
        "                        class_correct[true_idx] += 1\n",
        "\n",
        "            # Create results dictionary\n",
        "            results_metrics = {\n",
        "                \"class_metrics\": {},\n",
        "                \"overall_accuracy\": None,\n",
        "                \"prediction_distribution\": dict(predictions_counter)\n",
        "            }\n",
        "\n",
        "            # Calculate class-wise metrics\n",
        "            for i in range(num_classes):\n",
        "                metrics = {\n",
        "                    \"samples\": class_total[i],\n",
        "                    \"predictions\": predictions_counter[i],\n",
        "                    \"accuracy\": class_correct[i] / class_total[i] if class_total[i] > 0 else \"N/A\"\n",
        "                }\n",
        "                results_metrics[\"class_metrics\"][f\"class_{i}\"] = metrics\n",
        "\n",
        "            # Calculate overall accuracy\n",
        "            total_samples = sum(class_total.values())\n",
        "            results_metrics[\"overall_accuracy\"] = sum(class_correct.values()) / total_samples\n",
        "\n",
        "            # Save to txt file\n",
        "            with open(acc_filename, \"a\") as f:\n",
        "                f.write(f\"Epoch {epoch + 1} - Client {cliente}\\n\")\n",
        "                # Header with fixed widths\n",
        "                f.write(\"{:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
        "                    \"Class\", \"Accuracy\", \"Samples\", \"Predictions\"))\n",
        "                f.write(\"-\"*45 + \"\\n\")\n",
        "\n",
        "                # Class rows with consistent formatting\n",
        "                for cls in range(num_classes):\n",
        "                    metrics = results_metrics[\"class_metrics\"][f\"class_{cls}\"]\n",
        "\n",
        "                    # Format accuracy (handle \"N/A\" case)\n",
        "                    accuracy = (f\"{metrics['accuracy']:.4f}\"\n",
        "                              if isinstance(metrics['accuracy'], float)\n",
        "                              else \"  N/A  \")\n",
        "\n",
        "                    f.write(\"{:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
        "                        f\"Class {cls}\",\n",
        "                        accuracy,\n",
        "                        metrics['samples'],\n",
        "                        metrics['predictions']\n",
        "                    ))\n",
        "\n",
        "                # Footer with alignment\n",
        "                f.write(\"\\n{:<20} {:.4f}\".format(\"Overall Accuracy:\", results_metrics[\"overall_accuracy\"]))\n",
        "                f.write(\"\\n{:<20} {}\".format(\"Total Samples:\", total_samples))\n",
        "                f.write(\"\\n{:<20} {}\".format(\"Total Predictions:\", sum(predictions_counter.values())))\n",
        "                f.write(\"\\n{:<20} {:.4f}\".format(\"Client Evaluation Time:\", time.time() - client_eval_time))\n",
        "                f.write(\"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        print(\"Results saved to accuracy_report.txt\")\n",
        "\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      if len(chunk_dataset) == 0:\n",
        "        print(f\"Chunk {chunk_idx} for client {cliente} is empty, skipping.\")\n",
        "        continue\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size=batch_tam, shuffle=False)\n",
        "\n",
        "      net.load_state_dict(global_net.state_dict(), strict=True)\n",
        "      net.to(device)\n",
        "      net.train()\n",
        "      optim = optims[cliente]\n",
        "\n",
        "      batch_bar = tqdm(chunk_loader, desc=\"Batches\", leave=True, position=3)\n",
        "\n",
        "      for batch in batch_bar:\n",
        "        images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        if batch_size == 1:\n",
        "          print(\"Batch size is 1, skipping batch\")\n",
        "          continue\n",
        "        optim.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "\n",
        "      params.append(ndarrays_to_parameters([val.cpu().numpy() for _, val in net.state_dict().items()]))\n",
        "      results.append((cliente, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=params[cliente], num_examples=len(chunk_loader.dataset), metrics={})))\n",
        "\n",
        "    aggregated_ndarrays = aggregate_inplace(results)\n",
        "\n",
        "    params_dict = zip(global_net.state_dict().keys(), aggregated_ndarrays)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
        "    global_net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    # Evaluation\n",
        "    if chunk_idx % 10 == 0:\n",
        "        global_net.eval()\n",
        "        correct, loss = 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                images = batch[image].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                outputs = global_net(images)\n",
        "                loss += criterion(outputs, labels).item()\n",
        "                correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        accuracy = correct / len(testloader.dataset)\n",
        "        losses_dict[\"net_loss_chunk\"].append(loss / len(testloader))\n",
        "        losses_dict[\"net_acc_chunk\"].append(accuracy)\n",
        "\n",
        "        losses_dict[\"time_chunk\"].append(time.time() - chunk_start_time)\n",
        "\n",
        "  correct, loss = 0, 0.0\n",
        "  global_net.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch in testloader:\n",
        "          images = batch[image].to(device)\n",
        "          labels = batch[\"label\"].to(device)\n",
        "          outputs = global_net(images)\n",
        "          loss += criterion(outputs, labels).item()\n",
        "          correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "  accuracy = correct / len(testloader.dataset)\n",
        "  losses_dict[\"net_loss_round\"].append(loss / len(testloader))\n",
        "  losses_dict[\"net_acc_round\"].append(accuracy)\n",
        "\n",
        "\n",
        "  print(f\"Época {epoch+1} completa\")\n",
        "\n",
        "  losses_dict[\"time_round\"].append(time.time() - epoch_start_time)\n",
        "\n",
        "  try:\n",
        "      with open(loss_filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(losses_dict, f, ensure_ascii=False, indent=4) # indent makes it readable\n",
        "      print(f\"Losses dict successfully saved to {loss_filename}\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving losses dict to JSON: {e}\")\n",
        "\n",
        "  if (epoch+1)%1==0:\n",
        "    checkpoint = {\n",
        "          'epoch': epoch+1,  # número da última época concluída\n",
        "          'alvo_state_dict': global_net.state_dict(),\n",
        "          'optimizer_alvo_state_dict': [optim.state_dict() for optim in optims],\n",
        "        }\n",
        "    checkpoint_file = f\"checkpoint_epoch{epoch+1}.pth\"\n",
        "    if IN_COLAB:\n",
        "        checkpoint_file = os.path.join(save_dir, checkpoint_file)\n",
        "    torch.save(checkpoint, checkpoint_file)\n",
        "    print(f\"Global net saved to {checkpoint_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44064765",
      "metadata": {},
      "source": [
        "### Somente Gerador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7312a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "wgan = False\n",
        "f2a = False\n",
        "epochs = 3\n",
        "losses_dict = {\"g_losses_chunk\": [],\n",
        "               \"d_losses_chunk\": [],\n",
        "               \"g_losses_round\": [],\n",
        "               \"d_losses_round\": [],\n",
        "               \"time_chunk\": [],\n",
        "               \"time_round\": [],\n",
        "               \"disc_time\": [],\n",
        "               \"gen_time\": [],\n",
        "               \"img_syn_time\": [],\n",
        "               \"track_mismatch_time\": []\n",
        "               }\n",
        "\n",
        "epoch_bar = tqdm(range(0, epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "\n",
        "batch_size_gen = 1\n",
        "batch_tam = 32\n",
        "extra_g_e = 20\n",
        "latent_dim = 128\n",
        "num_classes = 10\n",
        "\n",
        "if IN_COLAB:\n",
        "  loss_filename = os.path.join(save_dir, \"losses.json\")\n",
        "  dmax_mismatch_log = os.path.join(save_dir, \"dmax_mismatch.txt\")\n",
        "  lambda_log = os.path.join(save_dir, \"lambda_log.txt\")\n",
        "\n",
        "else:\n",
        "  loss_filename = \"losses.json\"\n",
        "  dmax_mismatch_log = \"dmax_mismatch.txt\"\n",
        "  lambda_log = \"lambda_log.txt\"\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "  epoch_start_time = time.time()\n",
        "  mismatch_count = 0\n",
        "  total_checked = 0\n",
        "  g_loss_c = 0.0\n",
        "  d_loss_c = 0.0\n",
        "  total_d_samples = 0  # Amostras totais processadas pelos discriminadores\n",
        "  total_g_samples = 0  # Amostras totais processadas pelo gerador\n",
        "\n",
        "  chunk_bar = tqdm(range(num_chunks), desc=\"Chunks\", leave=True, position=1)\n",
        "\n",
        "  for chunk_idx in chunk_bar:\n",
        "    chunk_start_time = time.time()\n",
        "    # ====================================================================\n",
        "    # Treino dos Discriminadores (clientes) no bloco atual\n",
        "    # ====================================================================\n",
        "    d_loss_b = 0\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "\n",
        "    client_bar = tqdm(enumerate(zip(models, client_chunks)), desc=\"Clients\", leave=True, position=2)\n",
        "\n",
        "    for cliente, (disc, chunks) in client_bar:\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      if len(chunk_dataset) == 0:\n",
        "        print(f\"Chunk {chunk_idx} for client {cliente} is empty, skipping.\")\n",
        "        continue\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size=batch_tam, shuffle=True)\n",
        "\n",
        "      # Treinar o discriminador no bloco\n",
        "      disc.to(device)\n",
        "      optim_D = optim_Ds[cliente]\n",
        "\n",
        "      batch_bar = tqdm(chunk_loader, desc=\"Batches\", leave=True, position=4)\n",
        "\n",
        "      start_disc_time = time.time()\n",
        "      for batch in batch_bar:\n",
        "          images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "          batch_size = images.size(0)\n",
        "          if batch_size == 1:\n",
        "            print(\"Batch size is 1, skipping batch\")\n",
        "            continue\n",
        "\n",
        "          real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "          fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "          z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "          x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "\n",
        "          # Train D\n",
        "          optim_D.zero_grad()\n",
        "\n",
        "          if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            x_fake_l = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "\n",
        "            # Adicionar labels ao images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = x_fake_l.view(x_fake_l.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            images = torch.cat([images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z = torch.cat([z_noise, x_fake_l], dim=1)\n",
        "            fake_images = gen(z).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            d_loss = discriminator_loss(disc(images), disc(fake_images)) + 10 * gradient_penalty(disc, images, fake_images)\n",
        "\n",
        "          else:\n",
        "            # Dados Reais\n",
        "            y_real = disc(images, labels)\n",
        "            d_real_loss = disc.loss(y_real, real_ident)\n",
        "\n",
        "            # Dados Falsos\n",
        "            x_fake = gen(z_noise, x_fake_labels).detach()\n",
        "            y_fake_d = disc(x_fake, x_fake_labels)\n",
        "            d_fake_loss = disc.loss(y_fake_d, fake_ident)\n",
        "\n",
        "            # Loss total e backprop\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "          d_loss.backward()\n",
        "          #torch.nn.utils.clip_grad_norm_(disc.discriminator.parameters(), max_norm=1.0)\n",
        "          optim_D.step()\n",
        "          d_loss_b += d_loss.item()\n",
        "          total_chunk_samples += 1\n",
        "      disc_time = time.time() - start_disc_time  \n",
        "\n",
        "\n",
        "    # Média da perda dos discriminadores neste chunk\n",
        "    avg_d_loss_chunk = d_loss_b / total_chunk_samples if total_chunk_samples > 0 else 0.0\n",
        "    losses_dict[\"d_losses_chunk\"].append(avg_d_loss_chunk)\n",
        "    d_loss_c += avg_d_loss_chunk * total_chunk_samples\n",
        "    total_d_samples += total_chunk_samples\n",
        "\n",
        "    chunk_g_loss = 0.0\n",
        "\n",
        "    epoch_gen_bar = tqdm(range(extra_g_e), desc=\"Gerador\", leave=True, position=2)\n",
        "\n",
        "    start_gen_time = time.time()\n",
        "    for g_epoch in epoch_gen_bar:\n",
        "      # Train G\n",
        "      optim_G.zero_grad()\n",
        "\n",
        "      # Gera dados falsos\n",
        "      z_noise = torch.randn(batch_size_gen, latent_dim, device=device)\n",
        "      x_fake_labels = torch.randint(0, 10, (batch_size_gen,), device=device)\n",
        "      label = int(x_fake_labels.item())\n",
        "\n",
        "      if wgan:\n",
        "        x_fake_labels = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "        z_noise = torch.cat([z_noise, x_fake_labels], dim=1)\n",
        "        fake_images = gen(z_noise)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        image_fake_labels = x_fake_labels.view(x_fake_labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "        fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "        y_fake_gs = [model(fake_images.detach()) for model in models]\n",
        "\n",
        "      else:\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "        if f2a:\n",
        "          y_fakes = []\n",
        "          for D in models:\n",
        "              D = D.to(device)\n",
        "              y_fakes.append(D(x_fake, x_fake_labels))  # each is [B,1]\n",
        "          # stack into [N_discriminators, B, 1]\n",
        "          y_stack = torch.stack(y_fakes, dim=0)\n",
        "\n",
        "          # 4) Compute λ = ReLU(lambda_star) to enforce λ ≥ 0\n",
        "          lam = relu(lambda_star)\n",
        "\n",
        "          # 5) Soft‐max weights across the 0th dim (discriminators)\n",
        "          #    we want S_i = exp(λ D_i) / sum_j exp(λ D_j)\n",
        "          #    shape remains [N, B, 1]\n",
        "          S = torch.softmax(lam * y_stack, dim=0)\n",
        "\n",
        "          # 6) Weighted sum: D_agg shape [B,1]\n",
        "          D_agg = (S * y_stack).sum(dim=0)\n",
        "\n",
        "          # 7) Compute your generator loss + β λ² regularizer\n",
        "          real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "          adv_loss   = gen.loss(D_agg, real_ident)       # BCEWithLogitsLoss or whatever\n",
        "          reg_loss   = beta * lam.pow(2)                 # β λ²\n",
        "          g_loss     = adv_loss + reg_loss\n",
        "\n",
        "        else:\n",
        "          # Seleciona o melhor discriminador (Dmax)\n",
        "          y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "          y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "          dmax_index = y_fake_g_means.index(max(y_fake_g_means))\n",
        "          Dmax = models[dmax_index]\n",
        "\n",
        "          start_track_mismatch_time = time.time()\n",
        "          #Track mismatches\n",
        "          expected_indexes = label_to_client[class_labels.int2str(x_fake_labels.item())] ##PEGA SOMENTE A PRIMEIRA LABEL, SE BATCH_SIZE_GEN FOR DIFERENTE DE 1 VAI DAR ERRO\n",
        "          if dmax_index not in expected_indexes:\n",
        "              mismatch_count += 1\n",
        "              total_checked +=1\n",
        "              percent_mismatch =  mismatch_count / total_checked\n",
        "              with open(dmax_mismatch_log, \"a\") as mismatch_file:\n",
        "                  mismatch_file.write(f\"{epoch+1} {x_fake_labels.item()} {expected_indexes} {dmax_index} {percent_mismatch:.2f}\\n\")\n",
        "          else:\n",
        "              total_checked += 1\n",
        "              if g_epoch == extra_g_e - 1 and chunk_idx == num_chunks - 1:\n",
        "                percent_mismatch =  mismatch_count / total_checked\n",
        "                with open(dmax_mismatch_log, \"a\") as mismatch_file:\n",
        "                  mismatch_file.write(f\"{epoch+1} {x_fake_labels.item()} {expected_indexes} {dmax_index} {percent_mismatch:.2f}\\n\")\n",
        "          track_mismatch_time = time.time() - start_track_mismatch_time\n",
        "\n",
        "          # Calcula a perda do gerador\n",
        "          real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "          if wgan:\n",
        "            y_fake_g = Dmax(fake_images)\n",
        "            g_loss = generator_loss(y_fake_g)\n",
        "\n",
        "          else:\n",
        "            y_fake_g = Dmax(x_fake, x_fake_labels)  # Detach explícito\n",
        "            g_loss = gen.loss(y_fake_g, real_ident)\n",
        "\n",
        "      g_loss.backward()\n",
        "      #torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "      optim_G.step()\n",
        "      gen.to(device)\n",
        "      chunk_g_loss += g_loss.item()\n",
        "    gen_time = time.time() - start_gen_time\n",
        "\n",
        "    losses_dict[\"g_losses_chunk\"].append(chunk_g_loss / extra_g_e)\n",
        "    g_loss_c += chunk_g_loss /extra_g_e\n",
        "\n",
        "    losses_dict[\"time_chunk\"].append(time.time() - chunk_start_time)\n",
        "    losses_dict[\"disc_time\"].append(disc_time)\n",
        "    losses_dict[\"gen_time\"].append(gen_time)\n",
        "    losses_dict[\"track_mismatch_time\"].append(track_mismatch_time)\n",
        "\n",
        "\n",
        "  g_loss_e = g_loss_c/num_chunks\n",
        "  d_loss_e = d_loss_c / total_d_samples if total_d_samples > 0 else 0.0\n",
        "\n",
        "  losses_dict[\"g_losses_round\"].append(g_loss_e)\n",
        "  losses_dict[\"d_losses_round\"].append(d_loss_e)\n",
        "\n",
        "  if (epoch+1)%2==0:\n",
        "      checkpoint = {\n",
        "            'epoch': epoch+1,  # número da última época concluída\n",
        "            'gen_state_dict': gen.state_dict(),\n",
        "            'optim_G_state_dict': optim_G.state_dict(),\n",
        "            'discs_state_dict': [model.state_dict() for model in models],\n",
        "            'optim_Ds_state_dict:': [optim_d.state_dict() for optim_d in optim_Ds]\n",
        "          }\n",
        "      checkpoint_file = f\"checkpoint_epoch{epoch+1}.pth\"\n",
        "      if IN_COLAB:\n",
        "          checkpoint_file = os.path.join(save_dir, checkpoint_file)\n",
        "      torch.save(checkpoint, checkpoint_file)\n",
        "      print(f\"Global net saved to {checkpoint_file}\")\n",
        "\n",
        "      if f2a:\n",
        "        current_lambda_star = lambda_star.item()\n",
        "        current_lam         = F.relu(lambda_star).item()\n",
        "\n",
        "        with open(lambda_log, \"a\") as f:\n",
        "          f.write(f\"{current_lambda_star},{current_lam}\\n\")\n",
        "\n",
        "  print(f\"Época {epoch+1} completa\")\n",
        "  generate_plot(gen, \"cpu\", epoch+1, latent_dim=128)\n",
        "  gen.to(device)\n",
        "\n",
        "  losses_dict[\"time_round\"].append(time.time() - epoch_start_time)\n",
        "\n",
        "  try:\n",
        "      with open(loss_filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(losses_dict, f, ensure_ascii=False, indent=4) # indent makes it readable\n",
        "      print(f\"Losses dict successfully saved to {loss_filename}\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving losses dict to JSON: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3ce47d",
      "metadata": {
        "id": "2c3ce47d"
      },
      "source": [
        "# Gráficos de perda e acurácia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f7dbfb",
      "metadata": {
        "id": "01f7dbfb"
      },
      "source": [
        "## Le o arquivo de perda salvo no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-by_-71kSOeu",
      "metadata": {
        "id": "-by_-71kSOeu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ff7c31",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(\"../../DML-ICC/Experiments/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3233e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "files =[\n",
        "    #FedAvg\n",
        "    \"MNIST/Class/Trial1/metrics_fedavg.json\",\n",
        "    \"CIFAR10/Class/Trial1/metrics_fedavg.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/metrics_fedavg.json\",\n",
        "    \"CIFAR10/Dir05/Trial1/metrics_fedavg.json\",\n",
        "\n",
        "    # Chunked FedAvg\n",
        "    \"MNIST/Class/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"CIFAR10/Class/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"CIFAR10/Dir05/Trial1/losses_chunked_fedavg.json\",\n",
        "\n",
        "    \"CIFAR10/Class/Trial1/fedavg_50chunks_metrics.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/fedavg_50chunks_metrics.json\",\n",
        "    #\"CIFAR10/Dir05/Trial1/\n",
        "\n",
        "\n",
        "    # Chunked FedProx\n",
        "    \"MNIST/Class/Trial1/metrics_chunked_fedprox.json\",\n",
        "    \"CIFAR10/Class/Trial1/metrics_chunked_fedprox.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/metrics_chunked_fedprox.json\",\n",
        "    \"CIFAR10/Dir05/Trial1/metrics_chunked_fedprox.json\",\n",
        "\n",
        "    \"CIFAR10/Class/Trial1/fedprox_50chunks_metrics.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/fedprox_50chunks_metrics.json\",\n",
        "    #\"CIFAR10/Dir05/Trial1/metrics_chunked_fedprox.json\",\n",
        "\n",
        "\n",
        "    # Chunked Scaffold\n",
        "    \"MNIST/Class/Trial1/metrics_chunked_scaffold.json\",\n",
        "    \"CIFAR10/Class/Trial1/metrics_chunked_scaffold.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/metrics_chunked_scaffold.json\",\n",
        "    \"CIFAR10/Dir05/Trial1/metrics_chunked_scaffold.json\",\n",
        "\n",
        "    # FedGenIA\n",
        "    \"MNIST/Class/Trial1/metrics.json\",\n",
        "    \"CIFAR10/Class/Trial1/metrics.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/metrics.json\",\n",
        "    #\"CIFAR10/Dir05/Trial1/metrics.json\",\n",
        "    \"CIFAR10/Dir05/Trial2/metrics.json\",\n",
        "\n",
        "    \"CIFAR10/Class/Trial1/fedavg_fedgenia_50chunks_metrics.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/fedgenia_fedavg_50chunks_metrics.json\",\n",
        "\n",
        "\n",
        "    # FedGenIA + FedProx\n",
        "    \"MNIST/Class/Trial1/metrics_fedprox_fedgenia.json\",\n",
        "    \"CIFAR10/Class/Trial1/metrics_fedprox_fedgenia.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/metrics_fedprox_fedgenia.json\",\n",
        "    \"CIFAR10/Dir05/Trial1/metrics_fedprox_fedgenia.json\",\n",
        "\n",
        "    \"CIFAR10/Class/Trial1/fedgenia_fedprox_50chunks_metrics.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/fedgenia_fedprox_50chunks_metrics.json\",\n",
        "\n",
        "\n",
        "    # FedGenIA + Scaffold\n",
        "    \"MNIST/Class/Trial1/metrics_scaffold_fedgenia.json\",\n",
        "    \"CIFAR10/Class/Trial1/metrics_scaffold_fedgenia.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/metrics_scaffold_fedgenia.json\",\n",
        "    \"CIFAR10/Dir05/Trial1/metrics_scaffold_fedgenia.json\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4434eb27",
      "metadata": {},
      "outputs": [],
      "source": [
        "files = [\n",
        "    \"MNIST/Class/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"MNIST/Dir01/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"MNIST/Dir05/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"losses_iid_mnist_chunked_fedavg.json\",\n",
        "    \"CIFAR10/Class/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"CIFAR10/Dir05/Trial1/losses_chunked_fedavg.json\",\n",
        "    \"losses_iid_cifar_chunked_fedavg.json\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FaRDCz_cSJDf",
      "metadata": {
        "id": "FaRDCz_cSJDf"
      },
      "outputs": [],
      "source": [
        "files = [\n",
        "    \"MNIST/Class/Trial1/losses.json\",\n",
        "    \"MNIST/Class/Trial2/losses.json\",\n",
        "    \"MNIST/Class/Trial3/losses.json\",\n",
        "    # \"MNIST/Dir01/Trial1/losses.json\",\n",
        "    # \"MNIST/Dir01/Trial2/losses.json\",\n",
        "    # \"MNIST/Dir01/Trial3/losses.json\",\n",
        "    # \"MNIST/Dir05/Trial1/losses.json\",\n",
        "    # \"MNIST/Dir05/Trial2/losses.json\",\n",
        "    # \"MNIST/Dir05/Trial3/losses.json\",\n",
        "    \"CIFAR10/Class/Trial1/losses.json\",\n",
        "    \"CIFAR10/Class/Trial2/losses.json\",\n",
        "    \"CIFAR10/Class/Trial3/losses.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/losses.json\",\n",
        "    \"CIFAR10/Dir01/Trial2/losses.json\",\n",
        "    \"CIFAR10/Dir01/Trial3/losses.json\",\n",
        "    \"CIFAR10/Dir05/Trial1/losses.json\",\n",
        "    \"CIFAR10/Dir05/Trial2/losses.json\",\n",
        "    \"CIFAR10/Dir05/Trial3/losses.json\"\n",
        "]\n",
        "\n",
        "# if IN_COLAB:\n",
        "#   loss_filename = os.path.join(save_dir, loss_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49537f37",
      "metadata": {
        "id": "49537f37"
      },
      "outputs": [],
      "source": [
        "loaded_dicts = {}\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            loaded_dicts[file.replace(\"/\", \"_\")] = json.load(f)\n",
        "\n",
        "        print(f\"Dictionary successfully loaded from {file}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file}' not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from '{file}'. File might be corrupted or not JSON.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dictionary from JSON: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d24b634",
      "metadata": {
        "id": "8d24b634"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0588a2",
      "metadata": {
        "id": "8f0588a2"
      },
      "outputs": [],
      "source": [
        "def parse_client_accuracies(log_path):\n",
        "   # Regex to match \"Round X - Cliente Y\" and \"Overall Accuracy:    Z.ZZZZ\"\n",
        "   header_re   = re.compile(r\"Epoch\\s+\\d+\\s*-\\s*Client\\s*(\\d+)\", re.IGNORECASE)\n",
        "   accuracy_re = re.compile(r\"Overall Accuracy:\\s*([\\d.]+)\")\n",
        "\n",
        "\n",
        "   # Now client → list of accuracies\n",
        "   client_accuracies = defaultdict(list)\n",
        "\n",
        "\n",
        "   with open(log_path, 'r', encoding='utf-8') as f:\n",
        "       current_client = None\n",
        "\n",
        "\n",
        "       for line in f:\n",
        "           # Detect the client header\n",
        "           hdr = header_re.search(line)\n",
        "           if hdr:\n",
        "               current_client = int(hdr.group(1))\n",
        "               continue\n",
        "\n",
        "\n",
        "           # Once we see the accuracy line, append and reset\n",
        "           if current_client is not None:\n",
        "               acc = accuracy_re.search(line)\n",
        "               if acc:\n",
        "                   client_accuracies[current_client].append(float(acc.group(1)))\n",
        "                   current_client = None\n",
        "\n",
        "\n",
        "   return dict(client_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f397ca9c",
      "metadata": {
        "id": "f397ca9c"
      },
      "outputs": [],
      "source": [
        "log_files = [\n",
        "    \"CIFAR10/Dir01/Trial1/local_accuracy_report.txt\",\n",
        "    \"CIFAR10/Dir01/Trial1/local_accuracy_report_chunked_fedavg.txt\",\n",
        "    \"CIFAR10/Dir01/Trial1/local_accuracy_report_chunked_fedprox.txt\",\n",
        "    \"CIFAR10/Dir01/Trial1/local_accuracy_report_fedprox_fedgenia.txt\",\n",
        "    \"CIFAR10/Dir01/Trial1/fedavg_50chunks_metrics.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/fedgenia_fedavg_50chunks_metrics.json\",\n",
        "    \"CIFAR10/Dir01/Trial1/fedprox_50chunks_local_accuracy_report.txt\",\n",
        "    \"CIFAR10/Dir01/Trial1/fedgenia_fedprox_50chunks_metrics.json\"\n",
        "]\n",
        "\n",
        "local_acc_dict = {}\n",
        "\n",
        "for file in log_files:\n",
        "    local_acc_dict[file.replace(\"/\", \"_\")] = parse_client_accuracies(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de16e414",
      "metadata": {},
      "source": [
        "## Obter estatisticas dos trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac6a130",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9f0166",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group dicts by experiment (e.g. MNIST_Class, CIFAR_Dir01, etc.)\n",
        "grouped = defaultdict(list)\n",
        "\n",
        "for key, metrics in loaded_dicts.items():\n",
        "    # Extract experiment part (remove trial name)\n",
        "    # Example: MNIST_Class_Trial1_losses.json → MNIST_Class\n",
        "    parts = key.split(\"_\")\n",
        "    experiment_name = \"_\".join(parts[:-2])  # remove 'TrialX' and 'losses.json'\n",
        "    grouped[experiment_name].append(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c2c8ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment_stats = {}\n",
        "\n",
        "for exp_name, trials in grouped.items():\n",
        "    stats = {}\n",
        "    metric_keys = trials[0].keys()  # assume all trials share same keys\n",
        "    \n",
        "    for key in metric_keys:\n",
        "        # Stack all trials' metric lists into a numpy array\n",
        "        values = [t[key] for t in trials if len(t[key]) > 0]\n",
        "\n",
        "        if not values:\n",
        "            continue\n",
        "\n",
        "        # Pad shorter lists if needed to align lengths (optional)\n",
        "        min_len = min(len(v) for v in values)\n",
        "        values = [v[:min_len] for v in values]  # truncate to shortest length\n",
        "\n",
        "        arr = np.array(values)  # shape = (num_trials, num_values)\n",
        "        \n",
        "        stats[key] = {\n",
        "            \"mean\": np.mean(arr, axis=0).tolist(),\n",
        "            \"median\": np.median(arr, axis=0).tolist(),\n",
        "            \"std\": np.std(arr, axis=0).tolist()\n",
        "        }\n",
        "\n",
        "    experiment_stats[exp_name] = stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b890d518",
      "metadata": {
        "id": "b890d518"
      },
      "source": [
        "## Funcao de plotagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39493e97",
      "metadata": {
        "id": "39493e97"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "from typing import Mapping, Iterable, Any, Literal, Union, List, Tuple\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c05bff13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_series(\n",
        "    series: Mapping[str, Iterable[float]],\n",
        "    *,\n",
        "    x_values: Mapping[str, Iterable[float]] = None,\n",
        "    subplot_groups: List[List[str]] = None,\n",
        "    subplot_layout: Tuple[int, int] = None,\n",
        "    legend_subplot_index: Union[int, str] = 'all',\n",
        "    series_styles: Mapping[str, Mapping[str, Any]] = None,\n",
        "    xlim: Union[tuple[float, float], List[tuple[float, float]]] = None,\n",
        "    ylim: Union[tuple[float, float], List[tuple[float, float]]] = None,\n",
        "    first_step: Union[int, List[int]] = None,\n",
        "    xtick_step: Union[int, List[int]] = 1,\n",
        "    xtick_offset: int = 0,\n",
        "    num_xticks: Union[int, List[int]] = None,\n",
        "    num_yticks: Union[int, List[int]] = None,\n",
        "    y_ticks: Union[List[float], List[List[float]]] = None,\n",
        "    xlabel: Union[str, List[str]] = \"Epochs\",\n",
        "    ylabel: Union[str, List[str]] = \"Value\",\n",
        "    label_fontsize: float = None,\n",
        "    tick_fontsize: float = None,\n",
        "    title: Union[str, List[str]] = None,\n",
        "    title_fontsize: float = None,\n",
        "    highlight: Mapping[str, Literal[\"max\", \"min\", \"both\"]] = None,\n",
        "    highlight_marker: str = \"o\",\n",
        "    highlight_markersize: float = 4,\n",
        "    highlight_color: str = None,\n",
        "    highlight_text_size: int = 8,\n",
        "    highlight_text_color: str = None,\n",
        "    highlight_arrow_color: str = None,\n",
        "    highlight_arrow_style: str = \"->\",\n",
        "    highlight_arrow_linewidth: float = 1,\n",
        "    highlight_text_offset_max: tuple[float, float] = (0.1, 0.2),\n",
        "    highlight_text_offset_min: tuple[float, float] = (0.1, -0.2),\n",
        "    highlight_style: Mapping[str, Mapping[str, Any]] = None,\n",
        "    legend_loc: str = 'best',\n",
        "    legend_fontsize: float = 10,\n",
        "    legend_kwargs: Mapping[str, Any] = None,\n",
        "    figsize: tuple[float, float] = (10, 5),\n",
        "    hspace: float = None,\n",
        "    vspace: float = None,\n",
        "    subplot_margins: dict = None,\n",
        "    save: bool = False,\n",
        "    plot_name: str = \"plot.pdf\"\n",
        ") -> None:\n",
        "    if subplot_groups is None:\n",
        "        subplot_groups = [list(series.keys())]\n",
        "\n",
        "    num_plots = len(subplot_groups)\n",
        "\n",
        "    if subplot_layout:\n",
        "        nrows, ncols = subplot_layout\n",
        "        if nrows * ncols < num_plots:\n",
        "            raise ValueError(f\"Layout {subplot_layout} is too small for {num_plots} groups.\")\n",
        "    else:\n",
        "        nrows, ncols = num_plots, 1\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize, squeeze=False)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    def get_setting(value, index):\n",
        "        # This helper function is the key! It returns the specific or the general value.\n",
        "        if isinstance(value, list):\n",
        "            return value[index] if index < len(value) else None\n",
        "        return value\n",
        "\n",
        "    for i, (ax, group) in enumerate(zip(axes, subplot_groups)):\n",
        "        n = 0\n",
        "        if group:\n",
        "            n = max(len(series.get(name, [])) for name in group)\n",
        "\n",
        "        for name in group:\n",
        "            if name not in series:\n",
        "                continue\n",
        "            ys = series[name]\n",
        "            xs = x_values.get(name, range(len(ys))) if x_values else range(len(ys))\n",
        "            style = series_styles.get(name, {}) if series_styles else {}\n",
        "            current_highlight_style = highlight_style.get(name, {}) if highlight_style else {}\n",
        "            line, = ax.plot(xs, ys, label=name, **style)\n",
        "            mode = highlight.get(name) if highlight else None\n",
        "            base_color = style.get('color', line.get_color())\n",
        "            mcolor = highlight_color or base_color\n",
        "\n",
        "            if mode in (\"max\", \"both\"):\n",
        "                i_max = max(range(len(ys)), key=lambda j: ys[j])\n",
        "                ax.plot(i_max, ys[i_max], marker=highlight_marker, markersize=highlight_markersize, color=mcolor)\n",
        "                offset = current_highlight_style.get('highlight_offset_max', highlight_text_offset_max)\n",
        "                text_position = (i_max + offset[0], ys[i_max] + offset[1])\n",
        "                arrow_color = current_highlight_style.get('arrow_color', highlight_arrow_color or 'dimgrey')\n",
        "                arrow_style = current_highlight_style.get('arrow_style', highlight_arrow_style)\n",
        "                arrow_width = current_highlight_style.get('arrow_linewidth', highlight_arrow_linewidth)\n",
        "                text_color = current_highlight_style.get('text_color', highlight_text_color or 'black')\n",
        "                ax.annotate(f\"{ys[i_max]:.2f}\",\n",
        "                            xy=(i_max, ys[i_max]), \n",
        "                            xytext=text_position,\n",
        "                            arrowprops=dict(arrowstyle=arrow_style, color=arrow_color, linewidth=arrow_width),\n",
        "                            fontsize=highlight_text_size, \n",
        "                            color=text_color,\n",
        "                            va=\"bottom\", \n",
        "                            ha=\"center\")\n",
        "            if mode in (\"min\", \"both\"):\n",
        "                i_min = min(range(len(ys)), key=lambda j: ys[j])\n",
        "                ax.plot(i_min, ys[i_min], marker=highlight_marker, markersize=highlight_markersize, color=mcolor)\n",
        "                offset = current_highlight_style.get('highlight_offset_min', highlight_text_offset_min)\n",
        "                text_position = (i_min + offset[0], ys[i_min] + offset[1])\n",
        "                arrow_color = current_highlight_style.get('arrow_color', highlight_arrow_color or 'dimgrey')\n",
        "                arrow_style = current_highlight_style.get('arrow_style', highlight_arrow_style)\n",
        "                arrow_width = current_highlight_style.get('arrow_linewidth', highlight_arrow_linewidth)\n",
        "                text_color = current_highlight_style.get('text_color', highlight_text_color or 'black')\n",
        "                ax.annotate(f\"{ys[i_min]:.2f}\", \n",
        "                            xy=(i_min, ys[i_min]), \n",
        "                            xytext=text_position,\n",
        "                            arrowprops=dict(arrowstyle=arrow_style, color=arrow_color, linewidth=arrow_width),\n",
        "                            fontsize=highlight_text_size,\n",
        "                            color=text_color,\n",
        "                            va=\"top\", \n",
        "                            ha=\"center\")\n",
        "\n",
        "        if n > 0:\n",
        "            # <<< CHANGED: Get subplot-specific settings using the helper function >>>\n",
        "            current_num_yticks = get_setting(num_yticks, i)\n",
        "            current_y_ticks = get_setting(y_ticks, i) # Assuming y_ticks could also be a list of lists\n",
        "            current_num_xticks = get_setting(num_xticks, i)\n",
        "            current_first_step = get_setting(first_step, i)\n",
        "            current_xtick_step = get_setting(xtick_step, i)\n",
        "\n",
        "            if current_num_yticks or current_y_ticks:\n",
        "                if current_num_yticks:\n",
        "                    # Determine the range for tick calculation\n",
        "                    current_ylim = get_setting(ylim, i)\n",
        "                    if current_ylim:\n",
        "                        # Base ticks on the specified limits\n",
        "                        min_y, max_y = current_ylim\n",
        "                    else:\n",
        "                        # Find the range across all series in this specific group\n",
        "                        min_y = float('inf')\n",
        "                        max_y = float('-inf')\n",
        "                        for name in group:\n",
        "                            if name in series and len(series[name]) > 0:\n",
        "                                min_y = min(min_y, min(series[name]))\n",
        "                                max_y = max(max_y, max(series[name]))\n",
        "\n",
        "                        # Ensure we have a reasonable range\n",
        "                        if min_y == float('inf') or max_y == float('-inf'):\n",
        "                            min_y, max_y = 0, 1.0\n",
        "                        \n",
        "                        # Optional: round the range for cleaner ticks\n",
        "                        min_y = math.floor(min_y * 10) / 10\n",
        "                        max_y = math.ceil(max_y * 10) / 10\n",
        "\n",
        "                    yticks = np.linspace(min_y, max_y, current_num_yticks)\n",
        "\n",
        "                    yticks = np.unique(yticks)\n",
        "                else:\n",
        "                    yticks = current_y_ticks\n",
        "                ax.set_yticks(yticks)\n",
        "                ax.yaxis.set_major_formatter(StrMethodFormatter('{x:.2f}'))\n",
        "\n",
        "            if current_num_xticks:\n",
        "                xticks = np.linspace(1, n, current_num_xticks)\n",
        "                ax.set_xticks(xticks.astype(int))\n",
        "            elif current_first_step is not None:\n",
        "                labels = [1]\n",
        "                # Use the per-subplot step, falling back to the default of 1 if not specified\n",
        "                step = current_xtick_step if current_xtick_step is not None else 1\n",
        "                next_label = 1 + current_first_step\n",
        "                while next_label <= n:\n",
        "                    labels.append(next_label)\n",
        "                    next_label += step\n",
        "                positions = [lbl - 1 for lbl in labels]\n",
        "                labels = [lbl + xtick_offset for lbl in labels]\n",
        "                ax.set_xticks(positions, labels)\n",
        "            elif current_xtick_step is not None and current_xtick_step > 0:\n",
        "                positions = list(range(0, n, current_xtick_step))\n",
        "                labels = [pos + 1 + xtick_offset for pos in positions]\n",
        "                ax.set_xticks(positions, labels)\n",
        "\n",
        "        if num_xticks and xtick_offset != 0 and n > 0:\n",
        "            fig.canvas.draw()\n",
        "            current_ticks = ax.get_xticks()\n",
        "            new_labels = [int(tick) + xtick_offset for tick in current_ticks]\n",
        "            ax.set_xticklabels(new_labels)\n",
        "\n",
        "        ax.set_xlabel(get_setting(xlabel, i), fontsize=label_fontsize)\n",
        "        ax.set_ylabel(get_setting(ylabel, i), fontsize=label_fontsize)\n",
        "        ax.set_title(get_setting(title, i), fontsize=title_fontsize)\n",
        "\n",
        "        if tick_fontsize:\n",
        "            ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
        "        if legend_subplot_index == 'all' or i == legend_subplot_index:\n",
        "            base_kwargs = {'loc': legend_loc, 'fontsize': legend_fontsize}\n",
        "            if legend_kwargs:\n",
        "                base_kwargs.update(legend_kwargs)\n",
        "            ax.legend(**base_kwargs)\n",
        "\n",
        "        current_xlim = get_setting(xlim, i)\n",
        "        if current_xlim:\n",
        "            ax.set_xlim(*current_xlim)\n",
        "        elif n > 0:\n",
        "            # Set a sensible default xlim based on data\n",
        "            ax.set_xlim(0, n)\n",
        "        current_ylim = get_setting(ylim, i)\n",
        "        if current_ylim:\n",
        "            ax.set_ylim(*current_ylim)\n",
        "\n",
        "    for j in range(num_plots, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    if hspace is not None or vspace is not None or subplot_margins:\n",
        "        margins = subplot_margins or {}\n",
        "        plt.subplots_adjust(\n",
        "            hspace=hspace or 0.3,  # Default horizontal spacing\n",
        "            wspace=vspace or 0.2,  # Default vertical spacing\n",
        "            left=margins.get('left', 0.1),\n",
        "            right=margins.get('right', 0.9),\n",
        "            top=margins.get('top', 0.9),\n",
        "            bottom=margins.get('bottom', 0.1)\n",
        "        )\n",
        "    else:\n",
        "        fig.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        print(f\"Saving plot to {plot_name}\")\n",
        "        plt.savefig(plot_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febb881b",
      "metadata": {},
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7df4d40",
      "metadata": {},
      "source": [
        "### Loss e Acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a663f1c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "  series = {\n",
        "      \"Class Loss Trial 1\": loaded_dict_mnist_class[\"net_loss_round\"],\n",
        "      \"Class Accuracy Trial 1\": loaded_dict_mnist_class[\"net_acc_round\"],\n",
        "      \"Dir01 Loss Trial 1\": loaded_dict_mnist_dir01[\"net_loss_round\"],\n",
        "      \"Dir01 Accuracy Trial 1\": loaded_dict_mnist_dir01[\"net_acc_round\"],\n",
        "      \"Dir05 Loss Trial 1\": loaded_dict_mnist_dir05[\"net_loss_round\"],\n",
        "      \"Dir05 Accuracy Trial 1\": loaded_dict_mnist_dir05[\"net_acc_round\"],\n",
        "      \"Class Loss Trial 2\": loaded_dict_mnist_class_trial2[\"net_loss_round\"],\n",
        "      \"Class Accuracy Trial 2\": loaded_dict_mnist_class_trial2[\"net_acc_round\"],\n",
        "      \"Dir01 Loss Trial 2\": loaded_dict_mnist_dir01_trial2[\"net_loss_round\"],\n",
        "      \"Dir01 Accuracy Trial 2\": loaded_dict_mnist_dir01_trial2[\"net_acc_round\"],\n",
        "      \"Dir05 Loss Trial 2\": loaded_dict_mnist_dir05_trial2[\"net_loss_round\"],\n",
        "      \"Dir05 Accuracy Trial 2\": loaded_dict_mnist_dir05_trial2[\"net_acc_round\"],\n",
        "  },\n",
        "  series_styles={\n",
        "      \"Class Loss Trial 1\": {\"color\": \"blue\", \"linestyle\": \"-\"},\n",
        "      \"Class Accuracy Trial 1\": {\"color\": \"blue\", \"linestyle\": \"--\"},\n",
        "      \"Dir01 Loss Trial 1\": {\"color\": \"orange\", \"linestyle\": \"-\"},\n",
        "      \"Dir01 Accuracy Trial 1\": {\"color\": \"orange\", \"linestyle\": \"--\"},\n",
        "      \"Dir05 Loss Trial 1\": {\"color\": \"green\", \"linestyle\": \"-\"},\n",
        "      \"Dir05 Accuracy Trial 1\": {\"color\": \"green\", \"linestyle\": \"--\"},\n",
        "      \"Class Loss Trial 2\": {\"color\": \"lightblue\", \"linestyle\": \"-\"},\n",
        "      \"Class Accuracy Trial 2\": {\"color\": \"lightblue\", \"linestyle\": \"--\"},\n",
        "      \"Dir01 Loss Trial 2\": {\"color\": \"yellow\", \"linestyle\": \"-\"},\n",
        "      \"Dir01 Accuracy Trial 2\": {\"color\": \"yellow\", \"linestyle\": \"--\"},\n",
        "      \"Dir05 Loss Trial 2\": {\"color\": \"lightgreen\", \"linestyle\": \"-\"},\n",
        "      \"Dir05 Accuracy Trial 2\": {\"color\": \"lightgreen\", \"linestyle\": \"--\"},\n",
        "  },\n",
        "  highlight = {\n",
        "      \"Accuracy\": \"max\"\n",
        "  },\n",
        "  highlight_markersize=4,\n",
        "  xtick_step=5,\n",
        "  first_step=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b0d95c",
      "metadata": {},
      "source": [
        "### Local Acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209bd071",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        # \"Global - Chunked FedAvg\": loaded_dict_cifar_mnist[\"net_acc_round\"][:100],\n",
        "        # \"Global - FedGenIA\": loaded_dict_cifar_mnist_gerafed[\"net_acc_round\"][:100],\n",
        "\n",
        "        \"Client 0 - Chunked FedAvg\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_chunked_fedavg.txt\"][0],\n",
        "        \"Client 0 - Chunked FedProx\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_chunked_fedprox.txt\"][0],\n",
        "        \"Client 0 - FedGenIA\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report.txt\"][0],\n",
        "        \"Client 0 - FedGenIA + FedProx\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_fedprox_fedgenia.txt\"][0],\n",
        "\n",
        "        \"Client 1 - Chunked FedAvg\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_chunked_fedavg.txt\"][1],\n",
        "        \"Client 1 - Chunked FedProx\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_chunked_fedprox.txt\"][1],\n",
        "        \"Client 1 - FedGenIA\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report.txt\"][1],\n",
        "        \"Client 1 - FedGenIA + FedProx\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_fedprox_fedgenia.txt\"][1],\n",
        "\n",
        "        \"Chunked FedAvg\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_chunked_fedavg.txt\"][2],\n",
        "        \"Chunked FedProx\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_chunked_fedprox.txt\"][2],\n",
        "        \"FedGenIA\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report.txt\"][2],\n",
        "        \"FedGenIA + FedProx\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_fedprox_fedgenia.txt\"][2],\n",
        "\n",
        "        \"Client 3 - Chunked FedAvg\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_chunked_fedavg.txt\"][3],\n",
        "        \"Client 3 - Chunked FedProx\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_chunked_fedprox.txt\"][3],\n",
        "        \"Client 3 - FedGenIA\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report.txt\"][3],\n",
        "        \"Client 3 - FedGenIA + FedProx\": local_acc_dict[\"CIFAR10_Dir01_Trial1_local_accuracy_report_fedprox_fedgenia.txt\"][3],\n",
        "    },\n",
        "    series_styles = {\n",
        "        # \"Global - Chunked FedAvg\": {\"color\": \"lightblue\", \"linestyle\": \"-\"},\n",
        "        # \"Global - FedGenIA\": {\"color\": \"lightblue\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Client 0 - Chunked FedAvg\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Client 0 - Chunked FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"-\"},\n",
        "        \"Client 0 - FedGenIA\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"Client 0 - FedGenIA + FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Client 1 - Chunked FedAvg\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Client 1 - Chunked FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"-\"},\n",
        "        \"Client 1 - FedGenIA\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"Client 1 - FedGenIA + FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Chunked FedAvg\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Chunked FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"-\"},\n",
        "        \"FedGenIA\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Client 3 - Chunked FedAvg\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Client 3 - Chunked FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"-\"},\n",
        "        \"Client 3 - FedGenIA\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"Client 3 - FedGenIA + FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "    },\n",
        "    subplot_groups=[\n",
        "        [\"Client 0 - Chunked FedAvg\", \"Client 0 - Chunked FedProx\", \"Client 0 - FedGenIA\", \"Client 0 - FedGenIA + FedProx\"],\n",
        "        [\"Client 1 - Chunked FedAvg\", \"Client 1 - Chunked FedProx\", \"Client 1 - FedGenIA\", \"Client 1 - FedGenIA + FedProx\"],\n",
        "        [\"Chunked FedAvg\", \"Chunked FedProx\", \"FedGenIA\", \"FedGenIA + FedProx\"],\n",
        "        [\"Client 3 - Chunked FedAvg\", \"Client 3 - Chunked FedProx\", \"Client 3 - FedGenIA\", \"Client 3 - FedGenIA + FedProx\"]\n",
        "    ],\n",
        "    highlight={\n",
        "    #    \"Global - Chunked FedAvg\": \"max\",\n",
        "    #     \"Global - FedGenIA\": \"max\",\n",
        "\n",
        "        \"Client 0 - Chunked FedAvg\": \"max\",\n",
        "        \"Client 0 - Chunked FedProx\": \"max\",\n",
        "        \"Client 0 - FedGenIA\": \"max\",\n",
        "        \"Client 0 - FedGenIA + FedProx\": \"max\",\n",
        "\n",
        "        \"Client 1 - Chunked FedAvg\": \"max\",\n",
        "        \"Client 1 - Chunked FedProx\": \"max\",\n",
        "        \"Client 1 - FedGenIA\": \"max\",\n",
        "        \"Client 1 - FedGenIA + FedProx\": \"max\",\n",
        "\n",
        "        \"Chunked FedAvg\": \"max\",\n",
        "        \"Chunked FedProx\": \"max\",\n",
        "        \"FedGenIA\": \"max\",\n",
        "        \"FedGenIA + FedProx\": \"max\",\n",
        "\n",
        "        \"Client 3 - Chunked FedAvg\": \"max\",\n",
        "        \"Client 3 - Chunked FedProx\": \"max\",\n",
        "        \"Client 3 - FedGenIA\": \"max\",\n",
        "        \"Client 3 - FedGenIA + FedProx\": \"max\",\n",
        "    },\n",
        "    subplot_layout=(1,4),\n",
        "    figsize=(20, 2.65),\n",
        "    label_fontsize=16,\n",
        "    tick_fontsize=15,\n",
        "    highlight_markersize=6,\n",
        "    num_xticks=5,\n",
        "    num_yticks=[0,0,0,3,],\n",
        "    y_ticks=[[0,0.4,0.75], [0,0.4,0.75], [0.1,0.4,0.65], None],\n",
        "    ylim=[(0,0.75),(0,0.75),(0.1,0.65),(0,0.6)],\n",
        "    ylabel= [\"Accuracy\",\"\",\"\",\"\"],\n",
        "    highlight_text_size=15,\n",
        "    legend_subplot_index=2,\n",
        "    legend_fontsize=11.6,\n",
        "    legend_kwargs={\n",
        "        \"ncol\": 2,\n",
        "        \"columnspacing\": 0.6,\n",
        "        \"handletextpad\": 0.2,\n",
        "        \"borderpad\": 0.2,\n",
        "        \"bbox_to_anchor\": (0.024, 0.33),\n",
        "        \"handlelength\": 1,\n",
        "        \"labelspacing\": 0.3\n",
        "    },\n",
        "    title_fontsize=18,\n",
        "    title=[\"a) Client 0\", \"b) Client 1\", \"c) Client 2\", \"d) Client 3\"],\n",
        "    save=True,\n",
        "    plot_name=\"../figures/local_acc.pdf\",\n",
        "    highlight_style={\n",
        "        # \"Global - Chunked FedAvg\": {\"color\": \"blue\"},\n",
        "        # \"Global - FedGenIA\": {\"color\": \"blue\"},\n",
        "\n",
        "        \"Client 0 - Chunked FedAvg\": {\"highlight_offset_max\": (-16, 0.01)},\n",
        "        \"Client 0 - Chunked FedProx\": {\"highlight_offset_max\": (0, -0.3)},\n",
        "        \"Client 0 - FedGenIA\": {\"highlight_offset_max\": (-20, -0.5)},\n",
        "        \"Client 0 - FedGenIA + FedProx\": {\"highlight_offset_max\": (-40, -0.05)},\n",
        "\n",
        "        \"Client 1 - Chunked FedAvg\": {\"highlight_offset_max\": (0, 0.06)},\n",
        "        \"Client 1 - Chunked FedProx\": {\"highlight_offset_max\": (-17, 0)},\n",
        "        \"Client 1 - FedGenIA\": {\"highlight_offset_max\": (0, -0.45)},\n",
        "        \"Client 1 - FedGenIA + FedProx\": {\"highlight_offset_max\": (-15, -0.05)},\n",
        "\n",
        "        \"Chunked FedAvg\": {\"highlight_offset_max\": (10, -0.25)},\n",
        "        \"Chunked FedProx\": {\"highlight_offset_max\": (20, -0.3)},\n",
        "        \"FedGenIA\": {\"highlight_offset_max\": (0, -0.24)},\n",
        "        \"FedGenIA + FedProx\": {\"highlight_offset_max\": (17, -0.21)},\n",
        "\n",
        "        \"Client 3 - Chunked FedAvg\": {\"highlight_offset_max\": (-15, -0.14)},\n",
        "        \"Client 3 - Chunked FedProx\": {\"highlight_offset_max\": (-25, 0.14)},\n",
        "        \"Client 3 - FedGenIA\": {\"highlight_offset_max\": (-10, -0.25)},\n",
        "        \"Client 3 - FedGenIA + FedProx\": {\"highlight_offset_max\": (-15, 0.02)},\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8952ba7e",
      "metadata": {},
      "source": [
        "### Different distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf794d8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series={\n",
        "        \"IID\": loaded_dicts[\"losses_iid_mnist_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Dir05\": loaded_dicts[\"MNIST_Dir05_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Dir01\": loaded_dicts[\"MNIST_Dir01_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"][:100],\n",
        "        \"Class\": loaded_dicts[\"MNIST_Class_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"][:100],\n",
        "        \"IID cifar\": loaded_dicts[\"losses_iid_cifar_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Dir05 cifar\": loaded_dicts[\"CIFAR10_Dir05_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Dir01 cifar\": loaded_dicts[\"CIFAR10_Dir01_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"][:100],\n",
        "        \"Class cifar\": loaded_dicts[\"CIFAR10_Class_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"][:100],\n",
        "    },\n",
        "    subplot_groups=[\n",
        "        [\"IID\", \"Dir05\", \"Dir01\", \"Class\"]\n",
        "        ,[\"IID cifar\", \"Dir05 cifar\", \"Dir01 cifar\", \"Class cifar\"],\n",
        "    ],\n",
        "    legend_subplot_index=0,\n",
        "    title=[\"a) MNIST\", \"b) CIFAR-10\"],\n",
        "    title_fontsize=20,\n",
        "    highlight={\n",
        "        \"IID\": \"max\",\n",
        "        \"Dir05\": \"max\",\n",
        "        \"Dir01\": \"max\",\n",
        "        \"Class\": \"max\",\n",
        "        \"IID cifar\": \"max\",\n",
        "        \"Dir05 cifar\": \"max\",\n",
        "        \"Dir01 cifar\": \"max\",\n",
        "        \"Class cifar\": \"max\",\n",
        "    },\n",
        "    highlight_style={\n",
        "        \"IID\": {\"highlight_offset_max\": (5, -0.07)},\n",
        "        \"Dir05\": {\"highlight_offset_max\": (5, -0.081)},\n",
        "        \"Dir01\": {\"highlight_offset_max\": (-5, -0.069)},\n",
        "        \"Class\": {\"highlight_offset_max\": (5, -0.25)},\n",
        "        \"IID cifar\": {\"highlight_offset_max\": (-5, -0.2)},\n",
        "        \"Dir05 cifar\": {\"highlight_offset_max\": (0, -0.16)},\n",
        "        \"Dir01 cifar\": {\"highlight_offset_max\": (5, 0.07)},\n",
        "        \"Class cifar\": {\"highlight_offset_max\": (10, -0.04)},\n",
        "     }, \n",
        "    highlight_markersize=8,\n",
        "    xtick_step=5,\n",
        "    num_yticks=[3,3],\n",
        "    #y_ticks=[[0,0.5,1], [0.2,0.45,0.65]],\n",
        "    first_step=4,\n",
        "    ylabel=\"Accuracy\",\n",
        "    figsize=(15, 5.9),\n",
        "    highlight_text_size=20,\n",
        "    tick_fontsize=20,\n",
        "    label_fontsize=20,\n",
        "    legend_fontsize=18,\n",
        "    legend_kwargs={\n",
        "        \"ncol\": 2,\n",
        "        \"columnspacing\": 1,\n",
        "        \"handletextpad\": 0.5,\n",
        "        \"borderpad\": 0.2,\n",
        "        \"bbox_to_anchor\": (0.014, 0.5),\n",
        "        \"handlelength\": 2,\n",
        "        \"labelspacing\": 0.5\n",
        "    },\n",
        "    save=True,\n",
        "    plot_name=\"../figures/ACC_alvo.pdf\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc847093",
      "metadata": {},
      "source": [
        "### GAN loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fca7c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series={\n",
        "        \"G_01\": loaded_dict_chunk01['g_losses_round'],\n",
        "        \"D_01\": loaded_dict_chunk01['d_losses_round'],\n",
        "        \"G_10\": loaded_dict_chunk10['g_losses_round'],\n",
        "        \"D_10\": loaded_dict_chunk10['d_losses_round'],\n",
        "        \"G_50\": loaded_dict_chunk50['g_losses_round'],\n",
        "        \"D_50\": loaded_dict_chunk50['d_losses_round'],\n",
        "        \"G_100\": loaded_dict_chunk100['g_losses_round'],\n",
        "        \"D_100\": loaded_dict_chunk100['d_losses_round'],\n",
        "        \"G_500\": loaded_dict_chunk500['g_losses_round'],\n",
        "        \"D_500\": loaded_dict_chunk500['d_losses_round'],\n",
        "        \"G_1000\": loaded_dict_chunk1000['g_losses_round'],\n",
        "        \"D_1000\": loaded_dict_chunk1000['d_losses_round'],\n",
        "        \"G_5000\": loaded_dict_chunk5000['g_losses_round'],\n",
        "        \"D_5000\": loaded_dict_chunk5000['d_losses_round'],\n",
        "\n",
        "    },\n",
        "    series_styles={\n",
        "        \"G_01\": {\"color\": \"blue\", \"linestyle\": \"-\"},\n",
        "        \"D_01\": {\"color\": \"blue\", \"linestyle\": \"--\"},\n",
        "        \"G_10\": {\"color\": \"orange\", \"linestyle\": \"-\"},\n",
        "        \"D_10\": {\"color\": \"orange\", \"linestyle\": \"--\"},\n",
        "        \"G_50\": {\"color\": \"green\", \"linestyle\": \"-\"},\n",
        "        \"D_50\": {\"color\": \"green\", \"linestyle\": \"--\"},\n",
        "        \"G_100\": {\"color\": \"red\", \"linestyle\": \"-\"},\n",
        "        \"D_100\": {\"color\": \"red\", \"linestyle\": \"--\"},\n",
        "        \"G_500\": {\"color\": \"purple\", \"linestyle\": \"-\"},\n",
        "        \"D_500\": {\"color\": \"purple\", \"linestyle\": \"--\"},\n",
        "        \"G_1000\": {\"color\": \"brown\", \"linestyle\": \"-\"},\n",
        "        \"D_1000\": {\"color\": \"brown\", \"linestyle\": \"--\"},\n",
        "        \"G_5000\": {\"color\": \"pink\", \"linestyle\": \"-\"},\n",
        "        \"D_5000\": {\"color\": \"pink\", \"linestyle\": \"--\"},\n",
        "\n",
        "    },\n",
        "    xtick_step=5,\n",
        "    first_step=4,\n",
        "    ylabel=\"Loss\",\n",
        "    ylim=(0,1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7768e0ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "for key in loaded_dicts.keys():\n",
        "    print(f\"{key}: {len(loaded_dicts[key]['net_acc_round'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b608dfdb",
      "metadata": {},
      "source": [
        "### GeraFed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a80e05",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        \"Chunked FedAvg\": loaded_dicts[\"MNIST_Class_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Chunked FedProx\": loaded_dicts[\"MNIST_Class_Trial1_metrics_chunked_fedprox.json\"][\"net_acc_round\"],\n",
        "        # \"Chunked SCAFFOLD\": loaded_dicts[\"MNIST_Class_Trial1_metrics_chunked_scaffold.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA\": loaded_dicts[\"MNIST_Class_Trial1_metrics.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA + FedProx\": loaded_dicts[\"MNIST_Class_Trial1_metrics_fedprox_fedgenia.json\"][\"net_acc_round\"],\n",
        "        # \"FedGenIA + SCAFFOLD mnistclass\": loaded_dicts[\"MNIST_Class_Trial1_metrics_scaffold_fedgenia.json\"][\"net_acc_round\"],\n",
        "\n",
        "        \"Chunked FedAvg cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Chunked FedProx cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics_chunked_fedprox.json\"][\"net_acc_round\"],\n",
        "        # \"Chunked SCAFFOLD cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics_chunked_scaffold.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial2_metrics.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA + FedProx cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics_fedprox_fedgenia.json\"][\"net_acc_round\"],\n",
        "        # \"FedGenIA + SCAFFOLD cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics_scaffold_fedgenia.json\"][\"net_acc_round\"],\n",
        "\n",
        "        \"Chunked FedAvg cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"][:100],\n",
        "        \"Chunked FedProx cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_metrics_chunked_fedprox.json\"][\"net_acc_round\"],\n",
        "        # \"Chunked SCAFFOLD cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_metrics_chunked_scaffold.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_metrics.json\"][\"net_acc_round\"][:100],\n",
        "        \"FedGenIA + FedProx cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_metrics_fedprox_fedgenia.json\"][\"net_acc_round\"],\n",
        "        # \"FedGenIA + SCAFFOLD cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_metrics_scaffold_fedgenia.json\"][\"net_acc_round\"],\n",
        "\n",
        "        \"Chunked FedAvg cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Chunked FedProx cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics_chunked_fedprox.json\"][\"net_acc_round\"],\n",
        "        # \"Chunked SCAFFOLD cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics_chunked_scaffold.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA + FedProx cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics_fedprox_fedgenia.json\"][\"net_acc_round\"],\n",
        "        # \"FedGenIA + SCAFFOLD cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics_scaffold_fedgenia.json\"][\"net_acc_round\"],\n",
        "        \"50chunks FedGenIA\": loaded_dicts[\".._.._cifar10_ClassPartitioner_fedavg_50chunks_fedgenia_metrics.json\"][\"net_acc_round\"]\n",
        "        },\n",
        "\n",
        "    series_styles={\n",
        "        \"Chunked FedAvg\": {\"color\": \"cornflowerblue\"},\n",
        "        \"Chunked FedProx\": {\"color\": \"sandybrown\"},\n",
        "        #\"Chunked SCAFFOLD\": {\"color\": \"yellowgreen\"},\n",
        "        \"FedGenIA\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        #\"FedGenIA + SCAFFOLD\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Chunked FedAvg cifardir05\": {\"color\": \"cornflowerblue\"},\n",
        "        \"Chunked FedProx cifardir05\": {\"color\": \"sandybrown\"},\n",
        "        # \"Chunked SCAFFOLD cifardir05\": {\"color\": \"yellowgreen\"},\n",
        "        \"FedGenIA cifardir05\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx cifardir05\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        #\"FedGenIA + SCAFFOLD cifardir05\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Chunked FedAvg cifardir01\": {\"color\": \"cornflowerblue\"},\n",
        "        \"Chunked FedProx cifardir01\": {\"color\": \"sandybrown\"},\n",
        "        # \"Chunked SCAFFOLD cifardir01\": {\"color\": \"yellowgreen\"},\n",
        "        \"FedGenIA cifardir01\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx cifardir01\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        # \"FedGenIA + SCAFFOLD cifardir01\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Chunked FedAvg cifarclass\": {\"color\": \"cornflowerblue\"},\n",
        "        \"Chunked FedProx cifarclass\": {\"color\": \"sandybrown\"},\n",
        "        #\"Chunked SCAFFOLD cifarclass\": {\"color\": \"yellowgreen\"},\n",
        "        \"FedGenIA cifarclass\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx cifarclass\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        #\"FedGenIA + SCAFFOLD cifarclass\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "        \"50chunks FedGenIA\": {\"color\": \"red\"}\n",
        "    },\n",
        "\n",
        "    subplot_groups=[\n",
        "                    [\"Chunked FedAvg\", \"Chunked FedProx\", \"Chunked SCAFFOLD\", \"FedGenIA\", \"FedGenIA + FedProx\", \"FedGenIA + SCAFFOLD\"],\n",
        "                    [\"Chunked FedAvg cifardir05\", \"Chunked FedProx cifardir05\", \"Chunked SCAFFOLD cifardir05\", \"FedGenIA cifardir05\", \"FedGenIA + FedProx cifardir05\", \"FedGenIA + SCAFFOLD cifardir05\"],\n",
        "                    [\"Chunked FedAvg cifardir01\", \"Chunked FedProx cifardir01\", \"Chunked SCAFFOLD cifardir01\", \"FedGenIA cifardir01\", \"FedGenIA + FedProx cifardir01\", \"FedGenIA + SCAFFOLD cifardir01\"],\n",
        "                    [\"Chunked FedAvg cifarclass\", \"Chunked FedProx cifarclass\", \"Chunked SCAFFOLD cifarclass\", \"FedGenIA cifarclass\", \"FedGenIA + FedProx cifarclass\", \"FedGenIA + SCAFFOLD cifarclass\", \"50chunks FedGenIA\"],\n",
        "                    ],\n",
        "    subplot_layout=(1,4),\n",
        "    figsize=(20,2.65),\n",
        "    highlight={\n",
        "        \"Chunked FedAvg\": \"max\",\n",
        "        \"Chunked FedProx\": \"max\",\n",
        "        #\"Chunked SCAFFOLD\": \"max\",\n",
        "        \"FedGenIA\": \"max\",\n",
        "        \"FedGenIA + FedProx\": \"max\",\n",
        "        #\"FedGenIA + SCAFFOLD\": \"max\",\n",
        "        \n",
        "        \"Chunked FedAvg cifardir05\": \"max\",\n",
        "        \"Chunked FedProx cifardir05\": \"max\",\n",
        "        # \"Chunked SCAFFOLD cifardir05\": \"max\",\n",
        "        \"FedGenIA cifardir05\": \"max\",\n",
        "        \"FedGenIA + FedProx cifardir05\": \"max\",\n",
        "        # \"FedGenIA + SCAFFOLD cifardir05\": \"max\",\n",
        "\n",
        "        \"Chunked FedAvg cifardir01\": \"max\",\n",
        "        \"Chunked FedProx cifardir01\": \"max\",\n",
        "        # \"Chunked SCAFFOLD cifardir01\": \"max\",\n",
        "        \"FedGenIA cifardir01\": \"max\",\n",
        "        \"FedGenIA + FedProx cifardir01\": \"max\",\n",
        "        # \"FedGenIA + SCAFFOLD cifardir01\": \"max\",\n",
        "\n",
        "        \"Chunked FedAvg cifarclass\": \"max\",\n",
        "        \"Chunked FedProx cifarclass\": \"max\",\n",
        "        # \"Chunked SCAFFOLD cifarclass\": \"max\",\n",
        "        \"FedGenIA cifarclass\": \"max\",\n",
        "        \"FedGenIA + FedProx cifarclass\": \"max\",\n",
        "        # \"FedGenIA + SCAFFOLD cifarclass\": \"max\",\n",
        "    },\n",
        "    highlight_style={\n",
        "        \"Chunked FedAvg\": {\"highlight_offset_max\": (-5, -0.40), 'arrow_color': 'cornflowerblue', 'arrow_style': '->', 'arrow_linewidth': 1},\n",
        "        \"Chunked FedProx\": {\"highlight_offset_max\": (8, -0.4), 'arrow_color': 'sandybrown', 'arrow_style': '->', 'arrow_linewidth': 1},\n",
        "        \"FedGenIA\": {\"highlight_offset_max\": (-7, -0.55), 'arrow_color': 'cornflowerblue', 'arrow_style': '->', 'arrow_linewidth': 1},\n",
        "        \"FedGenIA + FedProx\": {\"highlight_offset_max\": (-25, -0.45), 'arrow_color': 'sandybrown', 'arrow_style': '->', 'arrow_linewidth': 1},\n",
        "\n",
        "        \"Chunked FedAvg cifardir05\": {\"highlight_offset_max\": (5, -0.25),},\n",
        "        \"Chunked FedProx cifardir05\": {\"highlight_offset_max\": (-10, -0.35),},\n",
        "        \"FedGenIA cifardir05\": {\"highlight_offset_max\": (25, -0.20),},\n",
        "        \"FedGenIA + FedProx cifardir05\": {\"highlight_offset_max\": (0, -0.3)},\n",
        "\n",
        "        \"Chunked FedAvg cifardir01\": {\"highlight_offset_max\": (-5, -0.12)},\n",
        "        \"Chunked FedProx cifardir01\": {\"highlight_offset_max\": (-16, 0.02)},\n",
        "        \"FedGenIA cifardir01\": {\"highlight_offset_max\": (0, -0.2)},\n",
        "        \"FedGenIA + FedProx cifardir01\": {\"highlight_offset_max\": (0, -0.25)},\n",
        "\n",
        "        \"Chunked FedAvg cifarclass\": {\"highlight_offset_max\": (7, -0.27)},\n",
        "        \"Chunked FedProx cifarclass\": {\"highlight_offset_max\": (30, 0)},\n",
        "        \"FedGenIA cifarclass\": {\"highlight_offset_max\": (6.5, 0.1)},\n",
        "        \"FedGenIA + FedProx cifarclass\": {\"highlight_offset_max\": (20, 0)},\n",
        "     }, \n",
        "    num_xticks=5,\n",
        "    num_yticks=[None,None,3,3],\n",
        "    y_ticks=[[0,0.5,1], [0.2,0.45,0.65], None, None],\n",
        "    ylim=[(0,1.05), (0.2,0.65), (0.1, 0.6), (0,0.5)],\n",
        "    ylabel=[\"Accuracy\",\"\",\"\",\"\"],\n",
        "    title=[\"a) MNIST Class\", \"b) CIFAR Dir05\", \"c) CIFAR Dir01\", \"d) CIFAR Class\"],\n",
        "    legend_subplot_index=0,\n",
        "    label_fontsize=16,\n",
        "    tick_fontsize=15,\n",
        "    highlight_markersize=6,\n",
        "    highlight_text_size=15,\n",
        "    legend_fontsize=13,\n",
        "    legend_kwargs={\n",
        "        \"ncol\": 2,\n",
        "        \"columnspacing\": 0.4,\n",
        "        \"handletextpad\": 0.2,\n",
        "        \"borderpad\": 0.1,\n",
        "        \"bbox_to_anchor\": (1.0114, 0.4),\n",
        "        \"handlelength\": 1,\n",
        "        \"labelspacing\": 0.3\n",
        "    },\n",
        "    title_fontsize=18,\n",
        "    save=False,\n",
        "    plot_name=\"./FedGenIAxbaselines.pdf\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d0079b",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        \"Chunked FedAvg\": loaded_dicts[\"MNIST_Class_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Chunked FedProx\": loaded_dicts[\"MNIST_Class_Trial1_metrics_chunked_fedprox.json\"][\"net_acc_round\"],\n",
        "        # \"Chunked SCAFFOLD\": loaded_dicts[\"MNIST_Class_Trial1_metrics_chunked_scaffold.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA\": loaded_dicts[\"MNIST_Class_Trial1_metrics.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA + FedProx\": loaded_dicts[\"MNIST_Class_Trial1_metrics_fedprox_fedgenia.json\"][\"net_acc_round\"],\n",
        "        # \"FedGenIA + SCAFFOLD mnistclass\": loaded_dicts[\"MNIST_Class_Trial1_metrics_scaffold_fedgenia.json\"][\"net_acc_round\"],\n",
        "\n",
        "        \"Chunked FedAvg cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_losses_chunked_fedavg.json\"][\"net_acc_round\"],\n",
        "        \"Chunked FedProx cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics_chunked_fedprox.json\"][\"net_acc_round\"],\n",
        "        # \"Chunked SCAFFOLD cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics_chunked_scaffold.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial2_metrics.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA + FedProx cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics_fedprox_fedgenia.json\"][\"net_acc_round\"],\n",
        "        # \"FedGenIA + SCAFFOLD cifardir05\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics_scaffold_fedgenia.json\"][\"net_acc_round\"],\n",
        "\n",
        "        \"Chunked FedAvg cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_fedavg_50chunks_metrics.json\"][\"net_acc_round\"][:100],\n",
        "        \"Chunked FedProx cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_fedprox_50chunks_metrics.json\"][\"net_acc_round\"],\n",
        "        # \"Chunked SCAFFOLD cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_metrics_chunked_scaffold.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_fedgenia_fedavg_50chunks_metrics.json\"][\"net_acc_round\"][:100],\n",
        "        \"FedGenIA + FedProx cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_fedgenia_fedprox_50chunks_metrics.json\"][\"net_acc_round\"],\n",
        "        # \"FedGenIA + SCAFFOLD cifardir01\": loaded_dicts[\"CIFAR10_Dir01_Trial1_metrics_scaffold_fedgenia.json\"][\"net_acc_round\"],\n",
        "\n",
        "        \"Chunked FedAvg cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_fedavg_50chunks_metrics.json\"][\"net_acc_round\"],\n",
        "        \"Chunked FedProx cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_fedprox_50chunks_metrics.json\"][\"net_acc_round\"],\n",
        "        # \"Chunked SCAFFOLD cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics_chunked_scaffold.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_fedavg_fedgenia_50chunks_metrics.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA + FedProx cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_fedgenia_fedprox_50chunks_metrics.json\"][\"net_acc_round\"],\n",
        "        # \"FedGenIA + SCAFFOLD cifarclass\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics_scaffold_fedgenia.json\"][\"net_acc_round\"],\n",
        "        },\n",
        "\n",
        "    series_styles={\n",
        "        \"Chunked FedAvg\": {\"color\": \"cornflowerblue\"},\n",
        "        \"Chunked FedProx\": {\"color\": \"sandybrown\"},\n",
        "        #\"Chunked SCAFFOLD\": {\"color\": \"yellowgreen\"},\n",
        "        \"FedGenIA\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        #\"FedGenIA + SCAFFOLD\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Chunked FedAvg cifardir05\": {\"color\": \"cornflowerblue\"},\n",
        "        \"Chunked FedProx cifardir05\": {\"color\": \"sandybrown\"},\n",
        "        # \"Chunked SCAFFOLD cifardir05\": {\"color\": \"yellowgreen\"},\n",
        "        \"FedGenIA cifardir05\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx cifardir05\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        #\"FedGenIA + SCAFFOLD cifardir05\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Chunked FedAvg cifardir01\": {\"color\": \"cornflowerblue\"},\n",
        "        \"Chunked FedProx cifardir01\": {\"color\": \"sandybrown\"},\n",
        "        # \"Chunked SCAFFOLD cifardir01\": {\"color\": \"yellowgreen\"},\n",
        "        \"FedGenIA cifardir01\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx cifardir01\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        # \"FedGenIA + SCAFFOLD cifardir01\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "\n",
        "        \"Chunked FedAvg cifarclass\": {\"color\": \"cornflowerblue\"},\n",
        "        \"Chunked FedProx cifarclass\": {\"color\": \"sandybrown\"},\n",
        "        #\"Chunked SCAFFOLD cifarclass\": {\"color\": \"yellowgreen\"},\n",
        "        \"FedGenIA cifarclass\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"FedGenIA + FedProx cifarclass\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        #\"FedGenIA + SCAFFOLD cifarclass\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "        \"50chunks FedGenIA\": {\"color\": \"red\"}\n",
        "    },\n",
        "\n",
        "    subplot_groups=[\n",
        "                    [\"Chunked FedAvg\", \"Chunked FedProx\", \"Chunked SCAFFOLD\", \"FedGenIA\", \"FedGenIA + FedProx\", \"FedGenIA + SCAFFOLD\"],\n",
        "                    [\"Chunked FedAvg cifardir05\", \"Chunked FedProx cifardir05\", \"Chunked SCAFFOLD cifardir05\", \"FedGenIA cifardir05\", \"FedGenIA + FedProx cifardir05\", \"FedGenIA + SCAFFOLD cifardir05\"],\n",
        "                    [\"Chunked FedAvg cifardir01\", \"Chunked FedProx cifardir01\", \"Chunked SCAFFOLD cifardir01\", \"FedGenIA cifardir01\", \"FedGenIA + FedProx cifardir01\", \"FedGenIA + SCAFFOLD cifardir01\"],\n",
        "                    [\"Chunked FedAvg cifarclass\", \"Chunked FedProx cifarclass\", \"Chunked SCAFFOLD cifarclass\", \"FedGenIA cifarclass\", \"FedGenIA + FedProx cifarclass\", \"FedGenIA + SCAFFOLD cifarclass\", \"50chunks FedGenIA\"],\n",
        "                    ],\n",
        "    subplot_layout=(1,4),\n",
        "    figsize=(20,2.65),\n",
        "    highlight={\n",
        "        \"Chunked FedAvg\": \"max\",\n",
        "        \"Chunked FedProx\": \"max\",\n",
        "        #\"Chunked SCAFFOLD\": \"max\",\n",
        "        \"FedGenIA\": \"max\",\n",
        "        \"FedGenIA + FedProx\": \"max\",\n",
        "        #\"FedGenIA + SCAFFOLD\": \"max\",\n",
        "        \n",
        "        \"Chunked FedAvg cifardir05\": \"max\",\n",
        "        \"Chunked FedProx cifardir05\": \"max\",\n",
        "        # \"Chunked SCAFFOLD cifardir05\": \"max\",\n",
        "        \"FedGenIA cifardir05\": \"max\",\n",
        "        \"FedGenIA + FedProx cifardir05\": \"max\",\n",
        "        # \"FedGenIA + SCAFFOLD cifardir05\": \"max\",\n",
        "\n",
        "        \"Chunked FedAvg cifardir01\": \"max\",\n",
        "        \"Chunked FedProx cifardir01\": \"max\",\n",
        "        # \"Chunked SCAFFOLD cifardir01\": \"max\",\n",
        "        \"FedGenIA cifardir01\": \"max\",\n",
        "        \"FedGenIA + FedProx cifardir01\": \"max\",\n",
        "        # \"FedGenIA + SCAFFOLD cifardir01\": \"max\",\n",
        "\n",
        "        \"Chunked FedAvg cifarclass\": \"max\",\n",
        "        \"Chunked FedProx cifarclass\": \"max\",\n",
        "        # \"Chunked SCAFFOLD cifarclass\": \"max\",\n",
        "        \"FedGenIA cifarclass\": \"max\",\n",
        "        \"FedGenIA + FedProx cifarclass\": \"max\",\n",
        "        # \"FedGenIA + SCAFFOLD cifarclass\": \"max\",\n",
        "    },\n",
        "    highlight_style={\n",
        "        \"Chunked FedAvg\": {\"highlight_offset_max\": (-5, -0.40), 'arrow_color': 'cornflowerblue', 'arrow_style': '->', 'arrow_linewidth': 1},\n",
        "        \"Chunked FedProx\": {\"highlight_offset_max\": (8, -0.4), 'arrow_color': 'sandybrown', 'arrow_style': '->', 'arrow_linewidth': 1},\n",
        "        \"FedGenIA\": {\"highlight_offset_max\": (-7, -0.55), 'arrow_color': 'cornflowerblue', 'arrow_style': '->', 'arrow_linewidth': 1},\n",
        "        \"FedGenIA + FedProx\": {\"highlight_offset_max\": (-25, -0.45), 'arrow_color': 'sandybrown', 'arrow_style': '->', 'arrow_linewidth': 1},\n",
        "\n",
        "        \"Chunked FedAvg cifardir05\": {\"highlight_offset_max\": (5, -0.25),},\n",
        "        \"Chunked FedProx cifardir05\": {\"highlight_offset_max\": (-10, -0.35),},\n",
        "        \"FedGenIA cifardir05\": {\"highlight_offset_max\": (25, -0.20),},\n",
        "        \"FedGenIA + FedProx cifardir05\": {\"highlight_offset_max\": (0, -0.3)},\n",
        "\n",
        "        \"Chunked FedAvg cifardir01\": {\"highlight_offset_max\": (-5, -0.12)},\n",
        "        \"Chunked FedProx cifardir01\": {\"highlight_offset_max\": (-16, 0.02)},\n",
        "        \"FedGenIA cifardir01\": {\"highlight_offset_max\": (0, -0.2)},\n",
        "        \"FedGenIA + FedProx cifardir01\": {\"highlight_offset_max\": (0, -0.25)},\n",
        "\n",
        "        \"Chunked FedAvg cifarclass\": {\"highlight_offset_max\": (7, -0.27)},\n",
        "        \"Chunked FedProx cifarclass\": {\"highlight_offset_max\": (30, 0)},\n",
        "        \"FedGenIA cifarclass\": {\"highlight_offset_max\": (6.5, 0.1)},\n",
        "        \"FedGenIA + FedProx cifarclass\": {\"highlight_offset_max\": (20, 0)},\n",
        "     }, \n",
        "    num_xticks=5,\n",
        "    num_yticks=[None,None,3,3],\n",
        "    y_ticks=[[0,0.5,1], [0.2,0.45,0.65], None, None],\n",
        "    ylim=[(0,1.05), (0.2,0.65), (0.1, 0.6), (0,0.5)],\n",
        "    ylabel=[\"Accuracy\",\"\",\"\",\"\"],\n",
        "    title=[\"a) MNIST Class\", \"b) CIFAR Dir05\", \"c) CIFAR Dir01\", \"d) CIFAR Class\"],\n",
        "    legend_subplot_index=0,\n",
        "    label_fontsize=16,\n",
        "    tick_fontsize=15,\n",
        "    highlight_markersize=6,\n",
        "    highlight_text_size=15,\n",
        "    legend_fontsize=13,\n",
        "    legend_kwargs={\n",
        "        \"ncol\": 2,\n",
        "        \"columnspacing\": 0.4,\n",
        "        \"handletextpad\": 0.2,\n",
        "        \"borderpad\": 0.1,\n",
        "        \"bbox_to_anchor\": (1.0114, 0.4),\n",
        "        \"handlelength\": 1,\n",
        "        \"labelspacing\": 0.3\n",
        "    },\n",
        "    title_fontsize=18,\n",
        "    save=False,\n",
        "    plot_name=\"./FedGenIAxbaselines.pdf\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e874a0",
      "metadata": {},
      "source": [
        "#### Aggregated Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eece0598",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        \"MNIST Class\": experiment_stats[\"MNIST_Class\"][\"net_acc_round\"][\"std\"],\n",
        "        \"CIFAR Dir05\": experiment_stats[\"CIFAR10_Dir05\"][\"net_acc_round\"][\"std\"],\n",
        "        \"CIFAR Dir01\": experiment_stats[\"CIFAR10_Dir01\"][\"net_acc_round\"][\"std\"],\n",
        "        \"CIFAR Class\": experiment_stats[\"CIFAR10_Class\"][\"net_acc_round\"][\"std\"],\n",
        "        },\n",
        "    # series_styles={\n",
        "    #     \"Chunked FedAvg cifarclass\": {\"color\": \"cornflowerblue\"},\n",
        "    #     \"FedGenIA cifarclass\": {\"color\": \"sandybrown\"},\n",
        "    #     \"Chunked FedAvg cifardir05\": {\"color\": \"cornflowerblue\"},\n",
        "    #     \"FedGenIA cifardir05\": {\"color\": \"sandybrown\"},\n",
        "    #     \"Chunked FedAvg cifardir01\": {\"color\": \"cornflowerblue\"},\n",
        "    #     \"FedGenIA cifardir01\": {\"color\": \"sandybrown\"},\n",
        "    #     \"Chunked FedAvg\": {\"color\": \"cornflowerblue\"},\n",
        "    #     \"FedGenIA\": {\"color\": \"sandybrown\"},\n",
        "    # },\n",
        "    # subplot_groups=[\n",
        "    #                 [\"Chunked FedAvg\", \"FedGenIA\"],\n",
        "    #                 [\"Chunked FedAvg cifardir05\", \"FedGenIA cifardir05\"],\n",
        "    #                 [\"Chunked FedAvg cifardir01\", \"FedGenIA cifardir01\"],\n",
        "    #                 [\"Chunked FedAvg cifarclass\", \"FedGenIA cifarclass\"],],\n",
        "    #subplot_layout=(1,4),\n",
        "    figsize=(16,8),\n",
        "    # highlight={\n",
        "    #     \"Chunked FedAvg cifarclass\": \"max\",\n",
        "    #     \"FedGenIA cifarclass\": \"max\",\n",
        "    #     \"Chunked FedAvg cifardir05\": \"max\",\n",
        "    #     \"FedGenIA cifardir05\": \"max\",\n",
        "    #     \"Chunked FedAvg cifardir01\": \"max\",\n",
        "    #     \"FedGenIA cifardir01\": \"max\",\n",
        "    #     \"Chunked FedAvg\": \"max\",\n",
        "    #     \"FedGenIA\": \"max\",\n",
        "    # },\n",
        "    # highlight_style={\n",
        "    #     \"Chunked FedAvg cifarclass\": {\"highlight_offset_max\": (20, 0.02)},\n",
        "    #     \"FedGenIA cifarclass\": {\"highlight_offset_max\": (-15, 0.005)},\n",
        "    #     \"Chunked FedAvg cifardir05\": {\"highlight_offset_max\": (0, -0.23)},\n",
        "    #     \"FedGenIA cifardir05\": {\"highlight_offset_max\": (-5, -0.22)},\n",
        "    #     \"Chunked FedAvg cifardir01\": {\"highlight_offset_max\": (-5, -0.17)},\n",
        "    #     \"FedGenIA cifardir01\": {\"highlight_offset_max\": (-5, -0.16)},\n",
        "    #     \"Chunked FedAvg\": {\"highlight_offset_max\": (5, -0.35)},\n",
        "    #     \"FedGenIA\": {\"highlight_offset_max\": (-10, -0.4)},\n",
        "    #  }, \n",
        "    # num_xticks=5,\n",
        "    num_yticks=3,\n",
        "    ylabel=[\"Accuracy\"],\n",
        "    # title=[\"a) MNIST Class\", \"b) CIFAR Dir05\", \"c) CIFAR Dir01\", \"d) CIFAR Class\"],\n",
        "    legend_subplot_index=0,\n",
        "    label_fontsize=16,\n",
        "    tick_fontsize=15,\n",
        "    # highlight_markersize=6,\n",
        "    # highlight_text_size=14,\n",
        "    legend_fontsize=12,\n",
        "    title_fontsize=18,\n",
        "    xtick_step=10,\n",
        "    first_step=9\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f94216",
      "metadata": {},
      "source": [
        "#### Comparing Trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a835bce",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        \"Trial1\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics.json\"][\"net_acc_round\"],\n",
        "        \"Trial2\": loaded_dicts[\"CIFAR10_Class_Trial2_metrics.json\"][\"net_acc_round\"],\n",
        "        \"Trial3\": loaded_dicts[\"CIFAR10_Class_Trial3_metrics.json\"][\"net_acc_round\"]\n",
        "        },\n",
        "    series_styles={\n",
        "        \"FedGenIA com agregação por chunk\": {\"color\": \"cornflowerblue\"},\n",
        "        \"FedGenIA com agregação só com primeiro chunk\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"Chunked FedAvg\": {\"color\": \"sandybrown\"},\n",
        "        \"CIFAR10 Class metrics\": {\"color\": \"cornflowerblue\"},\n",
        "        \"CIFAR10 Class losses\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"CIFAR10 Class alvo\": {\"color\": \"sandybrown\"},\n",
        "        \"CIFAR10 Dir01 metrics\": {\"color\": \"cornflowerblue\"},\n",
        "        \"CIFAR10 Dir01 losses\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"CIFAR10 Dir01 alvo\": {\"color\": \"sandybrown\"},\n",
        "        \"CIFAR10 Dir05 metrics\": {\"color\": \"cornflowerblue\"},\n",
        "        \"CIFAR10 Dir05 losses\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"CIFAR10 Dir05 alvo\": {\"color\": \"sandybrown\"},\n",
        "    },\n",
        "    subplot_layout=(1,1),\n",
        "    figsize=(20,10),\n",
        "    # highlight={\n",
        "    #     \"Chunked FedAvg cifarclass\": \"max\",\n",
        "    #     \"FedGenIA cifarclass\": \"max\",\n",
        "    #     \"Chunked FedAvg cifardir05\": \"max\",\n",
        "    #     \"FedGenIA cifardir05\": \"max\",\n",
        "    #     \"Chunked FedAvg cifardir01\": \"max\",\n",
        "    #     \"FedGenIA cifardir01\": \"max\",\n",
        "    #     \"Chunked FedAvg\": \"max\",\n",
        "    #     \"FedGenIA\": \"max\",\n",
        "    # },\n",
        "    # highlight_style={\n",
        "    #     \"Chunked FedAvg cifarclass\": {\"highlight_offset_max\": (20, 0.02)},\n",
        "    #     \"FedGenIA cifarclass\": {\"highlight_offset_max\": (-15, 0.005)},\n",
        "    #     \"Chunked FedAvg cifardir05\": {\"highlight_offset_max\": (0, -0.23)},\n",
        "    #     \"FedGenIA cifardir05\": {\"highlight_offset_max\": (-5, -0.22)},\n",
        "    #     \"Chunked FedAvg cifardir01\": {\"highlight_offset_max\": (-5, -0.17)},\n",
        "    #     \"FedGenIA cifardir01\": {\"highlight_offset_max\": (-5, -0.16)},\n",
        "    #     \"Chunked FedAvg\": {\"highlight_offset_max\": (5, -0.35)},\n",
        "    #     \"FedGenIA\": {\"highlight_offset_max\": (-10, -0.4)},\n",
        "    #  }, \n",
        "    # num_xticks=5,\n",
        "    num_yticks=4,\n",
        "    ylabel=[\"Accuracy\"],\n",
        "    title=[\"\"],\n",
        "    legend_subplot_index=0,\n",
        "    label_fontsize=16,\n",
        "    tick_fontsize=15,\n",
        "    highlight_markersize=6,\n",
        "    highlight_text_size=14,\n",
        "    legend_fontsize=12,\n",
        "    title_fontsize=18,\n",
        "    first_step=9,\n",
        "    xtick_step=10,\n",
        "    save=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0637fcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        \"CIFAR10 Class metrics\": loaded_dicts[\"CIFAR10_Class_Trial1_metrics.json\"][\"net_acc_round\"],\n",
        "        \"CIFAR10 Class losses\": loaded_dicts[\"CIFAR10_Class_Trial1_losses.json\"][\"net_acc_round\"],\n",
        "        \"CIFAR10 Class alvo\": loaded_dicts[\"Alvo_4c_NIIDClass_CIFAR_losses.json\"][\"net_acc_round\"],\n",
        "        \"CIFAR10 Dir01 metrics\": loaded_dicts[\"CIFAR10_Dir01_Trial1_metrics.json\"][\"net_acc_round\"],\n",
        "        \"CIFAR10 Dir01 losses\": loaded_dicts[\"CIFAR10_Dir01_Trial1_losses.json\"][\"net_acc_round\"][:100],\n",
        "        \"CIFAR10 Dir01 alvo\": loaded_dicts[\"Alvo_4c_01Dir_CIFAR_losses.json\"][\"net_acc_round\"][:100],\n",
        "        \"CIFAR10 Dir05 metrics\": loaded_dicts[\"CIFAR10_Dir05_Trial1_metrics.json\"][\"net_acc_round\"],\n",
        "        \"CIFAR10 Dir05 losses\": loaded_dicts[\"CIFAR10_Dir05_Trial1_losses.json\"][\"net_acc_round\"],\n",
        "        \"CIFAR10 Dir05 alvo\": loaded_dicts[\"Alvo_4c_05Dir_CIFAR_losses.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA com agregação por chunk\": loaded_dicts[\"MNIST_Class_Trial1_metrics.json\"][\"net_acc_round\"],\n",
        "        \"FedGenIA com agregação só com primeiro chunk\": loaded_dicts[\"MNIST_Class_Trial1_losses.json\"][\"net_acc_round\"],\n",
        "        \"Chunked FedAvg\": loaded_dicts[\"Alvo_4c_NIIDClass_MNIST_losses.json\"][\"net_acc_round\"]\n",
        "        },\n",
        "    series_styles={\n",
        "        \"FedGenIA com agregação por chunk\": {\"color\": \"cornflowerblue\"},\n",
        "        \"FedGenIA com agregação só com primeiro chunk\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"Chunked FedAvg\": {\"color\": \"sandybrown\"},\n",
        "        \"CIFAR10 Class metrics\": {\"color\": \"cornflowerblue\"},\n",
        "        \"CIFAR10 Class losses\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"CIFAR10 Class alvo\": {\"color\": \"sandybrown\"},\n",
        "        \"CIFAR10 Dir01 metrics\": {\"color\": \"cornflowerblue\"},\n",
        "        \"CIFAR10 Dir01 losses\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"CIFAR10 Dir01 alvo\": {\"color\": \"sandybrown\"},\n",
        "        \"CIFAR10 Dir05 metrics\": {\"color\": \"cornflowerblue\"},\n",
        "        \"CIFAR10 Dir05 losses\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"CIFAR10 Dir05 alvo\": {\"color\": \"sandybrown\"},\n",
        "    },\n",
        "    subplot_groups=[\n",
        "                    [\"FedGenIA com agregação por chunk\", \"FedGenIA com agregação só com primeiro chunk\", \"Chunked FedAvg\"],\n",
        "                    [\"CIFAR10 Class metrics\", \"CIFAR10 Class losses\", \"CIFAR10 Class alvo\"],\n",
        "                    [\"CIFAR10 Dir01 metrics\", \"CIFAR10 Dir01 losses\", \"CIFAR10 Dir01 alvo\"],\n",
        "                    [\"CIFAR10 Dir05 metrics\", \"CIFAR10 Dir05 losses\", \"CIFAR10 Dir05 alvo\"],\n",
        "                    ],\n",
        "    subplot_layout=(2,2),\n",
        "    figsize=(20,10),\n",
        "    # highlight={\n",
        "    #     \"Chunked FedAvg cifarclass\": \"max\",\n",
        "    #     \"FedGenIA cifarclass\": \"max\",\n",
        "    #     \"Chunked FedAvg cifardir05\": \"max\",\n",
        "    #     \"FedGenIA cifardir05\": \"max\",\n",
        "    #     \"Chunked FedAvg cifardir01\": \"max\",\n",
        "    #     \"FedGenIA cifardir01\": \"max\",\n",
        "    #     \"Chunked FedAvg\": \"max\",\n",
        "    #     \"FedGenIA\": \"max\",\n",
        "    # },\n",
        "    # highlight_style={\n",
        "    #     \"Chunked FedAvg cifarclass\": {\"highlight_offset_max\": (20, 0.02)},\n",
        "    #     \"FedGenIA cifarclass\": {\"highlight_offset_max\": (-15, 0.005)},\n",
        "    #     \"Chunked FedAvg cifardir05\": {\"highlight_offset_max\": (0, -0.23)},\n",
        "    #     \"FedGenIA cifardir05\": {\"highlight_offset_max\": (-5, -0.22)},\n",
        "    #     \"Chunked FedAvg cifardir01\": {\"highlight_offset_max\": (-5, -0.17)},\n",
        "    #     \"FedGenIA cifardir01\": {\"highlight_offset_max\": (-5, -0.16)},\n",
        "    #     \"Chunked FedAvg\": {\"highlight_offset_max\": (5, -0.35)},\n",
        "    #     \"FedGenIA\": {\"highlight_offset_max\": (-10, -0.4)},\n",
        "    #  }, \n",
        "    # num_xticks=5,\n",
        "    num_yticks=4,\n",
        "    ylabel=[\"Accuracy\", \"Accuracy\", \"Accuracy\", \"Accuracy\"],\n",
        "    title=[\"MNIST Class\", \"CIFAR Class\", \"CIFAR Dir01\", \"CIFAR Dir05\"],\n",
        "    legend_subplot_index=0,\n",
        "    label_fontsize=16,\n",
        "    tick_fontsize=15,\n",
        "    highlight_markersize=6,\n",
        "    highlight_text_size=14,\n",
        "    legend_fontsize=12,\n",
        "    title_fontsize=18,\n",
        "    first_step=9,\n",
        "    xtick_step=10,\n",
        "    save=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af29f8a9",
      "metadata": {},
      "source": [
        "### Optim Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "933a5a75",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        \"Adam_loss\": loaded_dict_mnist[\"net_loss_round\"],\n",
        "        \"Adam_GeraFed_loss\": loaded_dict_mnist_gerafed[\"net_loss_round\"],\n",
        "        \"Adam_reiniciando_loss\": loaded_dict_mnist_adam_reiniciando[\"net_loss_round\"],\n",
        "        \"Adam_reiniciando_GeraFed_loss\": loaded_dict_mnist_adam_reiniciando_gerafed[\"net_loss_round\"],\n",
        "        \"SGD_loss\": loaded_dict_mnist_sgd[\"net_loss_round\"],\n",
        "        \"SGD_GeraFed_loss\": loaded_dict_mnist_sgd_gerafed[\"net_loss_round\"],\n",
        "        \"Adam\": loaded_dict_mnist[\"net_acc_round\"],\n",
        "        \"Adam_GeraFed\": loaded_dict_mnist_gerafed[\"net_acc_round\"],\n",
        "        \"Adam_reiniciando\": loaded_dict_mnist_adam_reiniciando[\"net_acc_round\"],\n",
        "        \"Adam_reiniciando_GeraFed\": loaded_dict_mnist_adam_reiniciando_gerafed[\"net_acc_round\"],\n",
        "        \"SGD\": loaded_dict_mnist_sgd[\"net_acc_round\"],\n",
        "        \"SGD_GeraFed\": loaded_dict_mnist_sgd_gerafed[\"net_acc_round\"],\n",
        "    },\n",
        "    subplot_groups=[\n",
        "        [\"Adam_loss\", \"Adam_GeraFed_loss\", \"Adam_reiniciando_loss\", \"Adam_reiniciando_GeraFed_loss\", \"SGD_loss\", \"SGD_GeraFed_loss\"],\n",
        "        [\"Adam\", \"Adam_GeraFed\", \"Adam_reiniciando\", \"Adam_reiniciando_GeraFed\", \"SGD\", \"SGD_GeraFed\"]\n",
        "    ],\n",
        "    subplot_layout=(2, 1),\n",
        "    series_styles={\n",
        "        \"Adam_loss\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Adam_GeraFed_loss\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"Adam_reiniciando_loss\": {\"color\": \"darkturquoise\", \"linestyle\": \"-\"},\n",
        "        \"Adam_reiniciando_GeraFed_loss\": {\"color\": \"darkturquoise\", \"linestyle\": \"--\"},\n",
        "        \"SGD_loss\": {\"color\": \"yellowgreen\", \"linestyle\": \"-\"},\n",
        "        \"SGD_GeraFed_loss\": {\"color\": \"yellowgreen\", \"linestyle\": \"--\"},\n",
        "        \"Adam\": {\"color\": \"cornflowerblue\",  \"linestyle\": \"-\"},\n",
        "        \"Adam_GeraFed\": {\"color\": \"cornflowerblue\",  \"linestyle\": \"--\"},\n",
        "        \"Adam_reiniciando\": {\"color\": \"darkturquoise\",  \"linestyle\": \"-\"},\n",
        "        \"Adam_reiniciando_GeraFed\": {\"color\": \t\"darkturquoise\", \t\"linestyle\": \"--\"},\n",
        "        \"SGD\": {\"color\": \t\"yellowgreen\", \t\"linestyle\": \"-\"},\n",
        "        \"SGD_GeraFed\": {\"color\": \t\"yellowgreen\", \t\"linestyle\": \"--\"},\n",
        "    },\n",
        "    xlabel=[\"Épocas\", \"\"],\n",
        "    ylabel=[\"Loss\", \"Acurácia\"],\n",
        "    legend_subplot_index=1,\n",
        "    xtick_step=10,\n",
        "    first_step=9,\n",
        "    figsize=(8, 8),\n",
        "    legend_fontsize=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d902d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series={\"Class Adam\": loaded_dict_cifar_class[\"net_acc_round\"],\n",
        "            \"Dir01 Adam\": loaded_dict_cifar_dir01[\"net_acc_round\"][:100],\n",
        "            \"Dir05 Adam\": loaded_dict_cifar_dir05[\"net_acc_round\"],\n",
        "            \"Class SGD\": loaded_dict_cifar_class_sgd[\"net_acc_round\"],\n",
        "            \"Dir01 SGD\": loaded_dict_cifar_dir01_sgd[\"net_acc_round\"],\n",
        "            \"Dir05 SGD\": loaded_dict_cifar_dir05_sgd[\"net_acc_round\"],\n",
        "    },\n",
        "    series_styles={\n",
        "        \"Class Adam\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Dir01 Adam\": {\"color\": \"cornflowerblue\", \"linestyle\": \"--\"},\n",
        "        \"Dir05 Adam\": {\"color\": \"cornflowerblue\", \"linestyle\": \":\"},\n",
        "        \"Class SGD\": {\"color\": \"sandybrown\", \"linestyle\": \"-\"},\n",
        "        \"Dir01 SGD\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        \"Dir05 SGD\": {\"color\": \"sandybrown\", \"linestyle\": \":\"},\n",
        "    },\n",
        "    highlight={\n",
        "        \"Class Adam\": \"max\",\n",
        "        \"Dir01 Adam\": \"max\",\n",
        "        \"Dir05 Adam\": \"max\",\n",
        "        \"Class SGD\": \"max\",\n",
        "        \"Dir01 SGD\": \"max\",\n",
        "        \"Dir05 SGD\": \"max\",\n",
        "    },\n",
        "    highlight_style={\n",
        "        \"Class Adam\": {\"highlight_offset_max\": (20, 0.02)},\n",
        "        \"Dir01 Adam\": {\"highlight_offset_max\": (-5, -0.17)},\n",
        "        \"Dir05 Adam\": {\"highlight_offset_max\": (0, -0.23)},\n",
        "        \"Class SGD\": {\"highlight_offset_max\": (20, 0.005)},\n",
        "        \"Dir01 SGD\": {\"highlight_offset_max\": (-5, -0.16)},\n",
        "        \"Dir05 SGD\": {\"highlight_offset_max\": (-5, -0.22)},\n",
        "     },\n",
        "    xtick_step=10,\n",
        "    first_step=9\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d66909",
      "metadata": {},
      "source": [
        "## Plot generator images per epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa906d3",
      "metadata": {
        "id": "bfa906d3"
      },
      "outputs": [],
      "source": [
        "gen = F2U_GAN(condition=True).to(\"cpu\")\n",
        "checkpoint_loaded = torch.load(\"../Experimentos/NB_F2U/GeraFed_4c_01Dir/CIFAR/checkpoint_epoch100.pth\", map_location=\"cpu\")\n",
        "gen.load_state_dict(checkpoint_loaded[\"gen_state_dict\"])\n",
        "generate_plot(gen, \"cpu\", 50, latent_dim=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95346fb",
      "metadata": {},
      "source": [
        "## Evaluate Times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be75e38",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# A helper function to add labels on top of the bars\n",
        "def add_labels(rects, ax):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}', # Format the number to 2 decimal places\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontsize=14) # Fontsize for the labels on bars\n",
        "\n",
        "# --- Main Code ---\n",
        "\n",
        "# Data for the bar plots\n",
        "labels = ['Classifier Training', 'Image Generation']\n",
        "first_epoch_a = [0.1, 0.02]\n",
        "last_epoch_a = [0.23, 0.3]\n",
        "first_epoch_b = [0.09, 0.03]\n",
        "last_epoch_b = [0.2, 0.42]\n",
        "\n",
        "# Setting the positions of the bars\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "# Creating the figure and subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# --- Font sizes ---\n",
        "title_fontsize = 18\n",
        "label_fontsize = 14\n",
        "tick_fontsize = 12\n",
        "legend_fontsize = 12\n",
        "\n",
        "# --- Barplot a) ---\n",
        "# Capture the bar containers in variables (rects1a, rects2a)\n",
        "rects1a = ax1.bar(x - width/2, first_epoch_a, width, label='First Epoch', color=\"cornflowerblue\")\n",
        "rects2a = ax1.bar(x + width/2, last_epoch_a, width, label='Last Epoch', color=\"sandybrown\")\n",
        "\n",
        "# Add titles and labels\n",
        "ax1.set_ylabel('Time (s)', fontsize=label_fontsize)\n",
        "ax1.set_title('a)', fontsize=title_fontsize)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(labels, fontsize=label_fontsize)\n",
        "ax1.tick_params(axis='y', labelsize=tick_fontsize)\n",
        "ax1.legend(fontsize=legend_fontsize)\n",
        "\n",
        "# Add the labels on top of the bars\n",
        "add_labels(rects1a, ax1)\n",
        "add_labels(rects2a, ax1)\n",
        "\n",
        "# --- Barplot b) ---\n",
        "# Capture the bar containers in variables (rects1b, rects2b)\n",
        "rects1b = ax2.bar(x - width/2, first_epoch_b, width, label='First Epoch', color=\"cornflowerblue\")\n",
        "rects2b = ax2.bar(x + width/2, last_epoch_b, width, label='Last Epoch', color=\"sandybrown\")\n",
        "\n",
        "# Add titles and labels\n",
        "ax2.set_ylabel('Time (s)', fontsize=label_fontsize)\n",
        "ax2.set_title('b)', fontsize=title_fontsize)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels, fontsize=label_fontsize)\n",
        "ax2.tick_params(axis='y', labelsize=tick_fontsize)\n",
        "ax2.legend(fontsize=legend_fontsize)\n",
        "\n",
        "# Add the labels on top of the bars\n",
        "add_labels(rects1b, ax2)\n",
        "add_labels(rects2b, ax2)\n",
        "\n",
        "# Adjust y-axis limits to make space for the labels\n",
        "ax1.set_ylim(0, ax1.get_ylim()[1] * 1.1)\n",
        "ax2.set_ylim(0, ax2.get_ylim()[1] * 1.1)\n",
        "\n",
        "# Adjust the layout\n",
        "fig.tight_layout(pad=3.0)\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed4a97a6",
      "metadata": {},
      "source": [
        "## Network Traffic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7df05c2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_weights(net):\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "def get_weights_gen(net):\n",
        "    return [val.cpu().numpy() for key, val in net.state_dict().items() if 'discriminator' in key or 'label' in key]\n",
        "\n",
        "\n",
        "def set_weights(net, parameters):\n",
        "    device = next(net.parameters()).device\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3fe472b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_mnist = Net()\n",
        "classifier_cifar = Net_Cifar()\n",
        "GAN_MNIST = F2U_GAN()\n",
        "GAN_CIFAR = F2U_GAN_CIFAR()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "598a49cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_mnist_params = get_weights(classifier_mnist)\n",
        "classifier_cifar_params = get_weights(classifier_cifar)\n",
        "GAN_MNIST_disc_params = get_weights_gen(GAN_MNIST)\n",
        "GAN_CIFAR_disc_params = get_weights_gen(GAN_CIFAR)\n",
        "GAN_MNIST_gen_params = [val.cpu().numpy() for key, val in GAN_MNIST.state_dict().items() if 'generator' in key or 'label' in key]\n",
        "GAN_CIFAR_gen_params = [val.cpu().numpy() for key, val in GAN_CIFAR.state_dict().items() if 'generator' in key or 'label' in key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "016519d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cumulative step plot for upload/download traffic over rounds.\n",
        "import numpy as np\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3b0672bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_size_mb(params, divisor=10**6):\n",
        "    buffer = io.BytesIO()\n",
        "    np.savez(buffer, *params)\n",
        "    return len(buffer.getvalue()) / divisor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d83f0db2",
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_mnist_MB = get_model_size_mb(classifier_mnist_params)\n",
        "classifier_cifar_MB = get_model_size_mb(classifier_cifar_params)\n",
        "disc_mnist_MB       = get_model_size_mb(GAN_MNIST_disc_params)\n",
        "disc_cifar_MB       = get_model_size_mb(GAN_CIFAR_disc_params)\n",
        "gen_mnist_MB        = get_model_size_mb(GAN_MNIST_gen_params)\n",
        "gen_cifar_MB        = get_model_size_mb(GAN_CIFAR_gen_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "015b4532",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x380 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "# Per-epoch traffic (GB)\n",
        "upload_per_epoch_gerafed_mnist = (classifier_mnist_MB + disc_mnist_MB)/10 #/1000 pra Giga e x100 por epoch por cause do chunk.\n",
        "download_per_epoch_gerafed_mnist = (classifier_mnist_MB + gen_mnist_MB)/10\n",
        "\n",
        "upload_per_epoch_gerafed_cifar = (classifier_cifar_MB + disc_cifar_MB)/10 #/1000 pra Giga e x100 por epoch por cause do chunk.\n",
        "download_per_epoch_gerafed_cifar = (classifier_cifar_MB + gen_cifar_MB)/10\n",
        "\n",
        "\n",
        "upload_per_epoch_chunkedfedavg_mnist = classifier_mnist_MB/10\n",
        "download_per_epoch_chunkedfedavg_mnist = classifier_mnist_MB/10\n",
        "\n",
        "upload_per_epoch_chunkedfedavg_cifar = classifier_cifar_MB/10\n",
        "download_per_epoch_chunkedfedavg_cifar = classifier_cifar_MB/10\n",
        "\n",
        "\n",
        "# Cumulative arrays with an initial 0 so the plot has horizontal lines before first epoch\n",
        "x = np.arange(0, epochs + 1)  # 0..epochs inclusive\n",
        "\n",
        "cum_upload_gerafed_mnist = np.insert(np.cumsum(np.full(epochs, upload_per_epoch_gerafed_mnist)), 0, 0)\n",
        "cum_download_gerafed_mnist = np.insert(np.cumsum(np.full(epochs, download_per_epoch_gerafed_mnist)), 0, 0)\n",
        "\n",
        "cum_upload_gerafed_cifar = np.insert(np.cumsum(np.full(epochs, upload_per_epoch_gerafed_cifar)), 0, 0)\n",
        "cum_download_gerafed_cifar = np.insert(np.cumsum(np.full(epochs, download_per_epoch_gerafed_cifar)), 0, 0)\n",
        "\n",
        "\n",
        "cum_upload_chunkedfedavg_mnist = np.insert(np.cumsum(np.full(epochs, upload_per_epoch_chunkedfedavg_mnist)), 0, 0)\n",
        "cum_download_chunkedfedavg_mnist = np.insert(np.cumsum(np.full(epochs, download_per_epoch_chunkedfedavg_mnist)), 0, 0)\n",
        "\n",
        "cum_upload_chunkedfedavg_cifar = np.insert(np.cumsum(np.full(epochs, upload_per_epoch_chunkedfedavg_cifar)), 0, 0)\n",
        "cum_download_chunkedfedavg_cifar = np.insert(np.cumsum(np.full(epochs, download_per_epoch_chunkedfedavg_cifar)), 0, 0)\n",
        "\n",
        "\n",
        "cum_upload_and_download_chunkedfedavg_mnist = cum_upload_chunkedfedavg_mnist + cum_download_chunkedfedavg_mnist\n",
        "\n",
        "cum_upload_and_download_chunkedfedavg_cifar = cum_upload_chunkedfedavg_cifar + cum_download_chunkedfedavg_cifar\n",
        "\n",
        "\n",
        "total_upload_GB_gerafed_mnist = cum_upload_gerafed_mnist[-1]\n",
        "total_download_GB_gerafed_mnist = cum_download_gerafed_mnist[-1]\n",
        "\n",
        "total_upload_GB_gerafed_cifar = cum_upload_gerafed_cifar[-1]\n",
        "total_download_GB_gerafed_cifar = cum_download_gerafed_cifar[-1]\n",
        "\n",
        "\n",
        "total_upload_GB_chunkedfedavg_mnist = cum_upload_chunkedfedavg_mnist[-1]\n",
        "total_download_GB_chunkedfedavg_mnist = cum_download_chunkedfedavg_mnist[-1]\n",
        "\n",
        "total_upload_GB_chunkedfedavg_cifar = cum_upload_chunkedfedavg_cifar[-1]\n",
        "total_download_GB_chunkedfedavg_cifar = cum_download_chunkedfedavg_cifar[-1]\n",
        "\n",
        "total_upload_and_download_chunkedfedavg_mnist = total_upload_GB_chunkedfedavg_mnist + total_download_GB_chunkedfedavg_mnist\n",
        "\n",
        "total_upload_and_download_chunkedfedavg_cifar = total_upload_GB_chunkedfedavg_cifar + total_download_GB_chunkedfedavg_cifar\n",
        "\n",
        "\n",
        "# Single step plot (cumulative). Using where='post' so the vertical jumps happen at integer epochs.\n",
        "plt.figure(figsize=(20, 3.8))\n",
        "\n",
        "plt.step(x, cum_upload_gerafed_mnist, where='post', label=\"FedGenIA upload\", color=\"palevioletred\", linestyle='--', linewidth=6)\n",
        "plt.step(x, cum_download_gerafed_mnist, where='post', label=\"FedGenIA download\", color=\"yellowgreen\", linestyle='--', linewidth=6)\n",
        "plt.step(x, cum_upload_and_download_chunkedfedavg_mnist, where='post', label=\"Chunked FedAvg upload and download\", color=\"cornflowerblue\", linewidth=6)\n",
        "\n",
        "# plt.step(x, cum_upload_gerafed_cifar, where='post', label=\"GeraFed upload\", color=\"cornflowerblue\")\n",
        "# plt.step(x, cum_download_gerafed_cifar, where='post', label=\"GeraFed download\", color=\"royalblue\")\n",
        "# plt.step(x, cum_upload_and_download_chunkedfedavg_cifar, where='post', label=\"Chunked FedAvg upload and download\", color=\"sandybrown\")\n",
        "\n",
        "# plt.step(x, cum_download_chunkedfedavg, where='post', label=\"Chunked FedAvg download\", color=\"peru\")\n",
        "plt.xlim(0, epochs)\n",
        "plt.xticks(np.arange(0, epochs+1, max(1, epochs//10)), fontsize=26)\n",
        "plt.yticks(fontsize=26)\n",
        "plt.xlabel(\"Epoch\", fontsize=28)\n",
        "plt.ylabel(\"Cumulative GB\", fontsize=28)\n",
        "plt.grid(True)\n",
        "plt.legend(fontsize=23, loc='lower right', ncol=3, handlelength=2.5, handleheight=1.5, handletextpad=0.7)\n",
        "plt.yscale(\"log\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate final totals on the right side\n",
        "# plt.annotate(f\"{total_upload_GB_gerafed_mnist:.0f} GB\", xy=(epochs, total_upload_GB_gerafed_mnist),\n",
        "#              xytext=(epochs-5, total_upload_GB_gerafed_mnist + max(1, total_upload_GB_gerafed_mnist*0.02)),\n",
        "#              arrowprops=dict(arrowstyle=\"->\"), fontsize=14, va=\"bottom\")\n",
        "# plt.annotate(f\"{total_download_GB_gerafed_mnist:.0f} GB\", xy=(epochs, total_download_GB_gerafed_mnist),\n",
        "#              xytext=(epochs-5, total_download_GB_gerafed_mnist + max(1, total_download_GB_gerafed_mnist*0.02)),\n",
        "#              arrowprops=dict(arrowstyle=\"->\"), fontsize=12, va=\"bottom\")\n",
        "# plt.annotate(f\"{total_upload_and_download_chunkedfedavg_mnist:.0f} GB\", xy=(epochs, total_upload_and_download_chunkedfedavg_mnist),\n",
        "#              xytext=(epochs-5, total_upload_and_download_chunkedfedavg_mnist + max(1, total_upload_and_download_chunkedfedavg_mnist*0.02)),\n",
        "#              arrowprops=dict(arrowstyle=\"->\"), fontsize=9, va=\"bottom\")\n",
        "\n",
        "# plt.annotate(f\"{total_upload_GB_gerafed_cifar:.0f} GB\", xy=(epochs, total_upload_GB_gerafed_cifar),\n",
        "#              xytext=(epochs-5, total_upload_GB_gerafed_cifar + max(1, total_upload_GB_gerafed_cifar*0.02)),\n",
        "#              arrowprops=dict(arrowstyle=\"->\"), fontsize=9, va=\"bottom\")\n",
        "# plt.annotate(f\"{total_download_GB_gerafed_cifar:.0f} GB\", xy=(epochs, total_download_GB_gerafed_cifar),\n",
        "#              xytext=(epochs-5, total_download_GB_gerafed_cifar + max(1, total_download_GB_gerafed_cifar*0.02)),\n",
        "#              arrowprops=dict(arrowstyle=\"->\"), fontsize=9, va=\"bottom\")\n",
        "# plt.annotate(f\"{total_upload_and_download_chunkedfedavg_cifar:.0f} GB\", xy=(epochs, total_upload_and_download_chunkedfedavg_cifar),\n",
        "#              xytext=(epochs-5, total_upload_and_download_chunkedfedavg_cifar + max(1, total_upload_and_download_chunkedfedavg_cifar*0.02)),\n",
        "#              arrowprops=dict(arrowstyle=\"->\"), fontsize=9, va=\"bottom\")\n",
        "\n",
        "plt.savefig(\"../../../DML-ICC/Figures/network_traffic.pdf\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d66fca29",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(\"../figures/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b605738",
      "metadata": {},
      "source": [
        "## Number of Synthetic Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e65e16b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f302686",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate x (epoch) values from 0 to 100\n",
        "epochs = np.arange(0, 101)\n",
        "\n",
        "# Calculate y values for each epoch\n",
        "y_values = [int(13 * (math.exp(0.01*epoch) - 1) / (math.exp(0.01*50) - 1) * 10) for epoch in epochs]\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 2))\n",
        "plt.plot(epochs, y_values, color='cornflowerblue', linewidth=5)\n",
        "plt.xlabel(\"Epoch\", fontsize=18)\n",
        "plt.ylabel(\"|S|\", fontsize=18)\n",
        "plt.xticks(fontsize=16, ticks=np.linspace(0,100,5))\n",
        "plt.yticks(fontsize=16, ticks=[0, 88, 175, 267, 350])\n",
        "plt.xlim(0, 100)\n",
        "plt.ylim(0, 350)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeeddbca",
      "metadata": {
        "id": "aeeddbca"
      },
      "source": [
        "# Compara treino de classificador em dados reais, sintéticos e misturados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c400df8",
      "metadata": {
        "id": "6c400df8"
      },
      "outputs": [],
      "source": [
        "nets = [Net().to(device) for _ in range(num_partitions)]\n",
        "optims = [torch.optim.Adam(net.parameters(), lr=0.01) for net in nets]\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2117f9e5",
      "metadata": {
        "id": "2117f9e5"
      },
      "outputs": [],
      "source": [
        "for i, (net, optim) in enumerate(zip(nets, optims)):\n",
        "    net.train()\n",
        "    for epoch in range(50):\n",
        "        for data in trainloaders[i]:\n",
        "            inputs, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "            optim.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ad2d72",
      "metadata": {
        "id": "c7ad2d72"
      },
      "outputs": [],
      "source": [
        "testpartition = fds.load_split(\"test\")\n",
        "testpartition = testpartition.with_transform(apply_transforms)\n",
        "testloader = DataLoader(testpartition, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abfbb983",
      "metadata": {
        "id": "abfbb983"
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "for net in nets:\n",
        "    correct, loss = 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "    accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eedc9b7",
      "metadata": {
        "id": "0eedc9b7"
      },
      "outputs": [],
      "source": [
        "accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea7a7b0",
      "metadata": {
        "id": "5ea7a7b0"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 1000\n",
        "latent_dim = 128\n",
        "\n",
        "# gen = F2U_GAN()\n",
        "# gen.load_state_dict(torch.load(\"gen_round50.pt\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=gen.to(\"cpu\"), num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc9f7fe",
      "metadata": {
        "id": "3bc9f7fe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import ConcatDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e1cbfd",
      "metadata": {
        "id": "99e1cbfd"
      },
      "outputs": [],
      "source": [
        "combined_dataloaders = []\n",
        "for train_partition in train_partitions:\n",
        "    # Ensure the partition is transformed\n",
        "    cmb_ds = ConcatDataset([train_partition, generated_dataset])\n",
        "    combined_dataloaders.append(DataLoader(cmb_ds, batch_size=batch_size, shuffle=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85df355d",
      "metadata": {
        "id": "85df355d"
      },
      "outputs": [],
      "source": [
        "nets = [Net().to(device) for _ in range(num_partitions)]\n",
        "optims = [torch.optim.Adam(net.parameters(), lr=0.01) for net in nets]\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea13f13",
      "metadata": {
        "id": "9ea13f13"
      },
      "outputs": [],
      "source": [
        "for i, (net, optim) in enumerate(zip(nets, optims)):\n",
        "    net.train()\n",
        "    for epoch in range(50):\n",
        "        for data in combined_dataloaders[i]:\n",
        "            inputs, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "            optim.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9818e23a",
      "metadata": {
        "id": "9818e23a"
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "for net in nets:\n",
        "    correct, loss = 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "    accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b2cb6d",
      "metadata": {
        "id": "c2b2cb6d"
      },
      "outputs": [],
      "source": [
        "accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff260823",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definindo x e N\n",
        "x = list(range(1, 101))\n",
        "den = math.exp(0.01 * 50) - 1\n",
        "N = [int(13 * (math.exp(0.01 * (xi - 1)) - 1) / den) * 1000 for xi in x]\n",
        "y = [390*xi for xi in x]\n",
        "\n",
        "# Plot\n",
        "plt.figure()\n",
        "plt.plot(x, N)\n",
        "plt.plot(x, y)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"N\")\n",
        "plt.title(\"Plot de N = int(13 * (exp(0.01*(x-1)) - 1)/(exp(0.5) - 1)) * 1000\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6d8c276e",
        "314c3604"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gerafed",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
