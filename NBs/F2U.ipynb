{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07f8307e",
      "metadata": {
        "id": "07f8307e"
      },
      "source": [
        "# Inicialização"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f47d1996",
      "metadata": {
        "id": "f47d1996"
      },
      "source": [
        "## Prepara o ambiente local ou colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a7ea1c",
      "metadata": {
        "id": "88a7ea1c"
      },
      "outputs": [],
      "source": [
        "# --- Detectar Ambiente (Colab ou Local) ---\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    # Tenta importar um módulo específico do Colab\n",
        "    from google.colab import drive\n",
        "    import shutil # Usaremos para copiar, se necessário, mas salvar direto é melhor\n",
        "    import os\n",
        "\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        # Crie um diretório específico para salvar os resultados desta execução\n",
        "        save_base_dir = \"/content/drive/MyDrive/GAN_Training_Results\" # Ajuste o caminho como desejar\n",
        "        os.makedirs(save_base_dir, exist_ok=True)\n",
        "        # Opcional: Crie um subdiretório único para esta execução específica (ex: baseado em timestamp)\n",
        "        # import datetime\n",
        "        # timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        # save_dir = os.path.join(save_base_dir, f\"run_{timestamp}\")\n",
        "        # os.makedirs(save_dir, exist_ok=True)\n",
        "        # Por simplicidade, vamos usar o diretório base diretamente por enquanto\n",
        "        save_dir = save_base_dir\n",
        "        print(f\"✅ Google Drive montado. Arquivos serão salvos em: {save_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao montar o Google Drive: {e}\")\n",
        "        print(\"   Downloads diretos serão tentados, mas podem atrasar.\")\n",
        "        save_dir = \".\" # Salvar localmente se o Drive falhar\n",
        "    IN_COLAB = True\n",
        "    print(\"✅ Ambiente Google Colab detectado. Downloads automáticos (a cada 2 épocas) ativados.\")\n",
        "except ImportError:\n",
        "    print(\"✅ Ambiente local detectado. Downloads automáticos desativados.\")\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e68593",
      "metadata": {
        "id": "08e68593"
      },
      "source": [
        "## Importa Pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e43ad3e",
      "metadata": {
        "id": "1e43ad3e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5df872b",
      "metadata": {
        "id": "b5df872b"
      },
      "source": [
        "## Modelo Classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4395f020",
      "metadata": {
        "id": "4395f020"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*4*4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98df505",
      "metadata": {
        "id": "a98df505"
      },
      "outputs": [],
      "source": [
        "class Net_Cifar(nn.Module):\n",
        "    def __init__(self,seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8c276e",
      "metadata": {
        "id": "6d8c276e"
      },
      "source": [
        "## Carrega Dados MNIST centralizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a82cbfd1",
      "metadata": {
        "id": "a82cbfd1"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ad4aaf",
      "metadata": {
        "id": "13ad4aaf"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the training and test datasets\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "trainset_reduzido = torch.utils.data.random_split(trainset, [1000, len(trainset) - 1000])[0]\n",
        "# Create data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainloader_reduzido = DataLoader(trainset_reduzido, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
        "\n",
        "dataset = \"mnist\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64be3f56",
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# Define transform com ToTensor e Normalize para 3 canais\n",
        "transform_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),  # média por canal R,G,B\n",
        "                         (0.5, 0.5, 0.5))  # desvio padrão por canal\n",
        "])\n",
        "\n",
        "# Carrega os datasets de treino e teste\n",
        "trainset_cifar = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_cifar\n",
        ")\n",
        "testset_cifar = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_cifar\n",
        ")\n",
        "\n",
        "# Cria um subset reduzido de treino (por exemplo, 1000 amostras)\n",
        "#trainset_cifar_reduzido = random_split(trainset_cifar, [1000, len(trainset_cifar) - 1000])[0]\n",
        "\n",
        "# DataLoaders\n",
        "trainloader_cifar = DataLoader(\n",
        "    trainset_cifar,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "# trainloader_cifar_reduzido = DataLoader(\n",
        "#     trainset_cifar_reduzido,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=True,\n",
        "#     num_workers=2,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "testloader_cifar = DataLoader(\n",
        "    testset_cifar,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "dataset = \"cifar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c27cfeac",
      "metadata": {
        "id": "c27cfeac"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# parameters\n",
        "num_classes = 10\n",
        "samples_per_class = 5\n",
        "\n",
        "if dataset == \"cifar\":\n",
        "    class_names = [\n",
        "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "     'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "    ]\n",
        "\n",
        "# containers\n",
        "class_counts = {i: 0 for i in range(num_classes)}\n",
        "class_images = {i: [] for i in range(num_classes)}\n",
        "\n",
        "# gather up to 5 images per class\n",
        "for img, label in trainset:\n",
        "    if class_counts[label] < samples_per_class:\n",
        "        class_images[label].append(img)\n",
        "        class_counts[label] += 1\n",
        "    # stop early once we have enough of every class\n",
        "    if all(count >= samples_per_class for count in class_counts.values()):\n",
        "        break\n",
        "\n",
        "# plot\n",
        "fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(5, 9))\n",
        "for cls in range(num_classes):\n",
        "    for i in range(samples_per_class):\n",
        "        ax = axes[cls, i]\n",
        "        img = class_images[cls][i]\n",
        "        if dataset == \"mnist\":\n",
        "            ax.imshow(img.squeeze(), cmap='gray')\n",
        "        else:\n",
        "            img_denorm = (img * 0.5 + 0.5)  # denormalize for visualization\n",
        "            ax.imshow(img_denorm.permute(1, 2, 0).numpy())\n",
        "        ax.axis('off')\n",
        "    # label the rows on the leftmost subplot\n",
        "   # axes[cls, 0].set_ylabel(str(cls), rotation=0, labelpad=12, va='center', fontsize=12)\n",
        "\n",
        "# Ajustar o layout antes de calcular as posições\n",
        "plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "# Adicionar os rótulos das classes corretamente alinhados\n",
        "fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
        "for row in range(num_classes):\n",
        "    # Obter posição do subplot em coordenadas da figura\n",
        "    bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "    pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "    center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "    # Adicionar o rótulo\n",
        "    fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
        "\n",
        "plt.suptitle(\"Real\", fontsize=16)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f6fbda",
      "metadata": {
        "id": "54f6fbda"
      },
      "source": [
        "## Modelo Generativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea03ea69",
      "metadata": {
        "id": "ea03ea69"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "314c3604",
      "metadata": {
        "id": "314c3604"
      },
      "source": [
        "### CGAN (simples, mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23cfec37",
      "metadata": {
        "id": "23cfec37"
      },
      "outputs": [],
      "source": [
        "class CGAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=100):\n",
        "        super(CGAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes)\n",
        "        self.adv_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            *self._create_layer_gen(self.latent_dim + self.classes, 128, False),\n",
        "            *self._create_layer_gen(128, 256),\n",
        "            *self._create_layer_gen(256, 512),\n",
        "            *self._create_layer_gen(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.discriminator = nn.Sequential(\n",
        "            *self._create_layer_disc(self.classes + int(np.prod(self.img_shape)), 1024, False, True),\n",
        "            *self._create_layer_disc(1024, 512, True, True),\n",
        "            *self._create_layer_disc(512, 256, True, True),\n",
        "            *self._create_layer_disc(256, 128, False, False),\n",
        "            *self._create_layer_disc(128, 1, False, False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _create_layer_gen(self, size_in, size_out, normalize=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(size_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def _create_layer_disc(self, size_in, size_out, drop_out=True, act_func=True):\n",
        "        layers = [nn.Linear(size_in, size_out)]\n",
        "        if drop_out:\n",
        "            layers.append(nn.Dropout(0.4))\n",
        "        if act_func:\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, input, labels):\n",
        "        if input.dim() == 2:\n",
        "            z = torch.cat((self.label_embedding(labels), input), -1)\n",
        "            x = self.generator(z)\n",
        "            x = x.view(x.size(0), *self.img_shape) #Em\n",
        "            return x\n",
        "        elif input.dim() == 4:\n",
        "            x = torch.cat((input.view(input.size(0), -1), self.label_embedding(labels)), -1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1a5b73",
      "metadata": {
        "id": "4f1a5b73"
      },
      "source": [
        "### Arquitetura do paper F2U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb8292a",
      "metadata": {
        "id": "cbb8292a"
      },
      "outputs": [],
      "source": [
        "class F2U_GAN(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=128, condition=True, seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super(F2U_GAN, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only MNIST is supported\")\n",
        "\n",
        "        self.condition = condition\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes) if condition else None\n",
        "        #self.label_embedding_disc = nn.Embedding(self.classes, self.img_size*self.img_size) if condition else None\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.input_shape_gen = self.latent_dim + self.label_embedding.embedding_dim if condition else self.latent_dim\n",
        "        self.input_shape_disc = self.channels + self.classes if condition else self.channels\n",
        "\n",
        "        self.adv_loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Generator (unchanged) To calculate output shape of convtranspose layers, we can use the formula:\n",
        "        # output_shape = (input_shape - 1) * stride - 2 * padding + kernel_size + output_padding (or dilation * (kernel_size - 1) + 1 inplace of kernel_size if using dilation)\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.Linear(self.input_shape_gen, 256 * 7 * 7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (256, 7, 7)),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (256,7,7) -> (128,14,14)\n",
        "            nn.BatchNorm2d(128, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # (128,14,14) -> (64,28,28)\n",
        "            nn.BatchNorm2d(64, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, self.channels, kernel_size=3, stride=1, padding=1), # (64,28,28) -> (1,28,28)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Discriminator (corrected) To calculate output shape of conv layers, we can use the formula:\n",
        "        # output_shape = ⌊(input_shape - kernel_size + 2 * padding) / stride + 1⌋ (or (dilation * (kernel_size - 1) - 1) inplace of kernel_size if using dilation)\n",
        "        self.discriminator = nn.Sequential(\n",
        "        # Camada 1: (1,28,28) -> (32,13,13)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(self.input_shape_disc, 32, kernel_size=3, stride=2, padding=0)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Camada 2: (32,14,14) -> (64,7,7)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Camada 3: (64,7,7) -> (128,3,3)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Camada 4: (128,3,3) -> (256,1,1)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0)),  # Padding 0 aqui!\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Achata e concatena com as labels\n",
        "        nn.Flatten(), # (256,1,1) -> (256*1*1,)\n",
        "        nn.utils.spectral_norm(nn.Linear(256 * 1 * 1, 1))  # 256 (features)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, labels=None):\n",
        "        if input.dim() == 2:\n",
        "            # Generator forward pass (unchanged)\n",
        "            if self.condition:\n",
        "                embedded_labels = self.label_embedding(labels)\n",
        "                gen_input = torch.cat((input, embedded_labels), dim=1)\n",
        "                x = self.generator(gen_input)\n",
        "            else:\n",
        "                x = self.generator(input)\n",
        "            return x.view(-1, *self.img_shape)\n",
        "\n",
        "        elif input.dim() == 4:\n",
        "            # Discriminator forward pass\n",
        "            if self.condition:\n",
        "                embedded_labels = self.label_embedding(labels)\n",
        "                image_labels = embedded_labels.view(embedded_labels.size(0), self.label_embedding.embedding_dim, 1, 1).expand(-1, -1, self.img_size, self.img_size)\n",
        "                x = torch.cat((input, image_labels), dim=1)\n",
        "            else:\n",
        "                x = input\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decb2888",
      "metadata": {},
      "outputs": [],
      "source": [
        "class F2U_GAN_SlowDisc(nn.Module):\n",
        "    def __init__(self, dataset=\"mnist\", img_size=28, latent_dim=128, condition=True, seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super(F2U_GAN_SlowDisc, self).__init__()\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = 10\n",
        "            self.channels = 1\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only MNIST is supported\")\n",
        "\n",
        "        self.condition = condition\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes) if condition else None\n",
        "        #self.label_embedding_disc = nn.Embedding(self.classes, self.img_size*self.img_size) if condition else None\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = (self.channels, self.img_size, self.img_size)\n",
        "        self.input_shape_gen = self.latent_dim + self.label_embedding.embedding_dim if condition else self.latent_dim\n",
        "        self.input_shape_disc = self.channels + self.classes if condition else self.channels\n",
        "\n",
        "        self.adv_loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Generator (unchanged) To calculate output shape of convtranspose layers, we can use the formula:\n",
        "        # output_shape = (input_shape - 1) * stride - 2 * padding + kernel_size + output_padding (or dilation * (kernel_size - 1) + 1 inplace of kernel_size if using dilation)\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.Linear(self.input_shape_gen, 256 * 7 * 7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (256, 7, 7)),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (256,7,7) -> (128,14,14)\n",
        "            nn.BatchNorm2d(128, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # (128,14,14) -> (64,28,28)\n",
        "            nn.BatchNorm2d(64, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, self.channels, kernel_size=3, stride=1, padding=1), # (64,28,28) -> (1,28,28)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Discriminator (corrected) To calculate output shape of conv layers, we can use the formula:\n",
        "        # output_shape = ⌊(input_shape - kernel_size + 2 * padding) / stride + 1⌋ (or (dilation * (kernel_size - 1) - 1) inplace of kernel_size if using dilation)\n",
        "        self.discriminator = nn.Sequential(\n",
        "        # Camada 1: (1,28,28) -> (32,13,13)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(self.input_shape_disc, 32, kernel_size=3, stride=2, padding=0)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout2d(0.3),\n",
        "\n",
        "        # Camada 2: (32,14,14) -> (64,7,7)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout2d(0.3),\n",
        "\n",
        "        # Camada 3: (64,7,7) -> (128,3,3)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0)),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout2d(0.3),\n",
        "\n",
        "        # Camada 4: (128,3,3) -> (256,1,1)\n",
        "        nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0)),  # Padding 0 aqui!\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Achata e concatena com as labels\n",
        "        nn.Flatten(), # (256,1,1) -> (256*1*1,)\n",
        "        nn.utils.spectral_norm(nn.Linear(256 * 1 * 1, 1))  # 256 (features)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, labels=None):\n",
        "        if input.dim() == 2:\n",
        "            # Generator forward pass (unchanged)\n",
        "            if self.condition:\n",
        "                embedded_labels = self.label_embedding(labels)\n",
        "                gen_input = torch.cat((input, embedded_labels), dim=1)\n",
        "                x = self.generator(gen_input)\n",
        "            else:\n",
        "                x = self.generator(input)\n",
        "            return x.view(-1, *self.img_shape)\n",
        "\n",
        "        elif input.dim() == 4:\n",
        "            # Discriminator forward pass\n",
        "            if self.condition:\n",
        "                embedded_labels = self.label_embedding(labels)\n",
        "                image_labels = embedded_labels.view(embedded_labels.size(0), self.label_embedding.embedding_dim, 1, 1).expand(-1, -1, self.img_size, self.img_size)\n",
        "                input = input + torch.randn_like(input) * 0.1\n",
        "                x = torch.cat((input, image_labels), dim=1)\n",
        "            else:\n",
        "                x = input\n",
        "            return self.discriminator(x)\n",
        "\n",
        "    def loss(self, output, label):\n",
        "        return self.adv_loss(output, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52363e9",
      "metadata": {
        "id": "c52363e9"
      },
      "outputs": [],
      "source": [
        "class F2U_GAN_CIFAR(nn.Module):\n",
        "    def __init__(self, img_size=32, latent_dim=128, condition=True, seed=None):\n",
        "        if seed is not None:\n",
        "          torch.manual_seed(seed)\n",
        "        super(F2U_GAN_CIFAR, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.classes = 10\n",
        "        self.channels = 3\n",
        "        self.condition = condition\n",
        "\n",
        "        # Embedding para condicionamento\n",
        "        self.label_embedding = nn.Embedding(self.classes, self.classes) if self.condition else None\n",
        "\n",
        "        # Shapes de entrada\n",
        "        self.input_shape_gen = self.latent_dim + (self.classes if self.condition else 0)\n",
        "        self.input_shape_disc = self.channels + (self.classes if self.condition else 0)\n",
        "\n",
        "        # -----------------\n",
        "        #  Generator\n",
        "        # -----------------\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.Linear(self.input_shape_gen, 512 * 4 * 4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (512, 4, 4)),                  # → (512,4,4)\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # → (256,8,8)\n",
        "            nn.BatchNorm2d(256, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # → (128,16,16)\n",
        "            nn.BatchNorm2d(128, momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128,  64, kernel_size=4, stride=2, padding=1),  # → ( 64,32,32)\n",
        "            nn.BatchNorm2d(64,  momentum=0.1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d( 64,   self.channels, kernel_size=3, stride=1, padding=1),  # → (3,32,32)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # -----------------\n",
        "        #  Discriminator\n",
        "        # -----------------\n",
        "        layers = []\n",
        "        in_ch = self.input_shape_disc\n",
        "        cfg = [\n",
        "            ( 64, 3, 1),  # → spatial stays 32\n",
        "            ( 64, 4, 2),  # → 16\n",
        "            (128, 3, 1),  # → 16\n",
        "            (128, 4, 2),  # → 8\n",
        "            (256, 4, 2),  # → 4\n",
        "        ]\n",
        "        for out_ch, k, s in cfg:\n",
        "            layers += [\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=1)\n",
        "                ),\n",
        "                nn.LeakyReLU(0.1, inplace=True)\n",
        "            ]\n",
        "            in_ch = out_ch\n",
        "\n",
        "        layers += [\n",
        "            nn.Flatten(),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Linear(256 * 4 * 4, 1)\n",
        "            )\n",
        "        ]\n",
        "        self.discriminator = nn.Sequential(*layers)\n",
        "\n",
        "        # adversarial loss\n",
        "        self.adv_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, input, labels=None):\n",
        "        # Generator pass\n",
        "        if input.dim() == 2 and input.size(1) == self.latent_dim:\n",
        "            if self.condition:\n",
        "                if labels is None:\n",
        "                    raise ValueError(\"Labels must be provided for conditional generation\")\n",
        "                embedded = self.label_embedding(labels)\n",
        "                gen_input = torch.cat((input, embedded), dim=1)\n",
        "            else:\n",
        "                gen_input = input\n",
        "            img = self.generator(gen_input)\n",
        "            return img\n",
        "\n",
        "        # Discriminator pass\n",
        "        elif input.dim() == 4 and input.size(1) == self.channels:\n",
        "            x = input\n",
        "            if self.condition:\n",
        "                if labels is None:\n",
        "                    raise ValueError(\"Labels must be provided for conditional discrimination\")\n",
        "                embedded = self.label_embedding(labels)\n",
        "                # criar mapa de labels e concatenar\n",
        "                lbl_map = embedded.view(-1, self.classes, 1, 1).expand(-1, self.classes, self.img_size, self.img_size)\n",
        "                x = torch.cat((x, lbl_map), dim=1)\n",
        "            return self.discriminator(x)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Input shape not recognized\")\n",
        "\n",
        "    def loss(self, logits, targets):\n",
        "        return self.adv_loss(logits.view(-1), targets.float().view(-1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144a24f4",
      "metadata": {
        "id": "144a24f4"
      },
      "source": [
        "## Funções para geração de dataset e imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3487d31",
      "metadata": {
        "id": "b3487d31"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import random # Needed for handling remainders if samples aren't perfectly divisible\n",
        "\n",
        "class GeneratedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 generator,\n",
        "                 num_samples,\n",
        "                 latent_dim=100,\n",
        "                 num_classes=10, # Total classes the generator model knows\n",
        "                 desired_classes=None, # Optional: List of specific class indices to generate\n",
        "                 device=\"cpu\",\n",
        "                 image_col_name=\"image\",\n",
        "                 label_col_name=\"label\"):\n",
        "        \"\"\"\n",
        "        Generates a dataset using a conditional generative model, potentially\n",
        "        focusing on a subset of classes.\n",
        "\n",
        "        Args:\n",
        "            generator: The pre-trained generative model.\n",
        "            num_samples (int): Total number of images to generate across the desired classes.\n",
        "            latent_dim (int): Dimension of the latent space vector (z).\n",
        "            num_classes (int): The total number of classes the generator was trained on.\n",
        "                               This is crucial for correct label conditioning (e.g., one-hot dim).\n",
        "            desired_classes (list[int], optional): A list of integer class indices to generate.\n",
        "                                                  If None or empty, images for all classes\n",
        "                                                  (from 0 to num_classes-1) will be generated,\n",
        "                                                  distributed as evenly as possible.\n",
        "                                                  Defaults to None.\n",
        "            device (str): Device to run generation on ('cpu' or 'cuda').\n",
        "            image_col_name (str): Name for the image column in the output dictionary.\n",
        "            label_col_name (str): Name for the label column in the output dictionary.\n",
        "        \"\"\"\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        # Store the total number of classes the generator understands\n",
        "        self.total_num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.model_type = type(self.generator).__name__ # Get generator class name\n",
        "        self.image_col_name = image_col_name\n",
        "        self.label_col_name = label_col_name\n",
        "\n",
        "        # Determine the actual classes to generate based on desired_classes\n",
        "        if desired_classes is not None and len(desired_classes) > 0:\n",
        "            # Validate that desired classes are within the generator's known range\n",
        "            if not all(0 <= c < self.total_num_classes for c in desired_classes):\n",
        "                raise ValueError(f\"All desired classes must be integers between 0 and {self.total_num_classes - 1}\")\n",
        "            # Use only the unique desired classes, sorted for consistency\n",
        "            self._actual_classes_to_generate = sorted(list(set(desired_classes)))\n",
        "        else:\n",
        "            # If no specific classes desired, generate all classes\n",
        "            self._actual_classes_to_generate = list(range(self.total_num_classes))\n",
        "\n",
        "        # The 'classes' attribute of the dataset reflects only those generated\n",
        "        self.classes = self._actual_classes_to_generate\n",
        "        self.num_generated_classes = len(self.classes) # Number of classes being generated\n",
        "\n",
        "        if self.num_generated_classes == 0 and self.num_samples > 0:\n",
        "             raise ValueError(\"Cannot generate samples with an empty list of desired classes.\")\n",
        "        elif self.num_samples == 0:\n",
        "             print(\"Warning: num_samples is 0. Dataset will be empty.\")\n",
        "             self.images = torch.empty(0) # Adjust shape if known\n",
        "             self.labels = torch.empty(0, dtype=torch.long)\n",
        "        else:\n",
        "             # Generate the data only if needed\n",
        "             self.images, self.labels = self.generate_data()\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        \"\"\"Generates images and corresponding labels for the specified classes.\"\"\"\n",
        "        self.generator.eval()\n",
        "        self.generator.to(self.device)\n",
        "\n",
        "        # --- Create Labels ---\n",
        "        generated_labels_list = []\n",
        "        if self.num_generated_classes > 0:\n",
        "            # Distribute samples as evenly as possible among the desired classes\n",
        "            samples_per_class = self.num_samples // self.num_generated_classes\n",
        "            for cls in self._actual_classes_to_generate:\n",
        "                generated_labels_list.extend([cls] * samples_per_class)\n",
        "\n",
        "            # Handle remaining samples if num_samples is not perfectly divisible\n",
        "            num_remaining = self.num_samples - len(generated_labels_list)\n",
        "            if num_remaining > 0:\n",
        "                # Add remaining samples by randomly choosing from the desired classes\n",
        "                remainder_labels = random.choices(self._actual_classes_to_generate, k=num_remaining)\n",
        "                generated_labels_list.extend(remainder_labels)\n",
        "\n",
        "            # Shuffle labels for better distribution in batches later\n",
        "            random.shuffle(generated_labels_list)\n",
        "\n",
        "        # Convert labels list to tensor\n",
        "        labels = torch.tensor(generated_labels_list, dtype=torch.long, device=self.device)\n",
        "\n",
        "        # Double check label count (should match num_samples due to logic above)\n",
        "        if len(labels) != self.num_samples:\n",
        "             # This indicates an unexpected issue, potentially if num_generated_classes was 0 initially\n",
        "             # but num_samples > 0. Raise error or adjust. Let's adjust defensively.\n",
        "             print(f\"Warning: Label count mismatch. Expected {self.num_samples}, got {len(labels)}. Adjusting size.\")\n",
        "             if len(labels) > self.num_samples:\n",
        "                 labels = labels[:self.num_samples]\n",
        "             else:\n",
        "                 # Pad if too few (less likely with current logic unless num_generated_classes=0)\n",
        "                 num_needed = self.num_samples - len(labels)\n",
        "                 if self.num_generated_classes > 0:\n",
        "                      padding = torch.tensor(random.choices(self._actual_classes_to_generate, k=num_needed), dtype=torch.long, device=self.device)\n",
        "                      labels = torch.cat((labels, padding))\n",
        "                 # If no classes to generate from, labels tensor might remain smaller\n",
        "\n",
        "        # --- Create Latent Noise ---\n",
        "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "\n",
        "        # --- Generate Images in Batches ---\n",
        "        generated_images_list = []\n",
        "        # Consider making batch_size configurable\n",
        "        batch_size = min(1024, self.num_samples) if self.num_samples > 0 else 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, self.num_samples, batch_size):\n",
        "                z_batch = z[i : min(i + batch_size, self.num_samples)]\n",
        "                labels_batch = labels[i : min(i + batch_size, self.num_samples)]\n",
        "\n",
        "                # Skip if batch is empty (can happen if num_samples = 0)\n",
        "                if z_batch.shape[0] == 0:\n",
        "                    continue\n",
        "\n",
        "                # --- Condition the generator based on its type ---\n",
        "                if self.model_type == 'Generator': # Assumes input: concat(z, one_hot_label)\n",
        "                    # One-hot encode labels using the TOTAL number of classes the generator knows\n",
        "                    labels_one_hot_batch = F.one_hot(labels_batch, num_classes=self.total_num_classes).float()\n",
        "                    generator_input = torch.cat([z_batch, labels_one_hot_batch], dim=1)\n",
        "                    gen_imgs = self.generator(generator_input)\n",
        "                elif self.model_type in ('CGAN', 'F2U_GAN', 'F2U_GAN_CIFAR'): # Assumes input: z, label_index\n",
        "                    gen_imgs = self.generator(z_batch, labels_batch)\n",
        "                else:\n",
        "                    # Handle other potential generator architectures or raise an error\n",
        "                    raise NotImplementedError(f\"Generation logic not defined for model type: {self.model_type}\")\n",
        "\n",
        "                generated_images_list.append(gen_imgs.cpu()) # Move generated images to CPU\n",
        "\n",
        "        self.generator.cpu() # Move generator back to CPU after generation\n",
        "\n",
        "        # Concatenate all generated image batches\n",
        "        if generated_images_list:\n",
        "            all_gen_imgs = torch.cat(generated_images_list, dim=0)\n",
        "        else:\n",
        "            # If no images were generated (e.g., num_samples = 0)\n",
        "            # Create an empty tensor. Shape needs care - determine from generator or use placeholder.\n",
        "            # Let's attempt a placeholder [0, C, H, W] - requires knowing C, H, W.\n",
        "            # For now, a simple empty tensor. User might need to handle this downstream.\n",
        "            print(\"Warning: No images generated. Returning empty tensor for images.\")\n",
        "            all_gen_imgs = torch.empty(0)\n",
        "\n",
        "        return all_gen_imgs, labels.cpu() # Return images and labels (on CPU)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the actual number of samples generated\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(\"Dataset index out of range\")\n",
        "        return {\n",
        "            self.image_col_name: self.images[idx],\n",
        "            self.label_col_name: int(self.labels[idx]) # Return label as standard Python int\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c3d341",
      "metadata": {
        "id": "b1c3d341"
      },
      "outputs": [],
      "source": [
        "class UnconditionalGeneratedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 generator,\n",
        "                 num_samples,\n",
        "                 latent_dim=128,\n",
        "                 device=\"cpu\",\n",
        "                 image_col_name=\"image\"):\n",
        "        \"\"\"\n",
        "        Generates a dataset using an unconditional generative model.\n",
        "\n",
        "        Args:\n",
        "            generator: The pre-trained unconditional generative model.\n",
        "            num_samples (int): Total number of images to generate.\n",
        "            latent_dim (int): Dimension of the latent space vector (z).\n",
        "            device (str): Device to run generation on ('cpu' or 'cuda').\n",
        "            image_col_name (str): Name for the image column in the output dictionary.\n",
        "        \"\"\"\n",
        "        self.generator = generator\n",
        "        self.num_samples = num_samples\n",
        "        self.latent_dim = latent_dim\n",
        "        self.device = device\n",
        "        self.image_col_name = image_col_name\n",
        "\n",
        "        if self.num_samples < 0:\n",
        "            raise ValueError(\"num_samples must be non-negative\")\n",
        "        elif self.num_samples == 0:\n",
        "            print(\"Warning: num_samples is 0. Dataset will be empty.\")\n",
        "            self.images = torch.empty(0)\n",
        "        else:\n",
        "            self.images = self._generate_images()\n",
        "\n",
        "    def _generate_images(self):\n",
        "        self.generator.eval()\n",
        "        self.generator.to(self.device)\n",
        "\n",
        "        # Create latent noise\n",
        "        z = torch.randn(self.num_samples, self.latent_dim, device=self.device)\n",
        "\n",
        "        # Generate images in batches\n",
        "        generated_images = []\n",
        "        batch_size = min(1024, self.num_samples)\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, self.num_samples, batch_size):\n",
        "                z_batch = z[i : min(i + batch_size, self.num_samples)]\n",
        "                gen_imgs = self.generator(z_batch)\n",
        "                generated_images.append(gen_imgs.cpu())\n",
        "\n",
        "        self.generator.cpu()\n",
        "        return torch.cat(generated_images, dim=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(\"Dataset index out of range\")\n",
        "        return { self.image_col_name: self.images[idx] }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9452cf54",
      "metadata": {
        "id": "9452cf54"
      },
      "outputs": [],
      "source": [
        "def generate_plot(net, device, round_number, client_id = None, examples_per_class: int=5, classes: int=10, latent_dim: int=100):\n",
        "    \"\"\"Gera plot de imagens de cada classe\"\"\"\n",
        "\n",
        "    net_type = type(net).__name__\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    batch_size = examples_per_class * classes\n",
        "    dataset = \"mnist\" if  not net_type == \"F2U_GAN_CIFAR\" else \"cifar10\"\n",
        "\n",
        "    latent_vectors = torch.randn(batch_size, latent_dim, device=device)\n",
        "    labels = torch.tensor([i for i in range(classes) for _ in range(examples_per_class)], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if net_type == \"Generator\":\n",
        "            labels_one_hot = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            generated_images = net(torch.cat([latent_vectors, labels_one_hot], dim=1))\n",
        "        else:\n",
        "            generated_images = net(latent_vectors, labels)\n",
        "\n",
        "    # Criar uma figura com 10 linhas e 5 colunas de subplots\n",
        "    fig, axes = plt.subplots(classes, examples_per_class, figsize=(5, 9))\n",
        "\n",
        "    # Adiciona título no topo da figura\n",
        "    if isinstance(client_id, int):\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number} | Client: {client_id}\", ha=\"center\", fontsize=12)\n",
        "    else:\n",
        "        fig.text(0.5, 0.98, f\"Round: {round_number}\", ha=\"center\", fontsize=12)\n",
        "\n",
        "    # Exibir as imagens nos subplots\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if dataset == \"mnist\":\n",
        "            ax.imshow(generated_images[i, 0, :, :], cmap='gray')\n",
        "        else:\n",
        "            images = (generated_images[i] + 1)/2\n",
        "            ax.imshow(images.permute(1, 2, 0).clamp(0,1))\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    # Ajustar o layout antes de calcular as posições\n",
        "    plt.tight_layout(rect=[0.05, 0, 1, 0.96])\n",
        "\n",
        "    # Reduzir espaço entre colunas\n",
        "    # plt.subplots_adjust(wspace=0.05)\n",
        "\n",
        "    # Adicionar os rótulos das classes corretamente alinhados\n",
        "    fig.canvas.draw()  # Atualiza a renderização para obter posições corretas\n",
        "    for row in range(classes):\n",
        "        # Obter posição do subplot em coordenadas da figura\n",
        "        bbox = axes[row, 0].get_window_extent(fig.canvas.get_renderer())\n",
        "        pos = fig.transFigure.inverted().transform([(bbox.x0, bbox.y0), (bbox.x1, bbox.y1)])\n",
        "        center_y = (pos[0, 1] + pos[1, 1]) / 2  # Centro exato da linha\n",
        "\n",
        "        # Adicionar o rótulo\n",
        "        fig.text(0.04, center_y, str(row), va='center', fontsize=12, color='black')\n",
        "\n",
        "    IN_COLAB = False\n",
        "    try:\n",
        "        # Tenta importar um módulo específico do Colab\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if IN_COLAB:\n",
        "        if isinstance(client_id, int):\n",
        "            fig.savefig(os.path.join(save_dir, f\"{dataset}_{net_type}_r{round_number}_c{client_id}.png\"))\n",
        "            print(\"Imagem do cliente salva no drive\")\n",
        "        else:\n",
        "            fig.savefig(os.path.join(save_dir, f\"{dataset}{net_type}_r{round_number}.png\"))\n",
        "            print(\"Imagem do servidor salva no drive\")\n",
        "    else:\n",
        "        if isinstance(client_id, int):\n",
        "            fig.savefig(f\"{dataset}{net_type}_r{round_number}_c{client_id}.png\")\n",
        "            print(\"Imagem do cliente salva\")\n",
        "        else:\n",
        "            fig.savefig(f\"{dataset}{net_type}_r{round_number}.png\")\n",
        "            print(\"Imagem do servidor salva\")\n",
        "    plt.close(fig)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a88e7c6",
      "metadata": {
        "id": "3a88e7c6"
      },
      "outputs": [],
      "source": [
        "def plot_unconditional_generated(\n",
        "        generator,\n",
        "        device,\n",
        "        total_samples,\n",
        "        samples_per_row=5,\n",
        "        latent_dim=100,\n",
        "        save_path=None,\n",
        "        round_number=None):\n",
        "    \"\"\"\n",
        "    Generates and plots images from an unconditional generator in a grid.\n",
        "\n",
        "    Args:\n",
        "        generator: The unconditional torch generator model (z -> image).\n",
        "        device: Device to run generation on ('cpu' or 'cuda').\n",
        "        total_samples (int): Number of images to generate.\n",
        "        samples_per_row (int): Number of images per row in the grid.\n",
        "        latent_dim (int): Dimension of latent vector.\n",
        "        save_path (str, optional): Filepath to save the figure. If None, just shows plot.\n",
        "    \"\"\"\n",
        "\n",
        "    generator.eval()\n",
        "    generator.to(device)\n",
        "\n",
        "    # Sample latent vectors\n",
        "    z = torch.randn(total_samples, latent_dim, device=device)\n",
        "    with torch.no_grad():\n",
        "        imgs = generator(z)\n",
        "\n",
        "    # Determine grid size\n",
        "    cols = samples_per_row\n",
        "    rows = math.ceil(total_samples / cols)\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols-2*cols/(rows+cols), rows-1*rows/(rows+cols)))\n",
        "    axes = axes.flatten() if total_samples > 1 else [axes]\n",
        "\n",
        "    fig.text(0.5, 0.99, f\"Round: {round_number}\", ha=\"center\", fontsize=11)\n",
        "\n",
        "    for idx in range(rows * cols):\n",
        "        ax = axes[idx]\n",
        "        ax.axis('off')\n",
        "        if idx < total_samples:\n",
        "            img = imgs[idx]\n",
        "            # Assume (C, H, W) and single-channel\n",
        "            ax.imshow(img[0], cmap='gray')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        fig.savefig(save_path)\n",
        "        print(f\"Figure saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd1a6ff",
      "metadata": {
        "id": "5bd1a6ff"
      },
      "source": [
        "## Importa Pacotes Federado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7c02c2",
      "metadata": {
        "id": "3e7c02c2"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !pip install flwr_datasets\n",
        "    !pip install flwr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "9793cd9a",
      "metadata": {
        "id": "9793cd9a"
      },
      "outputs": [],
      "source": [
        "from flwr_datasets.partitioner import DirichletPartitioner, IidPartitioner\n",
        "from flwr_datasets.visualization import plot_label_distributions\n",
        "from flwr_datasets import FederatedDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ac2e55",
      "metadata": {
        "id": "a5ac2e55"
      },
      "source": [
        "## Particionador por classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "3acd60f5",
      "metadata": {
        "id": "3acd60f5"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Flower Labs GmbH. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Class-based partitioner for Hugging Face Datasets.\"\"\"\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from typing import Optional, List\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from flwr_datasets.partitioner.partitioner import Partitioner  # Assuming this is in the package structure\n",
        "\n",
        "\n",
        "class ClassPartitioner(Partitioner):\n",
        "    \"\"\"Partitions a dataset by class, ensuring each class appears in exactly one partition.\n",
        "\n",
        "    Attributes:\n",
        "        num_partitions (int): Total number of partitions to create\n",
        "        seed (int, optional): Random seed for reproducibility\n",
        "        label_column (str): Name of the column containing class labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_partitions: int,\n",
        "        seed: Optional[int] = None,\n",
        "        label_column: str = \"label\"\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self._num_partitions = num_partitions\n",
        "        self._seed = seed\n",
        "        self._label_column = label_column\n",
        "        self._partition_indices: Optional[List[List[int]]] = None\n",
        "\n",
        "    def _create_partitions(self) -> None:\n",
        "        \"\"\"Create class-based partitions and store indices.\"\"\"\n",
        "        # Extract labels from dataset\n",
        "        labels = self.dataset[self._label_column]\n",
        "\n",
        "        # Group indices by class\n",
        "        class_indices = defaultdict(list)\n",
        "        for idx, label in enumerate(labels):\n",
        "            class_indices[label].append(idx)\n",
        "\n",
        "        classes = list(class_indices.keys())\n",
        "        num_classes = len(classes)\n",
        "\n",
        "        # Validate number of partitions\n",
        "        if self._num_partitions > num_classes:\n",
        "            raise ValueError(\n",
        "                f\"Cannot create {self._num_partitions} partitions with only {num_classes} classes. \"\n",
        "                f\"Reduce partitions to ≤ {num_classes}.\"\n",
        "            )\n",
        "\n",
        "        # Shuffle classes for random distribution\n",
        "        rng = random.Random(self._seed)\n",
        "        rng.shuffle(classes)\n",
        "\n",
        "        # Split classes into partitions\n",
        "        partition_classes = np.array_split(classes, self._num_partitions)\n",
        "\n",
        "        # Create index lists for each partition\n",
        "        self._partition_indices = []\n",
        "        for class_group in partition_classes:\n",
        "            indices = []\n",
        "            for cls in class_group:\n",
        "                indices.extend(class_indices[cls])\n",
        "            self._partition_indices.append(indices)\n",
        "\n",
        "    @property\n",
        "    def dataset(self) -> Dataset:\n",
        "        return super().dataset\n",
        "\n",
        "    @dataset.setter\n",
        "    def dataset(self, value: Dataset) -> None:\n",
        "        # Use parent setter for basic validation\n",
        "        super(ClassPartitioner, ClassPartitioner).dataset.fset(self, value)\n",
        "\n",
        "        # Create partitions once dataset is set\n",
        "        self._create_partitions()\n",
        "\n",
        "    def load_partition(self, partition_id: int) -> Dataset:\n",
        "        \"\"\"Load a partition containing exclusive classes.\n",
        "\n",
        "        Args:\n",
        "            partition_id: The ID of the partition to load (0-based index)\n",
        "\n",
        "        Returns:\n",
        "            Dataset: Subset of the dataset containing only the specified partition's data\n",
        "        \"\"\"\n",
        "        if not self.is_dataset_assigned():\n",
        "            raise RuntimeError(\"Dataset must be assigned before loading partitions\")\n",
        "        if partition_id < 0 or partition_id >= self.num_partitions:\n",
        "            raise ValueError(f\"Invalid partition ID: {partition_id}\")\n",
        "\n",
        "        return self.dataset.select(self._partition_indices[partition_id])\n",
        "\n",
        "    @property\n",
        "    def num_partitions(self) -> int:\n",
        "        return self._num_partitions\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f\"ClassPartitioner(num_partitions={self._num_partitions}, \"\n",
        "                f\"seed={self._seed}, label_column='{self._label_column}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ee55f5",
      "metadata": {
        "id": "a1ee55f5"
      },
      "source": [
        "## Carrega e divide dados entre clientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "6fdba10d",
      "metadata": {
        "id": "6fdba10d"
      },
      "outputs": [],
      "source": [
        "num_partitions = 4\n",
        "alpha_dir = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd9c472",
      "metadata": {
        "id": "6cd9c472"
      },
      "source": [
        "Rodar somente o particionador desejado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "42644935",
      "metadata": {
        "id": "42644935"
      },
      "outputs": [],
      "source": [
        "partitioner = ClassPartitioner(num_partitions=num_partitions, seed=42, label_column=\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312626b5",
      "metadata": {
        "id": "312626b5"
      },
      "outputs": [],
      "source": [
        "partitioner = DirichletPartitioner(\n",
        "    num_partitions=num_partitions,\n",
        "    partition_by=\"label\",\n",
        "    alpha=alpha_dir,\n",
        "    min_partition_size=0,\n",
        "    self_balancing=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8582a179",
      "metadata": {
        "id": "8582a179"
      },
      "outputs": [],
      "source": [
        "partitioner = IidPartitioner(\n",
        "    num_partitions=num_partitions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f9a435",
      "metadata": {
        "id": "a3f9a435"
      },
      "outputs": [],
      "source": [
        "fds = FederatedDataset(\n",
        "    dataset=\"mnist\",\n",
        "    partitioners={\"train\": partitioner}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "7d21553c",
      "metadata": {
        "id": "7d21553c"
      },
      "outputs": [],
      "source": [
        "fds = FederatedDataset(\n",
        "    dataset=\"cifar10\",\n",
        "    partitioners={\"train\": partitioner}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "026da6c7",
      "metadata": {
        "id": "026da6c7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "partitioner = fds.partitioners[\"train\"]\n",
        "figure, axis, dataframe = plot_label_distributions(\n",
        "    partitioner=partitioner,\n",
        "    label_name=\"label\",\n",
        "    title=\"IID\",\n",
        "    legend=True,\n",
        "    verbose_labels=True,\n",
        "    size_unit=\"absolute\",\n",
        "    partition_id_axis=\"x\",\n",
        "    legend_kwargs={'fontsize': 14, 'title_fontsize': 14},\n",
        "    figsize=(6, 5)\n",
        ")\n",
        "\n",
        "axis.title.set_fontsize(14)\n",
        "\n",
        "# 2. Modify the returned 'axis' object for labels and ticks\n",
        "# Set font size for the axis titles (e.g., \"Partition ID\", \"Count\")\n",
        "axis.xaxis.label.set_fontsize(14)\n",
        "axis.yaxis.label.set_fontsize(14)\n",
        "\n",
        "# Set font size for the tick numbers on both axes\n",
        "axis.tick_params(axis='both', labelsize=14)\n",
        "\n",
        "# 3. Adjust layout and show the final plot\n",
        "figure.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ff7171",
      "metadata": {
        "id": "93ff7171"
      },
      "outputs": [],
      "source": [
        "train_partitions = [fds.load_partition(i, split=\"train\") for i in range(num_partitions)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j-L_He1Qvz0e",
      "metadata": {
        "id": "j-L_He1Qvz0e"
      },
      "source": [
        "Rodar proxima celula somente se quiser testar com dataset reduzido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w2n6dMxcvx2k",
      "metadata": {
        "id": "w2n6dMxcvx2k"
      },
      "outputs": [],
      "source": [
        "# num_samples = [int(len(train_partition)/10) for train_partition in train_partitions]\n",
        "# train_partitions = [train_partition.select(range(n)) for train_partition, n in zip(train_partitions, num_samples)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b7cf8f",
      "metadata": {
        "id": "d1b7cf8f"
      },
      "source": [
        "Cria dicionario de label para cliente para controle do dmax_mismatch. Tive que colocar aqui antes do apply_transform para não dar erro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07fdcb9",
      "metadata": {
        "id": "e07fdcb9"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b106654b",
      "metadata": {
        "id": "b106654b"
      },
      "outputs": [],
      "source": [
        "min_lbl_count = 0.05\n",
        "class_labels = train_partitions[0].info.features[\"label\"]\n",
        "labels_str = class_labels.names\n",
        "label_to_client = {lbl: [] for lbl in labels_str}\n",
        "for idx, ds in enumerate(train_partitions):\n",
        "    counts = Counter(ds['label'])\n",
        "    for label, cnt in counts.items():\n",
        "        if cnt / len(ds) >= min_lbl_count:\n",
        "            label_to_client[class_labels.int2str(label)].append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56dcbb9b",
      "metadata": {
        "id": "56dcbb9b"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df48b029",
      "metadata": {
        "id": "df48b029"
      },
      "outputs": [],
      "source": [
        "pytorch_transforms = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbbc667a",
      "metadata": {
        "id": "cbbc667a"
      },
      "outputs": [],
      "source": [
        "# Para CIFAR-10: 3 canais, normalização média=0.5 e std=0.5\n",
        "pytorch_transforms = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    # batch[\"image\"] é uma lista de PIL.Image ou tensores em H×W×C\n",
        "    # aplicamos o mesmo transform a cada imagem e depois empilhamos\n",
        "    batch[\"img\"] = torch.stack([pytorch_transforms(img) for img in batch[\"img\"]])\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34de09aa",
      "metadata": {
        "id": "34de09aa"
      },
      "outputs": [],
      "source": [
        "train_partitions = [train_partition.with_transform(apply_transforms) for train_partition in train_partitions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MwJAQ213fi-w",
      "metadata": {
        "id": "MwJAQ213fi-w"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CuBEfRZ6fX8i",
      "metadata": {
        "id": "CuBEfRZ6fX8i"
      },
      "outputs": [],
      "source": [
        "test_frac = 0.2\n",
        "client_datasets = []\n",
        "\n",
        "for train_part in train_partitions:\n",
        "    total     = len(train_part)\n",
        "    test_size = int(total * test_frac)\n",
        "    train_size = total - test_size\n",
        "\n",
        "    client_train, client_test = random_split(\n",
        "        train_part,\n",
        "        [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42),\n",
        "    )\n",
        "\n",
        "    client_datasets.append({\n",
        "        \"train\": client_train,\n",
        "        \"test\":  client_test,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ae0635",
      "metadata": {
        "id": "b0ae0635"
      },
      "source": [
        "## Inicializa modelos e otimizadores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b818e92",
      "metadata": {
        "id": "1b818e92"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5be9636",
      "metadata": {
        "id": "b5be9636"
      },
      "source": [
        "Rodar somente o modelo desejado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e76e482",
      "metadata": {
        "id": "8e76e482"
      },
      "outputs": [],
      "source": [
        "models = [CGAN() for i in range(num_partitions)]\n",
        "gen = CGAN().to(device)\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70debb0f",
      "metadata": {
        "id": "70debb0f"
      },
      "outputs": [],
      "source": [
        "models = [F2U_GAN(condition=True, seed=42) for i in range(num_partitions)]\n",
        "gen = F2U_GAN(condition=True, seed=42).to(device)\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52f5343",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [F2U_GAN_SlowDisc(condition=True, seed=42) for i in range(num_partitions)]\n",
        "gen = F2U_GAN(condition=True, seed=42).to(device)\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a813dfe",
      "metadata": {
        "id": "9a813dfe"
      },
      "outputs": [],
      "source": [
        "models = [F2U_GAN_CIFAR(condition=True, seed=42) for i in range(num_partitions)]\n",
        "gen = F2U_GAN_CIFAR(condition=True, seed=42).to(device)\n",
        "optim_G = torch.optim.Adam(gen.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f37272",
      "metadata": {
        "id": "70f37272"
      },
      "outputs": [],
      "source": [
        "optim_Ds = [\n",
        "    torch.optim.Adam(model.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "# scheduler_D = torch.optim.lr_scheduler.StepLR(optim_D, step_size=5, gamma=0.9)\n",
        "# scheduler_G = torch.optim.lr_scheduler.StepLR(optim_G, step_size=5, gamma=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e08c26a0",
      "metadata": {
        "id": "e08c26a0"
      },
      "source": [
        "Inicializa lambda para F2A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893b45d4",
      "metadata": {
        "id": "893b45d4"
      },
      "outputs": [],
      "source": [
        "# initial λ* (unconstrained), wrap with ReLU to keep λ ≥ 0\n",
        "lambda_star = nn.Parameter(torch.tensor(0.1, device=device))\n",
        "relu = nn.ReLU()\n",
        "\n",
        "beta = 0.1  # same β as in the paper\n",
        "\n",
        "# now make your generator optimizer also update lambda_star\n",
        "# (so its gradient from the βλ² term can flow)\n",
        "optim_G = torch.optim.Adam(\n",
        "    list(gen.parameters()) + [lambda_star],\n",
        "    lr=2e-4, betas=(0.5, 0.999)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3a7e9e",
      "metadata": {
        "id": "7a3a7e9e"
      },
      "source": [
        "# Treinamento dos modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b38a6962",
      "metadata": {
        "id": "b38a6962"
      },
      "source": [
        "## Cria chunks para o treinamento alternado entre discriminadora e geradora ser mais constante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b5d8adc",
      "metadata": {
        "id": "9b5d8adc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2505ab00",
      "metadata": {
        "id": "2505ab00"
      },
      "source": [
        "Quanto menos chunks, mais dados em cada chunk e mais dados são treinados na discriminadora antes de treinar a geradora. No paper do F2U, não está claro como os treinamentos são alternados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FTvOVoSLVpta",
      "metadata": {
        "id": "FTvOVoSLVpta"
      },
      "outputs": [],
      "source": [
        "# prompt: set each train partition as the only first minimum lenght of the partitions samples, the partitions have same lenght\n",
        "\n",
        "min_len = min(len(p) for p in train_partitions)\n",
        "train_partitions = [p.select(range(min_len)) for p in train_partitions]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CVz-ThoCVfoh",
      "metadata": {
        "id": "CVz-ThoCVfoh"
      },
      "outputs": [],
      "source": [
        "for train_partition in train_partitions:\n",
        "  print(len(train_partition))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff747284",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_chunks = 50\n",
        "seed = 42  # escolha qualquer inteiro para reprodutibilidade\n",
        "client_chunks = []\n",
        "\n",
        "for train_partition in client_datasets:\n",
        "    dataset = train_partition[\"train\"]\n",
        "    n = len(dataset)\n",
        "\n",
        "    # 1) embaralha os índices com seed fixa\n",
        "    indices = list(range(n))\n",
        "    random.seed(seed)\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    # 2) calcula tamanho aproximado de cada chunk\n",
        "    chunk_size = math.ceil(n / num_chunks)\n",
        "\n",
        "    # 3) divide em chunks usando fatias dos índices embaralhados\n",
        "    chunks = []\n",
        "    for i in range(num_chunks):\n",
        "        start = i * chunk_size\n",
        "        end = min((i + 1) * chunk_size, n)\n",
        "        chunk_indices = indices[start:end]\n",
        "        chunks.append(Subset(dataset, chunk_indices))\n",
        "\n",
        "    client_chunks.append(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o6WJTKp6B5vD",
      "metadata": {
        "id": "o6WJTKp6B5vD"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "client_test_loaders = [DataLoader(dataset=ds[\"test\"], batch_size=batch_size, shuffle=True) for ds in client_datasets]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ab8d65e",
      "metadata": {
        "id": "4ab8d65e"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e70bd3c8",
      "metadata": {
        "id": "e70bd3c8"
      },
      "outputs": [],
      "source": [
        "nets = [Net(42).to(device) for _ in range(num_partitions)]\n",
        "optims = [torch.optim.Adam(net.parameters(), lr=0.01) for net in nets]\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc668cf",
      "metadata": {
        "id": "bbc668cf"
      },
      "outputs": [],
      "source": [
        "testpartition = fds.load_split(\"test\")\n",
        "testpartition = testpartition.with_transform(apply_transforms)\n",
        "testloader = DataLoader(testpartition, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e86ba4bc",
      "metadata": {
        "id": "e86ba4bc"
      },
      "source": [
        "Carregar modelo pré-treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3c145b",
      "metadata": {
        "id": "3d3c145b"
      },
      "outputs": [],
      "source": [
        "global_net = Net(42).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b66f9a29",
      "metadata": {
        "id": "b66f9a29"
      },
      "outputs": [],
      "source": [
        "checkpoint_loaded = torch.load(\"../Experimentos/NB_F2U/GeraFed_4c_NIIDClass/MNIST/checkpoint_epoch100.pth\")\n",
        "\n",
        "global_net.load_state_dict(checkpoint_loaded['alvo_state_dict'])\n",
        "global_net.to(device)\n",
        "for optim, state in zip(optims, checkpoint_loaded['optimizer_alvo_state_dict']):\n",
        "    optim.load_state_dict(state)\n",
        "\n",
        "gen.load_state_dict(checkpoint_loaded[\"gen_state_dict\"])\n",
        "gen.to(device)\n",
        "optim_G.load_state_dict(checkpoint_loaded[\"optim_G_state_dict\"])\n",
        "\n",
        "for model, optim_d, state_model, state_optim in zip(models, optim_Ds, checkpoint_loaded[\"discs_state_dict\"], checkpoint_loaded[\"optim_Ds_state_dict:\"]):\n",
        "    model.load_state_dict(state_model)\n",
        "    model.to(device)\n",
        "    optim_d.load_state_dict(state_optim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1265b7",
      "metadata": {
        "id": "aa1265b7"
      },
      "source": [
        "Não esquecer de reinicializar os modelos e otimizadores se for reinicializar o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d116bedf",
      "metadata": {
        "id": "d116bedf"
      },
      "outputs": [],
      "source": [
        "from flwr.server.strategy.aggregate import aggregate_inplace\n",
        "from flwr.common import FitRes, Status, Code, ndarrays_to_parameters\n",
        "from collections import OrderedDict, defaultdict\n",
        "from torch.utils.data import ConcatDataset\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac00db4",
      "metadata": {},
      "source": [
        "### GeraFed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1447e9f",
      "metadata": {
        "collapsed": true,
        "id": "c1447e9f"
      },
      "outputs": [],
      "source": [
        "wgan = False\n",
        "f2a = False\n",
        "epochs = 1\n",
        "losses_dict = {\"g_losses_chunk\": [],\n",
        "               \"d_losses_chunk\": [],\n",
        "               \"g_losses_round\": [],\n",
        "               \"d_losses_round\": [],\n",
        "               \"net_loss_chunk\": [],\n",
        "               \"net_acc_chunk\": [],\n",
        "               \"net_loss_round\": [],\n",
        "               \"net_acc_round\": [],\n",
        "               \"time_chunk\": [],\n",
        "               \"time_round\": [],\n",
        "               \"net_time\": [],\n",
        "               \"disc_time\": [],\n",
        "               \"gen_time\": [],\n",
        "               \"img_syn_time\": [],\n",
        "               \"track_mismatch_time\": []\n",
        "               }\n",
        "\n",
        "epoch_bar = tqdm(range(0, epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "\n",
        "batch_size_gen = 1\n",
        "batch_tam = 32\n",
        "extra_g_e = 20\n",
        "latent_dim = 128\n",
        "num_classes = 10\n",
        "if type(nets[0]).__name__ == \"Net\":\n",
        "  image = \"image\"\n",
        "else:\n",
        "  image = \"img\"\n",
        "\n",
        "if IN_COLAB:\n",
        "  acc_filename = os.path.join(save_dir,\"accuracy_report.txt\")\n",
        "  loss_filename = os.path.join(save_dir, \"losses.json\")\n",
        "  dmax_mismatch_log = os.path.join(save_dir, \"dmax_mismatch.txt\")\n",
        "  lambda_log = os.path.join(save_dir, \"lambda_log.txt\")\n",
        "\n",
        "else:\n",
        "  acc_filename = \"accuracy_report.txt\"\n",
        "  loss_filename = \"losses.json\"\n",
        "  dmax_mismatch_log = \"dmax_mismatch.txt\"\n",
        "  lambda_log = \"lambda_log.txt\"\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "  epoch_start_time = time.time()\n",
        "  mismatch_count = 0\n",
        "  total_checked = 0\n",
        "  g_loss_c = 0.0\n",
        "  d_loss_c = 0.0\n",
        "  total_d_samples = 0  # Amostras totais processadas pelos discriminadores\n",
        "  total_g_samples = 0  # Amostras totais processadas pelo gerador\n",
        "  params = []\n",
        "  results = []\n",
        "\n",
        "  chunk_bar = tqdm(range(num_chunks), desc=\"Chunks\", leave=True, position=1)\n",
        "\n",
        "  for chunk_idx in chunk_bar:\n",
        "    chunk_start_time = time.time()\n",
        "    # ====================================================================\n",
        "    # Treino dos Discriminadores (clientes) no bloco atual\n",
        "    # ====================================================================\n",
        "    d_loss_b = 0\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "\n",
        "    client_bar = tqdm(enumerate(zip(nets, models, client_chunks)), desc=\"Clients\", leave=True, position=2)\n",
        "\n",
        "    for cliente, (net, disc, chunks) in client_bar:\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      if len(chunk_dataset) == 0:\n",
        "        print(f\"Chunk {chunk_idx} for client {cliente} is empty, skipping.\")\n",
        "        continue\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size=batch_tam, shuffle=True)\n",
        "      if chunk_idx == 0:\n",
        "        client_eval_time = time.time()\n",
        "        # Evaluation in client test\n",
        "        # Initialize counters\n",
        "        class_correct = defaultdict(int)\n",
        "        class_total = defaultdict(int)\n",
        "        predictions_counter = defaultdict(int)\n",
        "\n",
        "        global_net.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in client_test_loaders[cliente]:\n",
        "                images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "                outputs = global_net(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Update counts for each sample in batch\n",
        "                for true_label, pred_label in zip(labels, predicted):\n",
        "                    true_idx = true_label.item()\n",
        "                    pred_idx = pred_label.item()\n",
        "\n",
        "                    class_total[true_idx] += 1\n",
        "                    predictions_counter[pred_idx] += 1\n",
        "\n",
        "                    if true_idx == pred_idx:\n",
        "                        class_correct[true_idx] += 1\n",
        "\n",
        "            # Create results dictionary\n",
        "            results_metrics = {\n",
        "                \"class_metrics\": {},\n",
        "                \"overall_accuracy\": None,\n",
        "                \"prediction_distribution\": dict(predictions_counter)\n",
        "            }\n",
        "\n",
        "            # Calculate class-wise metrics\n",
        "            for i in range(num_classes):\n",
        "                metrics = {\n",
        "                    \"samples\": class_total[i],\n",
        "                    \"predictions\": predictions_counter[i],\n",
        "                    \"accuracy\": class_correct[i] / class_total[i] if class_total[i] > 0 else \"N/A\"\n",
        "                }\n",
        "                results_metrics[\"class_metrics\"][f\"class_{i}\"] = metrics\n",
        "\n",
        "            # Calculate overall accuracy\n",
        "            total_samples = sum(class_total.values())\n",
        "            results_metrics[\"overall_accuracy\"] = sum(class_correct.values()) / total_samples\n",
        "\n",
        "            # Save to txt file\n",
        "            with open(acc_filename, \"a\") as f:\n",
        "                f.write(f\"Epoch {epoch + 1} - Client {cliente}\\n\")\n",
        "                # Header with fixed widths\n",
        "                f.write(\"{:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
        "                    \"Class\", \"Accuracy\", \"Samples\", \"Predictions\"))\n",
        "                f.write(\"-\"*45 + \"\\n\")\n",
        "\n",
        "                # Class rows with consistent formatting\n",
        "                for cls in range(num_classes):\n",
        "                    metrics = results_metrics[\"class_metrics\"][f\"class_{cls}\"]\n",
        "\n",
        "                    # Format accuracy (handle \"N/A\" case)\n",
        "                    accuracy = (f\"{metrics['accuracy']:.4f}\"\n",
        "                              if isinstance(metrics['accuracy'], float)\n",
        "                              else \"  N/A  \")\n",
        "\n",
        "                    f.write(\"{:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
        "                        f\"Class {cls}\",\n",
        "                        accuracy,\n",
        "                        metrics['samples'],\n",
        "                        metrics['predictions']\n",
        "                    ))\n",
        "\n",
        "                # Footer with alignment\n",
        "                f.write(\"\\n{:<20} {:.4f}\".format(\"Overall Accuracy:\", results_metrics[\"overall_accuracy\"]))\n",
        "                f.write(\"\\n{:<20} {}\".format(\"Total Samples:\", total_samples))\n",
        "                f.write(\"\\n{:<20} {}\".format(\"Total Predictions:\", sum(predictions_counter.values())))\n",
        "                f.write(\"\\n{:<20} {:.4f}\".format(\"Client Evaluation Time:\", time.time() - client_eval_time))\n",
        "                f.write(\"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        print(\"Results saved to accuracy_report.txt\")\n",
        "\n",
        "      # Treinar o discriminador no bloco\n",
        "      net.load_state_dict(global_net.state_dict(), strict=True)\n",
        "      net.to(device)\n",
        "      net.train()\n",
        "      disc.to(device)\n",
        "      optim = optims[cliente]\n",
        "      optim_D = optim_Ds[cliente]\n",
        "\n",
        "      start_img_syn_time = time.time()\n",
        "      num_samples = int(13 * (math.exp(0.01*epoch) - 1) / (math.exp(0.01*50) - 1)) * 10\n",
        "      generated_dataset = GeneratedDataset(generator=gen.to(\"cpu\"), num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\", image_col_name=image)\n",
        "      gen.to(device)\n",
        "      cmb_ds = ConcatDataset([chunk_dataset, generated_dataset])\n",
        "      combined_dataloader= DataLoader(cmb_ds, batch_size=batch_tam, shuffle=True)\n",
        "\n",
        "      img_syn_time = time.time() - start_img_syn_time\n",
        "\n",
        "      batch_bar_net = tqdm(combined_dataloader, desc=\"Batches\", leave=True, position=3)\n",
        "      start_net_time = time.time()\n",
        "      for batch in batch_bar_net:\n",
        "        images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        if batch_size == 1:\n",
        "          print(\"Batch size is 1, skipping batch\")\n",
        "          continue\n",
        "        optim.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "      net_time = time.time() - start_net_time\n",
        "\n",
        "      batch_bar = tqdm(chunk_loader, desc=\"Batches\", leave=True, position=4)\n",
        "\n",
        "      start_disc_time = time.time()\n",
        "      for batch in batch_bar:\n",
        "          images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "          batch_size = images.size(0)\n",
        "          if batch_size == 1:\n",
        "            print(\"Batch size is 1, skipping batch\")\n",
        "            continue\n",
        "\n",
        "          real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "          fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "          z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "          x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "\n",
        "          # Train D\n",
        "          optim_D.zero_grad()\n",
        "\n",
        "          if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            x_fake_l = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "\n",
        "            # Adicionar labels ao images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = x_fake_l.view(x_fake_l.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            images = torch.cat([images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z = torch.cat([z_noise, x_fake_l], dim=1)\n",
        "            fake_images = gen(z).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            d_loss = discriminator_loss(disc(images), disc(fake_images)) + 10 * gradient_penalty(disc, images, fake_images)\n",
        "\n",
        "          else:\n",
        "            # Dados Reais\n",
        "            y_real = disc(images, labels)\n",
        "            d_real_loss = disc.loss(y_real, real_ident)\n",
        "\n",
        "            # Dados Falsos\n",
        "            x_fake = gen(z_noise, x_fake_labels).detach()\n",
        "            y_fake_d = disc(x_fake, x_fake_labels)\n",
        "            d_fake_loss = disc.loss(y_fake_d, fake_ident)\n",
        "\n",
        "            # Loss total e backprop\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "          d_loss.backward()\n",
        "          #torch.nn.utils.clip_grad_norm_(disc.discriminator.parameters(), max_norm=1.0)\n",
        "          optim_D.step()\n",
        "          d_loss_b += d_loss.item()\n",
        "          total_chunk_samples += 1\n",
        "      disc_time = time.time() - start_disc_time  \n",
        "\n",
        "      params.append(ndarrays_to_parameters([val.cpu().numpy() for _, val in net.state_dict().items()]))\n",
        "      results.append((cliente, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=params[cliente], num_examples=len(chunk_loader.dataset), metrics={})))\n",
        "\n",
        "    aggregated_ndarrays = aggregate_inplace(results)\n",
        "\n",
        "    params_dict = zip(global_net.state_dict().keys(), aggregated_ndarrays)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
        "    global_net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    # Evaluation\n",
        "    if chunk_idx % 10 == 0:\n",
        "        global_net.eval()\n",
        "        correct, loss = 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                images = batch[image].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                outputs = global_net(images)\n",
        "                loss += criterion(outputs, labels).item()\n",
        "                correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        accuracy = correct / len(testloader.dataset)\n",
        "        losses_dict[\"net_loss_chunk\"].append(loss / len(testloader))\n",
        "        losses_dict[\"net_acc_chunk\"].append(accuracy)\n",
        "\n",
        "\n",
        "    # Média da perda dos discriminadores neste chunk\n",
        "    avg_d_loss_chunk = d_loss_b / total_chunk_samples if total_chunk_samples > 0 else 0.0\n",
        "    losses_dict[\"d_losses_chunk\"].append(avg_d_loss_chunk)\n",
        "    d_loss_c += avg_d_loss_chunk * total_chunk_samples\n",
        "    total_d_samples += total_chunk_samples\n",
        "\n",
        "    chunk_g_loss = 0.0\n",
        "\n",
        "    epoch_gen_bar = tqdm(range(extra_g_e), desc=\"Gerador\", leave=True, position=2)\n",
        "\n",
        "    start_gen_time = time.time()\n",
        "    for g_epoch in epoch_gen_bar:\n",
        "      # Train G\n",
        "      optim_G.zero_grad()\n",
        "\n",
        "      # Gera dados falsos\n",
        "      z_noise = torch.randn(batch_size_gen, latent_dim, device=device)\n",
        "      x_fake_labels = torch.randint(0, 10, (batch_size_gen,), device=device)\n",
        "      label = int(x_fake_labels.item())\n",
        "\n",
        "      if wgan:\n",
        "        x_fake_labels = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "        z_noise = torch.cat([z_noise, x_fake_labels], dim=1)\n",
        "        fake_images = gen(z_noise)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        image_fake_labels = x_fake_labels.view(x_fake_labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "        fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "        y_fake_gs = [model(fake_images.detach()) for model in models]\n",
        "\n",
        "      else:\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "        if f2a:\n",
        "          y_fakes = []\n",
        "          for D in models:\n",
        "              D = D.to(device)\n",
        "              y_fakes.append(D(x_fake, x_fake_labels))  # each is [B,1]\n",
        "          # stack into [N_discriminators, B, 1]\n",
        "          y_stack = torch.stack(y_fakes, dim=0)\n",
        "\n",
        "          # 4) Compute λ = ReLU(lambda_star) to enforce λ ≥ 0\n",
        "          lam = relu(lambda_star)\n",
        "\n",
        "          # 5) Soft‐max weights across the 0th dim (discriminators)\n",
        "          #    we want S_i = exp(λ D_i) / sum_j exp(λ D_j)\n",
        "          #    shape remains [N, B, 1]\n",
        "          S = torch.softmax(lam * y_stack, dim=0)\n",
        "\n",
        "          # 6) Weighted sum: D_agg shape [B,1]\n",
        "          D_agg = (S * y_stack).sum(dim=0)\n",
        "\n",
        "          # 7) Compute your generator loss + β λ² regularizer\n",
        "          real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "          adv_loss   = gen.loss(D_agg, real_ident)       # BCEWithLogitsLoss or whatever\n",
        "          reg_loss   = beta * lam.pow(2)                 # β λ²\n",
        "          g_loss     = adv_loss + reg_loss\n",
        "\n",
        "        else:\n",
        "          # Seleciona o melhor discriminador (Dmax)\n",
        "          y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "          y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "          dmax_index = y_fake_g_means.index(max(y_fake_g_means))\n",
        "          Dmax = models[dmax_index]\n",
        "\n",
        "          start_track_mismatch_time = time.time()\n",
        "          #Track mismatches\n",
        "          expected_indexes = label_to_client[class_labels.int2str(x_fake_labels.item())] ##PEGA SOMENTE A PRIMEIRA LABEL, SE BATCH_SIZE_GEN FOR DIFERENTE DE 1 VAI DAR ERRO\n",
        "          if dmax_index not in expected_indexes:\n",
        "              mismatch_count += 1\n",
        "              total_checked +=1\n",
        "              percent_mismatch =  mismatch_count / total_checked\n",
        "              with open(dmax_mismatch_log, \"a\") as mismatch_file:\n",
        "                  mismatch_file.write(f\"{epoch+1} {x_fake_labels.item()} {expected_indexes} {dmax_index} {percent_mismatch:.2f}\\n\")\n",
        "          else:\n",
        "              total_checked += 1\n",
        "              if g_epoch == extra_g_e - 1 and chunk_idx == num_chunks - 1:\n",
        "                percent_mismatch =  mismatch_count / total_checked\n",
        "                with open(dmax_mismatch_log, \"a\") as mismatch_file:\n",
        "                  mismatch_file.write(f\"{epoch+1} {x_fake_labels.item()} {expected_indexes} {dmax_index} {percent_mismatch:.2f}\\n\")\n",
        "          track_mismatch_time = time.time() - start_track_mismatch_time\n",
        "\n",
        "          # Calcula a perda do gerador\n",
        "          real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "          if wgan:\n",
        "            y_fake_g = Dmax(fake_images)\n",
        "            g_loss = generator_loss(y_fake_g)\n",
        "\n",
        "          else:\n",
        "            y_fake_g = Dmax(x_fake, x_fake_labels)  # Detach explícito\n",
        "            g_loss = gen.loss(y_fake_g, real_ident)\n",
        "\n",
        "      g_loss.backward()\n",
        "      #torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "      optim_G.step()\n",
        "      gen.to(device)\n",
        "      chunk_g_loss += g_loss.item()\n",
        "    gen_time = time.time() - start_gen_time\n",
        "\n",
        "    losses_dict[\"g_losses_chunk\"].append(chunk_g_loss / extra_g_e)\n",
        "    g_loss_c += chunk_g_loss /extra_g_e\n",
        "\n",
        "    losses_dict[\"time_chunk\"].append(time.time() - chunk_start_time)\n",
        "    losses_dict[\"net_time\"].append(net_time)\n",
        "    losses_dict[\"disc_time\"].append(disc_time)\n",
        "    losses_dict[\"gen_time\"].append(gen_time)\n",
        "    losses_dict[\"img_syn_time\"].append(img_syn_time)\n",
        "    losses_dict[\"track_mismatch_time\"].append(track_mismatch_time)\n",
        "\n",
        "\n",
        "  g_loss_e = g_loss_c/num_chunks\n",
        "  d_loss_e = d_loss_c / total_d_samples if total_d_samples > 0 else 0.0\n",
        "\n",
        "  losses_dict[\"g_losses_round\"].append(g_loss_e)\n",
        "  losses_dict[\"d_losses_round\"].append(d_loss_e)\n",
        "\n",
        "  if (epoch+1)%2==0:\n",
        "      checkpoint = {\n",
        "            'epoch': epoch+1,  # número da última época concluída\n",
        "            'alvo_state_dict': global_net.state_dict(),\n",
        "            'optimizer_alvo_state_dict': [optim.state_dict() for optim in optims],\n",
        "            'gen_state_dict': gen.state_dict(),\n",
        "            'optim_G_state_dict': optim_G.state_dict(),\n",
        "            'discs_state_dict': [model.state_dict() for model in models],\n",
        "            'optim_Ds_state_dict:': [optim_d.state_dict() for optim_d in optim_Ds]\n",
        "          }\n",
        "      checkpoint_file = f\"checkpoint_epoch{epoch+1}.pth\"\n",
        "      if IN_COLAB:\n",
        "          checkpoint_file = os.path.join(save_dir, checkpoint_file)\n",
        "      torch.save(checkpoint, checkpoint_file)\n",
        "      print(f\"Global net saved to {checkpoint_file}\")\n",
        "\n",
        "      if f2a:\n",
        "        current_lambda_star = lambda_star.item()\n",
        "        current_lam         = F.relu(lambda_star).item()\n",
        "\n",
        "        with open(lambda_log, \"a\") as f:\n",
        "          f.write(f\"{current_lambda_star},{current_lam}\\n\")\n",
        "\n",
        "  correct, loss = 0, 0.0\n",
        "  global_net.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch in testloader:\n",
        "          images = batch[image].to(device)\n",
        "          labels = batch[\"label\"].to(device)\n",
        "          outputs = global_net(images)\n",
        "          loss += criterion(outputs, labels).item()\n",
        "          correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "  accuracy = correct / len(testloader.dataset)\n",
        "  losses_dict[\"net_loss_round\"].append(loss / len(testloader))\n",
        "  losses_dict[\"net_acc_round\"].append(accuracy)\n",
        "\n",
        "  print(f\"Época {epoch+1} completa\")\n",
        "  generate_plot(gen, \"cpu\", epoch+1, latent_dim=128)\n",
        "  gen.to(device)\n",
        "\n",
        "  losses_dict[\"time_round\"].append(time.time() - epoch_start_time)\n",
        "\n",
        "  try:\n",
        "      with open(loss_filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(losses_dict, f, ensure_ascii=False, indent=4) # indent makes it readable\n",
        "      print(f\"Losses dict successfully saved to {loss_filename}\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving losses dict to JSON: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4677e695",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26024cd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(100):\n",
        "print(\"Epoch\", epoch, int(13 * (math.exp(0.01*epoch) - 1) / (math.exp(0.01*50) - 1)) * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9aa99aa",
      "metadata": {},
      "source": [
        "### Somente Classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_OdHAvK0LQp3",
      "metadata": {
        "collapsed": true,
        "id": "_OdHAvK0LQp3"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "losses_dict = {\"net_loss_chunk\": [],\n",
        "               \"net_acc_chunk\": [],\n",
        "               \"net_loss_round\": [],\n",
        "               \"net_acc_round\": [],\n",
        "               \"time_chunk\": [],\n",
        "               \"time_round\": []}\n",
        "\n",
        "epoch_bar = tqdm(range(0, epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "\n",
        "batch_tam = 32\n",
        "latent_dim = 128\n",
        "num_classes = 10\n",
        "if type(nets[0]).__name__ == \"Net\":\n",
        "  image = \"image\"\n",
        "else:\n",
        "  image = \"img\"\n",
        "\n",
        "if IN_COLAB:\n",
        "  acc_filename = os.path.join(save_dir,\"accuracy_report.txt\")\n",
        "  loss_filename = os.path.join(save_dir, \"losses.json\")\n",
        "else:\n",
        "  acc_filename = \"accuracy_report.txt\"\n",
        "  loss_filename = \"losses.json\"\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "  epoch_start_time = time.time()\n",
        "\n",
        "  chunk_bar = tqdm(range(num_chunks), desc=\"Chunks\", leave=True, position=1)\n",
        "\n",
        "  for chunk_idx in chunk_bar:\n",
        "    params = []\n",
        "    results = []\n",
        "    chunk_start_time = time.time()\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "    client_bar = tqdm(enumerate(zip(nets, client_chunks)), desc=\"Clients\", leave=True, position=2)\n",
        "\n",
        "    for cliente, (net, chunks) in client_bar:\n",
        "\n",
        "      if chunk_idx == 0:\n",
        "        client_eval_time = time.time()\n",
        "        # Evaluation in client test\n",
        "        # Initialize counters\n",
        "        class_correct = defaultdict(int)\n",
        "        class_total = defaultdict(int)\n",
        "        predictions_counter = defaultdict(int)\n",
        "\n",
        "        global_net.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in client_test_loaders[cliente]:\n",
        "                images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "                outputs = global_net(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Update counts for each sample in batch\n",
        "                for true_label, pred_label in zip(labels, predicted):\n",
        "                    true_idx = true_label.item()\n",
        "                    pred_idx = pred_label.item()\n",
        "\n",
        "                    class_total[true_idx] += 1\n",
        "                    predictions_counter[pred_idx] += 1\n",
        "\n",
        "                    if true_idx == pred_idx:\n",
        "                        class_correct[true_idx] += 1\n",
        "\n",
        "            # Create results dictionary\n",
        "            results_metrics = {\n",
        "                \"class_metrics\": {},\n",
        "                \"overall_accuracy\": None,\n",
        "                \"prediction_distribution\": dict(predictions_counter)\n",
        "            }\n",
        "\n",
        "            # Calculate class-wise metrics\n",
        "            for i in range(num_classes):\n",
        "                metrics = {\n",
        "                    \"samples\": class_total[i],\n",
        "                    \"predictions\": predictions_counter[i],\n",
        "                    \"accuracy\": class_correct[i] / class_total[i] if class_total[i] > 0 else \"N/A\"\n",
        "                }\n",
        "                results_metrics[\"class_metrics\"][f\"class_{i}\"] = metrics\n",
        "\n",
        "            # Calculate overall accuracy\n",
        "            total_samples = sum(class_total.values())\n",
        "            results_metrics[\"overall_accuracy\"] = sum(class_correct.values()) / total_samples\n",
        "\n",
        "            # Save to txt file\n",
        "            with open(acc_filename, \"a\") as f:\n",
        "                f.write(f\"Epoch {epoch + 1} - Client {cliente}\\n\")\n",
        "                # Header with fixed widths\n",
        "                f.write(\"{:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
        "                    \"Class\", \"Accuracy\", \"Samples\", \"Predictions\"))\n",
        "                f.write(\"-\"*45 + \"\\n\")\n",
        "\n",
        "                # Class rows with consistent formatting\n",
        "                for cls in range(num_classes):\n",
        "                    metrics = results_metrics[\"class_metrics\"][f\"class_{cls}\"]\n",
        "\n",
        "                    # Format accuracy (handle \"N/A\" case)\n",
        "                    accuracy = (f\"{metrics['accuracy']:.4f}\"\n",
        "                              if isinstance(metrics['accuracy'], float)\n",
        "                              else \"  N/A  \")\n",
        "\n",
        "                    f.write(\"{:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
        "                        f\"Class {cls}\",\n",
        "                        accuracy,\n",
        "                        metrics['samples'],\n",
        "                        metrics['predictions']\n",
        "                    ))\n",
        "\n",
        "                # Footer with alignment\n",
        "                f.write(\"\\n{:<20} {:.4f}\".format(\"Overall Accuracy:\", results_metrics[\"overall_accuracy\"]))\n",
        "                f.write(\"\\n{:<20} {}\".format(\"Total Samples:\", total_samples))\n",
        "                f.write(\"\\n{:<20} {}\".format(\"Total Predictions:\", sum(predictions_counter.values())))\n",
        "                f.write(\"\\n{:<20} {:.4f}\".format(\"Client Evaluation Time:\", time.time() - client_eval_time))\n",
        "                f.write(\"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        print(\"Results saved to accuracy_report.txt\")\n",
        "\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      if len(chunk_dataset) == 0:\n",
        "        print(f\"Chunk {chunk_idx} for client {cliente} is empty, skipping.\")\n",
        "        continue\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size=batch_tam, shuffle=False)\n",
        "\n",
        "      net.load_state_dict(global_net.state_dict(), strict=True)\n",
        "      net.to(device)\n",
        "      net.train()\n",
        "      optim = optims[cliente]\n",
        "\n",
        "      batch_bar = tqdm(chunk_loader, desc=\"Batches\", leave=True, position=3)\n",
        "\n",
        "      for batch in batch_bar:\n",
        "        images, labels = batch[image].to(device), batch[\"label\"].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        if batch_size == 1:\n",
        "          print(\"Batch size is 1, skipping batch\")\n",
        "          continue\n",
        "        optim.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "\n",
        "      params.append(ndarrays_to_parameters([val.cpu().numpy() for _, val in net.state_dict().items()]))\n",
        "      results.append((cliente, FitRes(status=Status(code=Code.OK, message=\"Success\"), parameters=params[cliente], num_examples=len(chunk_loader.dataset), metrics={})))\n",
        "\n",
        "    aggregated_ndarrays = aggregate_inplace(results)\n",
        "\n",
        "    params_dict = zip(global_net.state_dict().keys(), aggregated_ndarrays)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
        "    global_net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    # Evaluation\n",
        "    if chunk_idx % 10 == 0:\n",
        "        global_net.eval()\n",
        "        correct, loss = 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                images = batch[image].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                outputs = global_net(images)\n",
        "                loss += criterion(outputs, labels).item()\n",
        "                correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        accuracy = correct / len(testloader.dataset)\n",
        "        losses_dict[\"net_loss_chunk\"].append(loss / len(testloader))\n",
        "        losses_dict[\"net_acc_chunk\"].append(accuracy)\n",
        "\n",
        "        losses_dict[\"time_chunk\"].append(time.time() - chunk_start_time)\n",
        "\n",
        "  correct, loss = 0, 0.0\n",
        "  global_net.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch in testloader:\n",
        "          images = batch[image].to(device)\n",
        "          labels = batch[\"label\"].to(device)\n",
        "          outputs = global_net(images)\n",
        "          loss += criterion(outputs, labels).item()\n",
        "          correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "  accuracy = correct / len(testloader.dataset)\n",
        "  losses_dict[\"net_loss_round\"].append(loss / len(testloader))\n",
        "  losses_dict[\"net_acc_round\"].append(accuracy)\n",
        "\n",
        "\n",
        "  print(f\"Época {epoch+1} completa\")\n",
        "\n",
        "  losses_dict[\"time_round\"].append(time.time() - epoch_start_time)\n",
        "\n",
        "  try:\n",
        "      with open(loss_filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(losses_dict, f, ensure_ascii=False, indent=4) # indent makes it readable\n",
        "      print(f\"Losses dict successfully saved to {loss_filename}\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving losses dict to JSON: {e}\")\n",
        "\n",
        "  if (epoch+1)%1==0:\n",
        "    checkpoint = {\n",
        "          'epoch': epoch+1,  # número da última época concluída\n",
        "          'alvo_state_dict': global_net.state_dict(),\n",
        "          'optimizer_alvo_state_dict': [optim.state_dict() for optim in optims],\n",
        "        }\n",
        "    checkpoint_file = f\"checkpoint_epoch{epoch+1}.pth\"\n",
        "    if IN_COLAB:\n",
        "        checkpoint_file = os.path.join(save_dir, checkpoint_file)\n",
        "    torch.save(checkpoint, checkpoint_file)\n",
        "    print(f\"Global net saved to {checkpoint_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44064765",
      "metadata": {},
      "source": [
        "### Somente Gerador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7312a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "wgan = False\n",
        "f2a = False\n",
        "epochs = 50\n",
        "losses_dict = {\"g_losses_chunk\": [],\n",
        "               \"d_losses_chunk\": [],\n",
        "               \"g_losses_round\": [],\n",
        "               \"d_losses_round\": [],\n",
        "               \"time_chunk\": [],\n",
        "               \"time_round\": [],\n",
        "               \"disc_time\": [],\n",
        "               \"gen_time\": [],\n",
        "               \"img_syn_time\": [],\n",
        "               \"track_mismatch_time\": []\n",
        "               }\n",
        "\n",
        "epoch_bar = tqdm(range(0, epochs), desc=\"Treinamento\", leave=True, position=0)\n",
        "\n",
        "batch_size_gen = 1\n",
        "batch_tam = 32\n",
        "extra_g_e = 20\n",
        "latent_dim = 128\n",
        "num_classes = 10\n",
        "\n",
        "if IN_COLAB:\n",
        "  loss_filename = os.path.join(save_dir, \"losses.json\")\n",
        "  dmax_mismatch_log = os.path.join(save_dir, \"dmax_mismatch.txt\")\n",
        "  lambda_log = os.path.join(save_dir, \"lambda_log.txt\")\n",
        "\n",
        "else:\n",
        "  loss_filename = \"losses.json\"\n",
        "  dmax_mismatch_log = \"dmax_mismatch.txt\"\n",
        "  lambda_log = \"lambda_log.txt\"\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "  epoch_start_time = time.time()\n",
        "  mismatch_count = 0\n",
        "  total_checked = 0\n",
        "  g_loss_c = 0.0\n",
        "  d_loss_c = 0.0\n",
        "  total_d_samples = 0  # Amostras totais processadas pelos discriminadores\n",
        "  total_g_samples = 0  # Amostras totais processadas pelo gerador\n",
        "\n",
        "  chunk_bar = tqdm(range(num_chunks), desc=\"Chunks\", leave=True, position=1)\n",
        "\n",
        "  for chunk_idx in chunk_bar:\n",
        "    chunk_start_time = time.time()\n",
        "    # ====================================================================\n",
        "    # Treino dos Discriminadores (clientes) no bloco atual\n",
        "    # ====================================================================\n",
        "    d_loss_b = 0\n",
        "    total_chunk_samples = 0\n",
        "\n",
        "\n",
        "    client_bar = tqdm(enumerate(zip(models, client_chunks)), desc=\"Clients\", leave=True, position=2)\n",
        "\n",
        "    for cliente, (disc, chunks) in client_bar:\n",
        "      # Carregar o bloco atual do cliente\n",
        "      chunk_dataset = chunks[chunk_idx]\n",
        "      if len(chunk_dataset) == 0:\n",
        "        print(f\"Chunk {chunk_idx} for client {cliente} is empty, skipping.\")\n",
        "        continue\n",
        "      chunk_loader = DataLoader(chunk_dataset, batch_size=batch_tam, shuffle=True)\n",
        "\n",
        "      # Treinar o discriminador no bloco\n",
        "      disc.to(device)\n",
        "      optim_D = optim_Ds[cliente]\n",
        "\n",
        "      batch_bar = tqdm(chunk_loader, desc=\"Batches\", leave=True, position=4)\n",
        "\n",
        "      start_disc_time = time.time()\n",
        "      for batch in batch_bar:\n",
        "          images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "          batch_size = images.size(0)\n",
        "          if batch_size == 1:\n",
        "            print(\"Batch size is 1, skipping batch\")\n",
        "            continue\n",
        "\n",
        "          real_ident = torch.full((batch_size, 1), 1., device=device)\n",
        "          fake_ident = torch.full((batch_size, 1), 0., device=device)\n",
        "\n",
        "          z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "          x_fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "\n",
        "          # Train D\n",
        "          optim_D.zero_grad()\n",
        "\n",
        "          if wgan:\n",
        "            labels = torch.nn.functional.one_hot(labels, 10).float().to(device)\n",
        "            x_fake_l = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "\n",
        "            # Adicionar labels ao images para treinamento do Discriminador\n",
        "            image_labels = labels.view(labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "            image_fake_labels = x_fake_l.view(x_fake_l.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "\n",
        "            images = torch.cat([images, image_labels], dim=1)\n",
        "\n",
        "            # Treinar Discriminador\n",
        "            z = torch.cat([z_noise, x_fake_l], dim=1)\n",
        "            fake_images = gen(z).detach()\n",
        "            fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "            d_loss = discriminator_loss(disc(images), disc(fake_images)) + 10 * gradient_penalty(disc, images, fake_images)\n",
        "\n",
        "          else:\n",
        "            # Dados Reais\n",
        "            y_real = disc(images, labels)\n",
        "            d_real_loss = disc.loss(y_real, real_ident)\n",
        "\n",
        "            # Dados Falsos\n",
        "            x_fake = gen(z_noise, x_fake_labels).detach()\n",
        "            y_fake_d = disc(x_fake, x_fake_labels)\n",
        "            d_fake_loss = disc.loss(y_fake_d, fake_ident)\n",
        "\n",
        "            # Loss total e backprop\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "          d_loss.backward()\n",
        "          #torch.nn.utils.clip_grad_norm_(disc.discriminator.parameters(), max_norm=1.0)\n",
        "          optim_D.step()\n",
        "          d_loss_b += d_loss.item()\n",
        "          total_chunk_samples += 1\n",
        "      disc_time = time.time() - start_disc_time  \n",
        "\n",
        "\n",
        "    # Média da perda dos discriminadores neste chunk\n",
        "    avg_d_loss_chunk = d_loss_b / total_chunk_samples if total_chunk_samples > 0 else 0.0\n",
        "    losses_dict[\"d_losses_chunk\"].append(avg_d_loss_chunk)\n",
        "    d_loss_c += avg_d_loss_chunk * total_chunk_samples\n",
        "    total_d_samples += total_chunk_samples\n",
        "\n",
        "    chunk_g_loss = 0.0\n",
        "\n",
        "    epoch_gen_bar = tqdm(range(extra_g_e), desc=\"Gerador\", leave=True, position=2)\n",
        "\n",
        "    start_gen_time = time.time()\n",
        "    for g_epoch in epoch_gen_bar:\n",
        "      # Train G\n",
        "      optim_G.zero_grad()\n",
        "\n",
        "      # Gera dados falsos\n",
        "      z_noise = torch.randn(batch_size_gen, latent_dim, device=device)\n",
        "      x_fake_labels = torch.randint(0, 10, (batch_size_gen,), device=device)\n",
        "      label = int(x_fake_labels.item())\n",
        "\n",
        "      if wgan:\n",
        "        x_fake_labels = torch.nn.functional.one_hot(x_fake_labels, 10).float()\n",
        "        z_noise = torch.cat([z_noise, x_fake_labels], dim=1)\n",
        "        fake_images = gen(z_noise)\n",
        "\n",
        "        # Seleciona o melhor discriminador (Dmax)\n",
        "        image_fake_labels = x_fake_labels.view(x_fake_labels.size(0), 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "        fake_images = torch.cat([fake_images, image_fake_labels], dim=1)\n",
        "\n",
        "        y_fake_gs = [model(fake_images.detach()) for model in models]\n",
        "\n",
        "      else:\n",
        "        x_fake = gen(z_noise, x_fake_labels)\n",
        "\n",
        "        if f2a:\n",
        "          y_fakes = []\n",
        "          for D in models:\n",
        "              D = D.to(device)\n",
        "              y_fakes.append(D(x_fake, x_fake_labels))  # each is [B,1]\n",
        "          # stack into [N_discriminators, B, 1]\n",
        "          y_stack = torch.stack(y_fakes, dim=0)\n",
        "\n",
        "          # 4) Compute λ = ReLU(lambda_star) to enforce λ ≥ 0\n",
        "          lam = relu(lambda_star)\n",
        "\n",
        "          # 5) Soft‐max weights across the 0th dim (discriminators)\n",
        "          #    we want S_i = exp(λ D_i) / sum_j exp(λ D_j)\n",
        "          #    shape remains [N, B, 1]\n",
        "          S = torch.softmax(lam * y_stack, dim=0)\n",
        "\n",
        "          # 6) Weighted sum: D_agg shape [B,1]\n",
        "          D_agg = (S * y_stack).sum(dim=0)\n",
        "\n",
        "          # 7) Compute your generator loss + β λ² regularizer\n",
        "          real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "          adv_loss   = gen.loss(D_agg, real_ident)       # BCEWithLogitsLoss or whatever\n",
        "          reg_loss   = beta * lam.pow(2)                 # β λ²\n",
        "          g_loss     = adv_loss + reg_loss\n",
        "\n",
        "        else:\n",
        "          # Seleciona o melhor discriminador (Dmax)\n",
        "          y_fake_gs = [model(x_fake.detach(), x_fake_labels) for model in models]\n",
        "          y_fake_g_means = [torch.mean(y).item() for y in y_fake_gs]\n",
        "          dmax_index = y_fake_g_means.index(max(y_fake_g_means))\n",
        "          Dmax = models[dmax_index]\n",
        "\n",
        "          start_track_mismatch_time = time.time()\n",
        "          #Track mismatches\n",
        "          expected_indexes = label_to_client[class_labels.int2str(x_fake_labels.item())] ##PEGA SOMENTE A PRIMEIRA LABEL, SE BATCH_SIZE_GEN FOR DIFERENTE DE 1 VAI DAR ERRO\n",
        "          if dmax_index not in expected_indexes:\n",
        "              mismatch_count += 1\n",
        "              total_checked +=1\n",
        "              percent_mismatch =  mismatch_count / total_checked\n",
        "              with open(dmax_mismatch_log, \"a\") as mismatch_file:\n",
        "                  mismatch_file.write(f\"{epoch+1} {x_fake_labels.item()} {expected_indexes} {dmax_index} {percent_mismatch:.2f}\\n\")\n",
        "          else:\n",
        "              total_checked += 1\n",
        "              if g_epoch == extra_g_e - 1 and chunk_idx == num_chunks - 1:\n",
        "                percent_mismatch =  mismatch_count / total_checked\n",
        "                with open(dmax_mismatch_log, \"a\") as mismatch_file:\n",
        "                  mismatch_file.write(f\"{epoch+1} {x_fake_labels.item()} {expected_indexes} {dmax_index} {percent_mismatch:.2f}\\n\")\n",
        "          track_mismatch_time = time.time() - start_track_mismatch_time\n",
        "\n",
        "          # Calcula a perda do gerador\n",
        "          real_ident = torch.full((batch_size_gen, 1), 1., device=device)\n",
        "          if wgan:\n",
        "            y_fake_g = Dmax(fake_images)\n",
        "            g_loss = generator_loss(y_fake_g)\n",
        "\n",
        "          else:\n",
        "            y_fake_g = Dmax(x_fake, x_fake_labels)  # Detach explícito\n",
        "            g_loss = gen.loss(y_fake_g, real_ident)\n",
        "\n",
        "      g_loss.backward()\n",
        "      #torch.nn.utils.clip_grad_norm_(gen.generator.parameters(), max_norm=1.0)\n",
        "      optim_G.step()\n",
        "      gen.to(device)\n",
        "      chunk_g_loss += g_loss.item()\n",
        "    gen_time = time.time() - start_gen_time\n",
        "\n",
        "    losses_dict[\"g_losses_chunk\"].append(chunk_g_loss / extra_g_e)\n",
        "    g_loss_c += chunk_g_loss /extra_g_e\n",
        "\n",
        "    losses_dict[\"time_chunk\"].append(time.time() - chunk_start_time)\n",
        "    losses_dict[\"disc_time\"].append(disc_time)\n",
        "    losses_dict[\"gen_time\"].append(gen_time)\n",
        "    losses_dict[\"track_mismatch_time\"].append(track_mismatch_time)\n",
        "\n",
        "\n",
        "  g_loss_e = g_loss_c/num_chunks\n",
        "  d_loss_e = d_loss_c / total_d_samples if total_d_samples > 0 else 0.0\n",
        "\n",
        "  losses_dict[\"g_losses_round\"].append(g_loss_e)\n",
        "  losses_dict[\"d_losses_round\"].append(d_loss_e)\n",
        "\n",
        "  if (epoch+1)%2==0:\n",
        "      checkpoint = {\n",
        "            'epoch': epoch+1,  # número da última época concluída\n",
        "            'gen_state_dict': gen.state_dict(),\n",
        "            'optim_G_state_dict': optim_G.state_dict(),\n",
        "            'discs_state_dict': [model.state_dict() for model in models],\n",
        "            'optim_Ds_state_dict:': [optim_d.state_dict() for optim_d in optim_Ds]\n",
        "          }\n",
        "      checkpoint_file = f\"checkpoint_epoch{epoch+1}.pth\"\n",
        "      if IN_COLAB:\n",
        "          checkpoint_file = os.path.join(save_dir, checkpoint_file)\n",
        "      torch.save(checkpoint, checkpoint_file)\n",
        "      print(f\"Global net saved to {checkpoint_file}\")\n",
        "\n",
        "      if f2a:\n",
        "        current_lambda_star = lambda_star.item()\n",
        "        current_lam         = F.relu(lambda_star).item()\n",
        "\n",
        "        with open(lambda_log, \"a\") as f:\n",
        "          f.write(f\"{current_lambda_star},{current_lam}\\n\")\n",
        "\n",
        "  print(f\"Época {epoch+1} completa\")\n",
        "  generate_plot(gen, \"cpu\", epoch+1, latent_dim=128)\n",
        "  gen.to(device)\n",
        "\n",
        "  losses_dict[\"time_round\"].append(time.time() - epoch_start_time)\n",
        "\n",
        "  try:\n",
        "      with open(loss_filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(losses_dict, f, ensure_ascii=False, indent=4) # indent makes it readable\n",
        "      print(f\"Losses dict successfully saved to {loss_filename}\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving losses dict to JSON: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3ce47d",
      "metadata": {
        "id": "2c3ce47d"
      },
      "source": [
        "# Gráficos de perda e acurácia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f7dbfb",
      "metadata": {
        "id": "01f7dbfb"
      },
      "source": [
        "## Le o arquivo de perda salvo no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "FaRDCz_cSJDf",
      "metadata": {
        "id": "FaRDCz_cSJDf"
      },
      "outputs": [],
      "source": [
        "loss_filename = \"../Experimentos/NB_F2U/Alvo_4c_01Dir//CIFAR/losses.json\"\n",
        "# if IN_COLAB:\n",
        "#   loss_filename = os.path.join(save_dir, loss_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "-by_-71kSOeu",
      "metadata": {
        "id": "-by_-71kSOeu"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "49537f37",
      "metadata": {
        "id": "49537f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary successfully loaded from ../Experimentos/NB_F2U/Alvo_4c_01Dir//CIFAR/losses.json\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with open(loss_filename, 'r', encoding='utf-8') as f:\n",
        "        # The load function also works the same\n",
        "        loaded_dict_cifar_dir01 = json.load(f)\n",
        "    print(f\"Dictionary successfully loaded from {loss_filename}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{loss_filename}' not found.\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: Could not decode JSON from '{loss_filename}'. File might be corrupted or not JSON.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dictionary from JSON: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "237af03a",
      "metadata": {
        "id": "237af03a"
      },
      "source": [
        "## Coleta acurácias locais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8d24b634",
      "metadata": {
        "id": "8d24b634"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8f0588a2",
      "metadata": {
        "id": "8f0588a2"
      },
      "outputs": [],
      "source": [
        "def parse_client_accuracies(log_path):\n",
        "   # Regex to match \"Round X - Cliente Y\" and \"Overall Accuracy:    Z.ZZZZ\"\n",
        "   header_re   = re.compile(r\"Epoch\\s+\\d+\\s*-\\s*Client\\s*(\\d+)\", re.IGNORECASE)\n",
        "   accuracy_re = re.compile(r\"Overall Accuracy:\\s*([\\d.]+)\")\n",
        "\n",
        "\n",
        "   # Now client → list of accuracies\n",
        "   client_accuracies = defaultdict(list)\n",
        "\n",
        "\n",
        "   with open(log_path, 'r', encoding='utf-8') as f:\n",
        "       current_client = None\n",
        "\n",
        "\n",
        "       for line in f:\n",
        "           # Detect the client header\n",
        "           hdr = header_re.search(line)\n",
        "           if hdr:\n",
        "               current_client = int(hdr.group(1))\n",
        "               continue\n",
        "\n",
        "\n",
        "           # Once we see the accuracy line, append and reset\n",
        "           if current_client is not None:\n",
        "               acc = accuracy_re.search(line)\n",
        "               if acc:\n",
        "                   client_accuracies[current_client].append(float(acc.group(1)))\n",
        "                   current_client = None\n",
        "\n",
        "\n",
        "   return dict(client_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f397ca9c",
      "metadata": {
        "id": "f397ca9c"
      },
      "outputs": [],
      "source": [
        "log_file = \"../Experimentos/NB_F2U/Alvo_4c_05Dir//CIFAR/accuracy_report.txt\"\n",
        "local_acc_cifar_dir05 = parse_client_accuracies(log_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b890d518",
      "metadata": {
        "id": "b890d518"
      },
      "source": [
        "## Funcao de plotagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "39493e97",
      "metadata": {
        "id": "39493e97"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from typing import Mapping, Iterable, Any, Literal, Union, List, Tuple\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c05bff13",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from typing import Mapping, Iterable, List, Tuple, Union, Any, Literal\n",
        "\n",
        "def plot_series(\n",
        "    series: Mapping[str, Iterable[float]],\n",
        "    *,\n",
        "    subplot_groups: List[List[str]] = None,\n",
        "    subplot_layout: Tuple[int, int] = None,\n",
        "    legend_subplot_index: Union[int, str] = 'all',\n",
        "    series_styles: Mapping[str, Mapping[str, Any]] = None,\n",
        "    xlim: Union[tuple[float, float], List[tuple[float, float]]] = None,\n",
        "    ylim: Union[tuple[float, float], List[tuple[float, float]]] = None,\n",
        "    first_step: int = None,\n",
        "    xtick_step: int = 1,\n",
        "    xtick_offset: int = 0,\n",
        "    num_ticks: int = None,\n",
        "    xlabel: Union[str, List[str]] = \"Epochs\",\n",
        "    ylabel: Union[str, List[str]] = \"Value\",\n",
        "    label_fontsize: float = None,\n",
        "    tick_fontsize: float = None,\n",
        "    title: Union[str, List[str]] = None,\n",
        "    title_fontsize: float = None,\n",
        "    highlight: Mapping[str, Literal[\"max\", \"min\", \"both\"]] = None,\n",
        "    highlight_marker: str = \"o\",\n",
        "    highlight_markersize: float = 4,\n",
        "    highlight_color: str = None,\n",
        "    highlight_text_size: int = 8,\n",
        "    highlight_text_offset_max: tuple[float, float] = (0.1, 0.2),\n",
        "    highlight_text_offset_min: tuple[float, float] = (0.1, -0.2),\n",
        "    highlight_style: Mapping[str, Mapping[str, Any]] = None,\n",
        "    legend_loc: str = 'best',\n",
        "    legend_fontsize: float = 10,\n",
        "    figsize: tuple[float, float] = (10, 5),\n",
        ") -> None:\n",
        "    if subplot_groups is None:\n",
        "        subplot_groups = [list(series.keys())]\n",
        "\n",
        "    num_plots = len(subplot_groups)\n",
        "\n",
        "    if subplot_layout:\n",
        "        nrows, ncols = subplot_layout\n",
        "        if nrows * ncols < num_plots:\n",
        "            raise ValueError(f\"Layout {subplot_layout} is too small for {num_plots} groups.\")\n",
        "    else:\n",
        "        nrows, ncols = num_plots, 1\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize, squeeze=False)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    def get_setting(value, index):\n",
        "        if isinstance(value, list):\n",
        "            return value[index] if index < len(value) else None\n",
        "        return value\n",
        "\n",
        "    for i, (ax, group) in enumerate(zip(axes, subplot_groups)):\n",
        "        n = 0\n",
        "        if group:\n",
        "            n = max(len(series.get(name, [])) for name in group)\n",
        "        xs = list(range(n))\n",
        "\n",
        "        for name in group:\n",
        "            if name not in series:\n",
        "                continue\n",
        "            ys = series[name]\n",
        "            style = series_styles.get(name, {}) if series_styles else {}\n",
        "            highlight_style = highlight_style.get(name, {}) if highlight_styles else {}\n",
        "            line, = ax.plot(xs, ys, label=name, **style)\n",
        "            mode = highlight.get(name) if highlight else None\n",
        "            base_color = style.get('color', line.get_color())\n",
        "            mcolor = highlight_color or base_color\n",
        "\n",
        "            if mode in (\"max\", \"both\"):\n",
        "                i_max = max(range(n), key=lambda j: ys[j])\n",
        "                ax.plot(i_max, ys[i_max], marker=highlight_marker, markersize=highlight_markersize, color=mcolor)\n",
        "                \n",
        "                offset = highlight_style.get('highlight_offset_max', highlight_text_offset_max)\n",
        "                text_position = (i_max + offset[0], ys[i_max] + offset[1])\n",
        "                \n",
        "                ax.annotate(\n",
        "                        f\"{ys[i_max]:.2f}\",\n",
        "                        xy=(i_max, ys[i_max]),\n",
        "                        xytext=text_position,\n",
        "                        arrowprops=dict(arrowstyle=\"->\", color='lightgray'),\n",
        "                        fontsize=highlight_text_size,\n",
        "                        va=\"bottom\",\n",
        "                        ha=\"center\"\n",
        "                    )\n",
        "            if mode in (\"min\", \"both\"):\n",
        "                i_min = min(range(n), key=lambda j: ys[j])\n",
        "                ax.plot(i_min, ys[i_min], marker=highlight_marker, markersize=highlight_markersize, color=mcolor)\n",
        "\n",
        "                offset = style.get('highlight_offset_min', highlight_text_offset_min)\n",
        "                text_position = (i_min + offset[0], ys[i_min] + offset[1])\n",
        "\n",
        "                ax.annotate(\n",
        "                    f\"{ys[i_min]:.2f}\",\n",
        "                    xy=(i_min, ys[i_min]),\n",
        "                    xytext=text_position,\n",
        "                    arrowprops=dict(arrowstyle=\"->\", color='black'),\n",
        "                    fontsize=highlight_text_size,\n",
        "                    va=\"top\",\n",
        "                    ha=\"center\"\n",
        "                )\n",
        "\n",
        "        if n > 0:\n",
        "            if num_ticks:\n",
        "                # Generate exactly `num_ticks` evenly spaced ticks, including endpoints\n",
        "                ticks = np.linspace(0, n, num_ticks)\n",
        "                # Set the ticks. Use .astype(int) if you need integer labels.\n",
        "                ax.set_xticks(ticks.astype(int))\n",
        "\n",
        "            elif first_step is not None:\n",
        "                # Your original logic is preserved as a fallback\n",
        "                labels = [1]\n",
        "                next_label = 1 + first_step\n",
        "                while next_label <= n:\n",
        "                    labels.append(next_label)\n",
        "                    next_label += xtick_step\n",
        "                positions = [lbl - 1 for lbl in labels]\n",
        "                labels = [lbl + xtick_offset for lbl in labels]\n",
        "                ax.set_xticks(positions, labels)\n",
        "\n",
        "            elif xtick_step > 0:\n",
        "                positions = list(range(0, n, xtick_step))\n",
        "                labels = [pos + 1 + xtick_offset for pos in positions]\n",
        "                ax.set_xticks(positions, labels)\n",
        "\n",
        "        if  num_ticks and xtick_offset != 0 and n > 0:\n",
        "            # Wait for Matplotlib to finalize the tick positions\n",
        "            fig.canvas.draw() \n",
        "            \n",
        "            # Get the automatically determined tick positions\n",
        "            current_ticks = ax.get_xticks()\n",
        "            \n",
        "            # Create new labels by adding the offset\n",
        "            new_labels = [int(tick) + xtick_offset for tick in current_ticks]\n",
        "            \n",
        "            # Apply the new labels to the existing tick positions\n",
        "            ax.set_xticklabels(new_labels)\n",
        "\n",
        "        ax.set_xlabel(get_setting(xlabel, i), fontsize=label_fontsize)\n",
        "        ax.set_ylabel(get_setting(ylabel, i), fontsize=label_fontsize)\n",
        "        ax.set_title(get_setting(title, i), fontsize=title_fontsize)\n",
        "\n",
        "        if tick_fontsize:\n",
        "            ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
        "\n",
        "        if legend_subplot_index == 'all' or i == legend_subplot_index:\n",
        "            ax.legend(loc=legend_loc, fontsize=legend_fontsize)\n",
        "\n",
        "        current_xlim = get_setting(xlim, i)\n",
        "        if current_xlim:\n",
        "            ax.set_xlim(*current_xlim)\n",
        "        elif n > 0:\n",
        "            ax.set_xlim(0, n)\n",
        "\n",
        "        current_ylim = get_setting(ylim, i)\n",
        "        if current_ylim:\n",
        "            ax.set_ylim(*current_ylim)\n",
        "\n",
        "    for j in range(num_plots, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febb881b",
      "metadata": {},
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7df4d40",
      "metadata": {},
      "source": [
        "### Loss e Acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a663f1c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "  series = {\n",
        "      \"Loss\": loaded_dict_cifar_class_gerafed[\"net_loss_round\"],\n",
        "      \"Accuracy\": loaded_dict_cifar_class_gerafed[\"net_acc_round\"]\n",
        "  },\n",
        "  highlight = {\n",
        "      \"Accuracy\": \"max\"\n",
        "  },\n",
        "  highlight_markersize=4,\n",
        "  xtick_step=5,\n",
        "  first_step=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b0d95c",
      "metadata": {},
      "source": [
        "### Local Acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209bd071",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "Line2D.set() got an unexpected keyword argument 'highlight_offset_max'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_series\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"Global - Chunked FedAvg\": loaded_dict_cifar_mnist[\"net_acc_round\"][:100],\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"Global - GeraFed\": loaded_dict_cifar_mnist_gerafed[\"net_acc_round\"][:100],\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 0 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_acc_cifar_dir01\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 0 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_acc_cifar_dir01_gerafed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 1 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_acc_cifar_dir01\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 1 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_acc_cifar_dir01_gerafed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 2 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_acc_cifar_dir01\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 2 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_acc_cifar_dir01_gerafed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_acc_cifar_dir01\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_acc_cifar_dir01_gerafed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries_styles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"Global - Chunked FedAvg\": {\"color\": \"lightblue\", \"linestyle\": \"-\"},\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"Global - GeraFed\": {\"color\": \"lightblue\", \"linestyle\": \"--\"},\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 0 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcornflowerblue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinestyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhighlight_offset_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 0 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msandybrown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinestyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 1 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcornflowerblue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinestyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhighlight_offset_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 1 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msandybrown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinestyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 2 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcornflowerblue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinestyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhighlight_offset_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 2 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msandybrown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinestyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcornflowerblue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinestyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msandybrown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinestyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubplot_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 0 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 0 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 1 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 1 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 2 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 2 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhighlight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#    \"Global - Chunked FedAvg\": \"max\",\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     \"Global - GeraFed\": \"max\",\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 0 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 0 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 1 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 1 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 2 - Chunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClient 2 - GeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChunked FedAvg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeraFed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubplot_layout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_fontsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtick_fontsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhighlight_markersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxtick_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_ticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfirst_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mylabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhighlight_text_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhighlight_text_offset_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegend_subplot_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegend_fontsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle_fontsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma) Client 0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb) Client 1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc) Client 2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md) Client 3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[110], line 65\u001b[0m, in \u001b[0;36mplot_series\u001b[0;34m(series, subplot_groups, subplot_layout, legend_subplot_index, series_styles, xlim, ylim, first_step, xtick_step, xtick_offset, num_ticks, xlabel, ylabel, label_fontsize, tick_fontsize, title, title_fontsize, highlight, highlight_marker, highlight_markersize, highlight_color, highlight_text_size, highlight_text_offset_max, highlight_text_offset_min, legend_loc, legend_fontsize, figsize)\u001b[0m\n\u001b[1;32m     63\u001b[0m ys \u001b[38;5;241m=\u001b[39m series[name]\n\u001b[1;32m     64\u001b[0m style \u001b[38;5;241m=\u001b[39m series_styles\u001b[38;5;241m.\u001b[39mget(name, {}) \u001b[38;5;28;01mif\u001b[39;00m series_styles \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m---> 65\u001b[0m line, \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m mode \u001b[38;5;241m=\u001b[39m highlight\u001b[38;5;241m.\u001b[39mget(name) \u001b[38;5;28;01mif\u001b[39;00m highlight \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     67\u001b[0m base_color \u001b[38;5;241m=\u001b[39m style\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m, line\u001b[38;5;241m.\u001b[39mget_color())\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/axes/_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/axes/_base.py:546\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(result)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [l[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m result]\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/axes/_base.py:546\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(result)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [l[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m result]\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/axes/_base.py:539\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel must be scalar or have the same length as the input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(label)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 539\u001b[0m result \u001b[38;5;241m=\u001b[39m (\u001b[43mmake_artist\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mncx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mncy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m j, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels))\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_kwargs:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(result)\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/axes/_base.py:338\u001b[0m, in \u001b[0;36m_process_plot_var_args._make_line\u001b[0;34m(self, axes, x, y, kw, kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m kw \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}  \u001b[38;5;66;03m# Don't modify the original kw.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setdefaults(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getdefaults(kw), kw)\n\u001b[0;32m--> 338\u001b[0m seg \u001b[38;5;241m=\u001b[39m \u001b[43mmlines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLine2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seg, kw\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/lines.py:407\u001b[0m, in \u001b[0;36mLine2D.__init__\u001b[0;34m(self, xdata, ydata, linewidth, linestyle, color, gapcolor, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_markeredgewidth(markeredgewidth)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# update kwargs before updating data to give the caller a\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# chance to init axes (and hence unit support)\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpickradius \u001b[38;5;241m=\u001b[39m pickradius\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mind_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/artist.py:1233\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_internal_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, kwargs):\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;124;03m    errors as if calling `set`.\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \n\u001b[1;32m   1231\u001b[0m \u001b[38;5;124;03m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_props\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{cls.__name__}\u001b[39;49;00m\u001b[38;5;124;43m.set() got an unexpected keyword argument \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{prop_name!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/gerafed/lib/python3.10/site-packages/matplotlib/artist.py:1206\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1204\u001b[0m             func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1205\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m-> 1206\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1207\u001b[0m                     errfmt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), prop_name\u001b[38;5;241m=\u001b[39mk),\n\u001b[1;32m   1208\u001b[0m                     name\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m   1209\u001b[0m             ret\u001b[38;5;241m.\u001b[39mappend(func(v))\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n",
            "\u001b[0;31mAttributeError\u001b[0m: Line2D.set() got an unexpected keyword argument 'highlight_offset_max'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        # \"Global - Chunked FedAvg\": loaded_dict_cifar_mnist[\"net_acc_round\"][:100],\n",
        "        # \"Global - GeraFed\": loaded_dict_cifar_mnist_gerafed[\"net_acc_round\"][:100],\n",
        "        \"Client 0 - Chunked FedAvg\": local_acc_cifar_dir01[0][:100],\n",
        "       \"Client 0 - GeraFed\": local_acc_cifar_dir01_gerafed[0][:100],\n",
        "        \"Client 1 - Chunked FedAvg\": local_acc_cifar_dir01[1][:100],\n",
        "       \"Client 1 - GeraFed\": local_acc_cifar_dir01_gerafed[1][:100],\n",
        "        \"Client 2 - Chunked FedAvg\": local_acc_cifar_dir01[2][:100],\n",
        "       \"Client 2 - GeraFed\": local_acc_cifar_dir01_gerafed[2][:100],\n",
        "        \"Chunked FedAvg\": local_acc_cifar_dir01[3],\n",
        "        \"GeraFed\": local_acc_cifar_dir01_gerafed[3][:100],\n",
        "    },\n",
        "    series_styles = {\n",
        "        # \"Global - Chunked FedAvg\": {\"color\": \"lightblue\", \"linestyle\": \"-\"},\n",
        "        # \"Global - GeraFed\": {\"color\": \"lightblue\", \"linestyle\": \"--\"},\n",
        "        \"Client 0 - Chunked FedAvg\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Client 0 - GeraFed\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        \"Client 1 - Chunked FedAvg\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Client 1 - GeraFed\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        \"Client 2 - Chunked FedAvg\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"Client 2 - GeraFed\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "        \"Chunked FedAvg\": {\"color\": \"cornflowerblue\", \"linestyle\": \"-\"},\n",
        "        \"GeraFed\": {\"color\": \"sandybrown\", \"linestyle\": \"--\"},\n",
        "    },\n",
        "    subplot_groups=[\n",
        "        [\"Client 0 - Chunked FedAvg\", \"Client 0 - GeraFed\"],\n",
        "        [\"Client 1 - Chunked FedAvg\", \"Client 1 - GeraFed\"],\n",
        "        [\"Client 2 - Chunked FedAvg\", \"Client 2 - GeraFed\"],\n",
        "        [\"Chunked FedAvg\", \"GeraFed\"]\n",
        "    ],\n",
        "    highlight={\n",
        "    #    \"Global - Chunked FedAvg\": \"max\",\n",
        "    #     \"Global - GeraFed\": \"max\",\n",
        "        \"Client 0 - Chunked FedAvg\": \"max\",\n",
        "       \"Client 0 - GeraFed\": \"max\",\n",
        "        \"Client 1 - Chunked FedAvg\": \"max\",\n",
        "       \"Client 1 - GeraFed\": \"max\",\n",
        "        \"Client 2 - Chunked FedAvg\": \"max\",\n",
        "       \"Client 2 - GeraFed\": \"max\",\n",
        "        \"Chunked FedAvg\": \"max\",\n",
        "        \"GeraFed\": \"max\",\n",
        "    },\n",
        "    highlight_style={\n",
        "        # \"Global - Chunked FedAvg\": {\"color\": \"blue\"},\n",
        "        # \"Global - GeraFed\": {\"color\": \"blue\"},\n",
        "        \"Client 0 - Chunked FedAvg\": {\"highlight_offset_max\": (-0.2, 0.1)},\n",
        "       \"Client 0 - GeraFed\": {\"highlight_offset_max\": (-0.2, 0.1)},\n",
        "        \"Client 1 - Chunked FedAvg\": {\"highlight_offset_max\": (-0.2, 0.1)},\n",
        "       \"Client 1 - GeraFed\": {\"highlight_offset_max\": (-0.2, 0.1)},\n",
        "        \"Client 2 - Chunked FedAvg\": {\"highlight_offset_max\": (-0.2, 0.1)},\n",
        "       \"Client 2 - GeraFed\": {\"highlight_offset_max\": (-0.2, 0.1)},\n",
        "        \"Chunked FedAvg\": {\"highlight_offset_max\": (-0.2, 0.1)},\n",
        "        \"GeraFed\": {\"highlight_offset_max\": (-0.2, 0.1)},\n",
        "    },\n",
        "    subplot_layout=(1,4),\n",
        "    label_fontsize=18,\n",
        "    tick_fontsize=16,\n",
        "    highlight_markersize=5,\n",
        "    xtick_step=5,\n",
        "    num_ticks=5,\n",
        "    first_step=4,\n",
        "    ylabel= [\"Accuracy\",\"\",\"\",\"\"],\n",
        "    figsize=(16, 4),\n",
        "    highlight_text_size=14,\n",
        "    highlight_text_offset_max=(0.2, 0.02),\n",
        "    legend_subplot_index=3,\n",
        "    legend_fontsize=14,\n",
        "    title_fontsize=18,\n",
        "    title=[\"a) Client 0\", \"b) Client 1\", \"c) Client 2\", \"d) Client 3\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8952ba7e",
      "metadata": {},
      "source": [
        "### Different distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf794d8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series={\n",
        "        \"IID\": loaded_dict_cifar_IID[\"net_acc_round\"],\n",
        "        \"Dir05\": loaded_dict_cifar_dir05[\"net_acc_round\"],\n",
        "        \"Dir01\": loaded_dict_cifar_dir01[\"net_acc_round\"][:100],\n",
        "        \"NIID Class\": loaded_dict_cifar_class[\"net_acc_round\"][:100],\n",
        "        \"IID mnist\": loaded_dict_mnist_IID[\"net_acc_round\"],\n",
        "        \"Dir05 mnist\": loaded_dict_mnist_Dir05[\"net_acc_round\"],\n",
        "        \"Dir01 mnist\": loaded_dict_mnist_Dir01[\"net_acc_round\"][:100],\n",
        "        \"NIID Class mnist\": loaded_dict_mnist_class[\"net_acc_round\"][:100],\n",
        "    },\n",
        "    subplot_groups=[\n",
        "         [\"IID mnist\", \"Dir05 mnist\", \"Dir01 mnist\", \"NIID Class mnist\"],\n",
        "        [\"IID\", \"Dir05\", \"Dir01\", \"NIID Class\"]\n",
        "    ],\n",
        "    legend_subplot_index=0,\n",
        "    title=[\"a) MNIST\", \"b) CIFAR-10\"],\n",
        "    highlight={\n",
        "        \"IID\": \"max\",\n",
        "        \"Dir05\": \"max\",\n",
        "        \"Dir01\": \"max\",\n",
        "        \"NIID Class\": \"max\",\n",
        "        \"IID mnist\": \"max\",\n",
        "        \"Dir05 mnist\": \"max\",\n",
        "        # \"Dir01 mnist\": \"max\",\n",
        "        \"NIID Class mnist\": \"max\",\n",
        "    },\n",
        "    highlight_markersize=4,\n",
        "    xtick_step=5,\n",
        "    first_step=4,\n",
        "    ylabel=\"Accuracy\",\n",
        "    figsize=(10, 6.4),\n",
        "    highlight_text_size=14,\n",
        "    tick_fontsize=14,\n",
        "    label_fontsize=14,\n",
        "    legend_fontsize=14\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc847093",
      "metadata": {},
      "source": [
        "### GAN loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fca7c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series={\n",
        "        \"Generator\": loaded_dict_cifar_class_gerafed['g_losses_round'],\n",
        "        \"Discriminator\": loaded_dict_cifar_class_gerafed['d_losses_round']\n",
        "    },\n",
        "    xtick_step=5,\n",
        "    first_step=4,\n",
        "    ylabel=\"Loss\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b608dfdb",
      "metadata": {},
      "source": [
        "### GeraFed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0637fcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_series(\n",
        "    series = {\n",
        "        \"Chunked FedAvg\": loaded_dict_cifar_class[\"net_acc_round\"],\n",
        "        \"GeraFed\": loaded_dict_cifar_class_gerafed[\"net_acc_round\"],\n",
        "        \"Chunked FedAvg cifardir05\": loaded_dict_cifar_dir05[\"net_acc_round\"],\n",
        "        \"GeraFed cifardir05\": loaded_dict_cifar_dir05_gerafed[\"net_acc_round\"],\n",
        "        \"Chunked FedAvg cifardir01\": loaded_dict_cifar_dir01[\"net_acc_round\"][:100],\n",
        "        \"GeraFed cifardir01\": loaded_dict_cifar_dir01_gerafed[\"net_acc_round\"][:100],\n",
        "        \"Chunked FedAvg mnistclass\": loaded_dict_mnist_class[\"net_acc_round\"],\n",
        "        \"GeraFed mnistclass\": loaded_dict_mnist_class_gerafed[\"net_acc_round\"]\n",
        "        },\n",
        "    series_styles={\n",
        "        \"Chunked FedAvg\": {\"color\": \"cornflowerblue\"},\n",
        "        \"GeraFed\": {\"color\": \"sandybrown\"},\n",
        "        \"Chunked FedAvg cifardir05\": {\"color\": \"cornflowerblue\"},\n",
        "        \"GeraFed cifardir05\": {\"color\": \"sandybrown\"},\n",
        "        \"Chunked FedAvg cifardir01\": {\"color\": \"cornflowerblue\"},\n",
        "        \"GeraFed cifardir01\": {\"color\": \"sandybrown\"},\n",
        "        \"Chunked FedAvg mnistclass\": {\"color\": \"cornflowerblue\"},\n",
        "        \"GeraFed mnistclass\": {\"color\": \"sandybrown\"},\n",
        "    },\n",
        "    subplot_groups=[\n",
        "                    [\"Chunked FedAvg mnistclass\", \"GeraFed mnistclass\"],\n",
        "                    [\"Chunked FedAvg cifardir05\", \"GeraFed cifardir05\"],\n",
        "                    [\"Chunked FedAvg cifardir01\", \"GeraFed cifardir01\"],\n",
        "                    [\"Chunked FedAvg\", \"GeraFed\"],],\n",
        "    subplot_layout=(2,2),\n",
        "    figsize=(14,4),\n",
        "    highlight={\n",
        "        \"Chunked FedAvg\": \"max\",\n",
        "        \"GeraFed\": \"max\",\n",
        "        \"Chunked FedAvg cifardir05\": \"max\",\n",
        "        \"GeraFed cifardir05\": \"max\",\n",
        "        \"Chunked FedAvg cifardir01\": \"max\",\n",
        "        \"GeraFed cifardir01\": \"max\",\n",
        "        \"Chunked FedAvg mnistclass\": \"max\",\n",
        "        \"GeraFed mnistclass\": \"max\",\n",
        "    },\n",
        "    highlight_markersize=4,\n",
        "    xtick_step=5,\n",
        "    first_step=4,\n",
        "    ylabel=\"Accuracy\",\n",
        "    title=[\"a) MNIST Class\", \"b) CIFAR Dir05\", \"c) CIFAR Dir01\", \"d) CIFAR Class\"],\n",
        "    legend_subplot_index=3,\n",
        "    highlight_text_size=10,\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d66909",
      "metadata": {},
      "source": [
        "## Plot generator images per epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa906d3",
      "metadata": {
        "id": "bfa906d3"
      },
      "outputs": [],
      "source": [
        "for i in range(1,5,1):\n",
        "    gen = F2U_GAN(condition=True).to(\"cpu\")\n",
        "    if IN_COLAB:\n",
        "      gen.load_state_dict(torch.load(os.path.join(save_dir,f\"gen_round{i}.pt\")))\n",
        "    else:\n",
        "      gen.load_state_dict(torch.load(f\"gen_round{i}.pt\"))\n",
        "    generate_plot(gen, \"cpu\", i, latent_dim=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95346fb",
      "metadata": {},
      "source": [
        "## Evaluate Times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be75e38",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# A helper function to add labels on top of the bars\n",
        "def add_labels(rects, ax):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}', # Format the number to 2 decimal places\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontsize=14) # Fontsize for the labels on bars\n",
        "\n",
        "# --- Main Code ---\n",
        "\n",
        "# Data for the bar plots\n",
        "labels = ['Classifier Training', 'Image Generation']\n",
        "first_epoch_a = [0.1, 0.02]\n",
        "last_epoch_a = [0.23, 0.3]\n",
        "first_epoch_b = [0.09, 0.03]\n",
        "last_epoch_b = [0.2, 0.42]\n",
        "\n",
        "# Setting the positions of the bars\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "# Creating the figure and subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# --- Font sizes ---\n",
        "title_fontsize = 18\n",
        "label_fontsize = 14\n",
        "tick_fontsize = 12\n",
        "legend_fontsize = 12\n",
        "\n",
        "# --- Barplot a) ---\n",
        "# Capture the bar containers in variables (rects1a, rects2a)\n",
        "rects1a = ax1.bar(x - width/2, first_epoch_a, width, label='First Epoch', color=\"cornflowerblue\")\n",
        "rects2a = ax1.bar(x + width/2, last_epoch_a, width, label='Last Epoch', color=\"sandybrown\")\n",
        "\n",
        "# Add titles and labels\n",
        "ax1.set_ylabel('Time (s)', fontsize=label_fontsize)\n",
        "ax1.set_title('a)', fontsize=title_fontsize)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(labels, fontsize=label_fontsize)\n",
        "ax1.tick_params(axis='y', labelsize=tick_fontsize)\n",
        "ax1.legend(fontsize=legend_fontsize)\n",
        "\n",
        "# Add the labels on top of the bars\n",
        "add_labels(rects1a, ax1)\n",
        "add_labels(rects2a, ax1)\n",
        "\n",
        "# --- Barplot b) ---\n",
        "# Capture the bar containers in variables (rects1b, rects2b)\n",
        "rects1b = ax2.bar(x - width/2, first_epoch_b, width, label='First Epoch', color=\"cornflowerblue\")\n",
        "rects2b = ax2.bar(x + width/2, last_epoch_b, width, label='Last Epoch', color=\"sandybrown\")\n",
        "\n",
        "# Add titles and labels\n",
        "ax2.set_ylabel('Time (s)', fontsize=label_fontsize)\n",
        "ax2.set_title('b)', fontsize=title_fontsize)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels, fontsize=label_fontsize)\n",
        "ax2.tick_params(axis='y', labelsize=tick_fontsize)\n",
        "ax2.legend(fontsize=legend_fontsize)\n",
        "\n",
        "# Add the labels on top of the bars\n",
        "add_labels(rects1b, ax2)\n",
        "add_labels(rects2b, ax2)\n",
        "\n",
        "# Adjust y-axis limits to make space for the labels\n",
        "ax1.set_ylim(0, ax1.get_ylim()[1] * 1.1)\n",
        "ax2.set_ylim(0, ax2.get_ylim()[1] * 1.1)\n",
        "\n",
        "# Adjust the layout\n",
        "fig.tight_layout(pad=3.0)\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed4a97a6",
      "metadata": {},
      "source": [
        "## Network Traffic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df05c2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from Simulation.task import get_weights, get_weights_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe472b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_mnist = Net()\n",
        "classifier_cifar = Net_Cifar()\n",
        "GAN_MNIST = F2U_GAN()\n",
        "GAN_CIFAR = F2U_GAN_CIFAR()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "598a49cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_mnist_params = get_weights(classifier_mnist)\n",
        "classifier_cifar_params = get_weights(classifier_cifar)\n",
        "GAN_MNIST_disc_params = get_weights_gen(GAN_MNIST)\n",
        "GAN_CIFAR_disc_params = get_weights_gen(GAN_CIFAR)\n",
        "GAN_MNIST_gen_params = [val.cpu().numpy() for key, val in GAN_MNIST.state_dict().items() if 'generator' in key or 'label' in key]\n",
        "GAN_CIFAR_gen_params = [val.cpu().numpy() for key, val in GAN_CIFAR.state_dict().items() if 'generator' in key or 'label' in key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016519d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cumulative step plot for upload/download traffic over rounds.\n",
        "import numpy as np\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0672bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_size_mb(params, divisor=10**6):\n",
        "    buffer = io.BytesIO()\n",
        "    np.savez(buffer, *params)\n",
        "    return len(buffer.getvalue()) / divisor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d83f0db2",
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_mnist_MB = get_model_size_mb(classifier_mnist_params)\n",
        "classifier_cifar_MB = get_model_size_mb(classifier_cifar_params)\n",
        "disc_mnist_MB       = get_model_size_mb(GAN_MNIST_disc_params)\n",
        "disc_cifar_MB       = get_model_size_mb(GAN_CIFAR_disc_params)\n",
        "gen_mnist_MB        = get_model_size_mb(GAN_MNIST_gen_params)\n",
        "gen_cifar_MB        = get_model_size_mb(GAN_CIFAR_gen_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015b4532",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "# Per-epoch traffic (GB)\n",
        "upload_per_epoch_gerafed = (classifier_mnist_MB + disc_mnist_MB)/10 #/1000 pra Giga e x100 por epoch por cause do chunk.\n",
        "download_per_epoch_gerafed = (classifier_mnist_MB + gen_mnist_MB)/10\n",
        "\n",
        "upload_per_epoch_chunkedfedavg = classifier_mnist_MB/10\n",
        "download_per_epoch_chunkedfedavg = classifier_mnist_MB/10\n",
        "\n",
        "\n",
        "# Cumulative arrays with an initial 0 so the plot has horizontal lines before first epoch\n",
        "x = np.arange(0, epochs + 1)  # 0..epochs inclusive\n",
        "\n",
        "\n",
        "cum_upload_gerafed = np.insert(np.cumsum(np.full(epochs, upload_per_epoch_gerafed)), 0, 0)\n",
        "cum_download_gerafed = np.insert(np.cumsum(np.full(epochs, download_per_epoch_gerafed)), 0, 0)\n",
        "\n",
        "cum_upload_chunkedfedavg = np.insert(np.cumsum(np.full(epochs, upload_per_epoch_chunkedfedavg)), 0, 0)\n",
        "cum_download_chunkedfedavg = np.insert(np.cumsum(np.full(epochs, download_per_epoch_chunkedfedavg)), 0, 0)\n",
        "\n",
        "\n",
        "total_upload_GB_gerafed = cum_upload_gerafed[-1]\n",
        "total_download_GB_gerafed = cum_download_gerafed[-1]\n",
        "\n",
        "total_upload_GB_chunkedfedavg = cum_upload_chunkedfedavg[-1]\n",
        "total_download_GB_chunkedfedavg = cum_download_chunkedfedavg[-1]\n",
        "\n",
        "\n",
        "# Single step plot (cumulative). Using where='post' so the vertical jumps happen at integer epochs.\n",
        "plt.figure(figsize=(10, 5))\n",
        "#plt.step(x, cum_upload_gerafed, where='post', label=\"GeraFed upload\", color=\"cornflowerblue\")\n",
        "#plt.step(x, cum_download_gerafed, where='post', label=\"GeraFed download\", color=\"royalblue\")\n",
        "plt.step(x, cum_upload_chunkedfedavg, where='post', label=\"Chunked FedAvg upload\", color=\"sandybrown\")\n",
        "plt.step(x, cum_download_chunkedfedavg, where='post', label=\"Chunked FedAvg download\", color=\"peru\")\n",
        "plt.xlim(0, epochs)\n",
        "plt.xticks(np.arange(0, epochs+1, max(1, epochs//10)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cumulative GB\")\n",
        "plt.title(\"Cumulative upload/download traffic (step plot)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate final totals on the right side\n",
        "#plt.annotate(f\"{total_upload_GB_gerafed:.0f} GB\", xy=(epochs, total_upload_GB_gerafed),\n",
        "            #  xytext=(epochs-5, total_upload_GB_gerafed + max(1, total_upload_GB_gerafed*0.02)),\n",
        "            #  arrowprops=dict(arrowstyle=\"->\"), fontsize=9, va=\"bottom\")\n",
        "#plt.annotate(f\"{total_download_GB_gerafed:.0f} GB\", xy=(epochs, total_download_GB_gerafed),\n",
        "            #  xytext=(epochs-5, total_download_GB_gerafed + max(1, total_download_GB_gerafed*0.02)),\n",
        "            #  arrowprops=dict(arrowstyle=\"->\"), fontsize=9, va=\"bottom\")\n",
        "plt.annotate(f\"{total_upload_GB_chunkedfedavg:.0f} GB\", xy=(epochs, total_upload_GB_chunkedfedavg),\n",
        "             xytext=(epochs-5, total_upload_GB_chunkedfedavg + max(1, total_upload_GB_chunkedfedavg*0.02)),\n",
        "             arrowprops=dict(arrowstyle=\"->\"), fontsize=9, va=\"bottom\")\n",
        "plt.annotate(f\"{total_download_GB_chunkedfedavg:.0f} GB\", xy=(epochs, total_download_GB_chunkedfedavg),\n",
        "             xytext=(epochs-5, total_download_GB_chunkedfedavg + max(1, total_download_GB_chunkedfedavg*0.02)),\n",
        "             arrowprops=dict(arrowstyle=\"->\"), fontsize=9, va=\"bottom\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b605738",
      "metadata": {},
      "source": [
        "## Number of Synthetic Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e65e16b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f302686",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate x (epoch) values from 0 to 100\n",
        "epochs = np.arange(0, 101)\n",
        "\n",
        "# Calculate y values for each epoch\n",
        "y_values = [int(13 * (math.exp(0.01*epoch) - 1) / (math.exp(0.01*50) - 1) * 10) for epoch in epochs]\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.plot(epochs, y_values, color='cornflowerblue', linewidth=5)\n",
        "plt.xlabel(\"Epoch\", fontsize=20)\n",
        "plt.ylabel(\"Synthetic Images\", fontsize=20)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.xlim(0, 100)\n",
        "plt.ylim(0, 350)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b56bcc9",
      "metadata": {},
      "source": [
        "## Chunks analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305738dc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "aeeddbca",
      "metadata": {
        "id": "aeeddbca"
      },
      "source": [
        "# Compara treino de classificador em dados reais, sintéticos e misturados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c400df8",
      "metadata": {
        "id": "6c400df8"
      },
      "outputs": [],
      "source": [
        "nets = [Net().to(device) for _ in range(num_partitions)]\n",
        "optims = [torch.optim.Adam(net.parameters(), lr=0.01) for net in nets]\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2117f9e5",
      "metadata": {
        "id": "2117f9e5"
      },
      "outputs": [],
      "source": [
        "for i, (net, optim) in enumerate(zip(nets, optims)):\n",
        "    net.train()\n",
        "    for epoch in range(50):\n",
        "        for data in trainloaders[i]:\n",
        "            inputs, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "            optim.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ad2d72",
      "metadata": {
        "id": "c7ad2d72"
      },
      "outputs": [],
      "source": [
        "testpartition = fds.load_split(\"test\")\n",
        "testpartition = testpartition.with_transform(apply_transforms)\n",
        "testloader = DataLoader(testpartition, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abfbb983",
      "metadata": {
        "id": "abfbb983"
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "for net in nets:\n",
        "    correct, loss = 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "    accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eedc9b7",
      "metadata": {
        "id": "0eedc9b7"
      },
      "outputs": [],
      "source": [
        "accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea7a7b0",
      "metadata": {
        "id": "5ea7a7b0"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_samples = 1000\n",
        "latent_dim = 128\n",
        "\n",
        "# gen = F2U_GAN()\n",
        "# gen.load_state_dict(torch.load(\"gen_round50.pt\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "generated_dataset = GeneratedDataset(generator=gen.to(\"cpu\"), num_samples=num_samples, latent_dim=latent_dim, num_classes=10, device=\"cpu\")\n",
        "generated_dataloader = DataLoader(generated_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc9f7fe",
      "metadata": {
        "id": "3bc9f7fe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import ConcatDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e1cbfd",
      "metadata": {
        "id": "99e1cbfd"
      },
      "outputs": [],
      "source": [
        "combined_dataloaders = []\n",
        "for train_partition in train_partitions:\n",
        "    # Ensure the partition is transformed\n",
        "    cmb_ds = ConcatDataset([train_partition, generated_dataset])\n",
        "    combined_dataloaders.append(DataLoader(cmb_ds, batch_size=batch_size, shuffle=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85df355d",
      "metadata": {
        "id": "85df355d"
      },
      "outputs": [],
      "source": [
        "nets = [Net().to(device) for _ in range(num_partitions)]\n",
        "optims = [torch.optim.Adam(net.parameters(), lr=0.01) for net in nets]\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea13f13",
      "metadata": {
        "id": "9ea13f13"
      },
      "outputs": [],
      "source": [
        "for i, (net, optim) in enumerate(zip(nets, optims)):\n",
        "    net.train()\n",
        "    for epoch in range(50):\n",
        "        for data in combined_dataloaders[i]:\n",
        "            inputs, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "            optim.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9818e23a",
      "metadata": {
        "id": "9818e23a"
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "for net in nets:\n",
        "    correct, loss = 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "    accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b2cb6d",
      "metadata": {
        "id": "c2b2cb6d"
      },
      "outputs": [],
      "source": [
        "accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff260823",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definindo x e N\n",
        "x = list(range(1, 101))\n",
        "den = math.exp(0.01 * 50) - 1\n",
        "N = [int(13 * (math.exp(0.01 * (xi - 1)) - 1) / den) * 1000 for xi in x]\n",
        "y = [390*xi for xi in x]\n",
        "\n",
        "# Plot\n",
        "plt.figure()\n",
        "plt.plot(x, N)\n",
        "plt.plot(x, y)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"N\")\n",
        "plt.title(\"Plot de N = int(13 * (exp(0.01*(x-1)) - 1)/(exp(0.5) - 1)) * 1000\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6d8c276e",
        "314c3604"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gerafed",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
